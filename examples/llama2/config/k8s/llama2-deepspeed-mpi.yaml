#apiVersion: kubeflow.org/v1
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: llama2-finetuning-deepspeed
spec:
  slotsPerWorker: 1  # Lanyun worker4 has 128 CPU cores.
  runPolicy:
    cleanPodPolicy: None  # No pods will be deleted after the job terminates.
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          # https://kubesec.io/basics/spec-hostipc/
          # https://www.fairwinds.com/blog/kubernetes-basics-tutorial-host-ipc-should-not-be-configured
          hostIPC: True  # Setting True for now. Subject to change.
          containers:
          - name: llama2-training-deepspeed-mpi
            image: sxwl/llama2-ft-test:latest
            imagePullPolicy: Always  # Not a big deal. More convenient for testing.
            command:
            - mpirun
            - -np
            - "12"
            #- --hostfile
            #- /path/to/hostfile
            - --allow-run-as-root
            - -bind-to
            - none
            - -map-by
            - slot
            - -x
            - NCCL_DEBUG=INFO
            - -x
            - NCCL_P2P_DISABLE=1
            - -x
            - LD_LIBRARY_PATH
            - -x
            - PATH
            - -mca
            - mpi_warn_on_fork
            - "0"
            - python3
            - llama2_demo.py
            - --checkpoint_dir
            - ./llama2_ckpt/
            - --deepspeed_mpi
            - --deepspeed
            volumeMounts:
            - name: mpi-hostfile
              mountPath: /llama2/hostfile
              subPath: hostfile
            env:
            - name: NCCL_TREE_THRESHOLD
              value: "0"  # Set NCCL tree threshold to 0 for DeepSpeed.
            - name: NCCL_ALGO
              value: "1"  # Set NCCL_ALGO to 1 for DeepSpeed.
      volumes:
        - name: mpi-hostfile
          # Assuming a ConfigMap with the necessary hostfile content was already created.
          configMap:
            name: mpi-hostfile-configmap
    Worker:
      replicas: 3  # Number of nodes for training.
      template:
        spec:
          hostIPC: True
          containers:
          - name: llama2-training-deepspeed-mpi
            image: sxwl/llama2-ft-test:latest
            imagePullPolicy: Always  # Not a big deal. More convenient for testing.
            resources:
              limits:
                nvidia.com/gpu: 4  # Number of GPUs required per replica.
            volumeMounts:
            - name: training-data
              mountPath: /llama2/data  # Mount the training data volume inside the container.
      volumes:
      - name: training-data
        persistentVolumeClaim:
          # Assuming a PersistentVolumeClaim was already created for training data.
          claimName: training-data-pvc
