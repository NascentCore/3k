apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-2-7b
  namespace: cpod
spec:
  predictor:
    containers:
      - name: kserve-container
        args:
          - --port
          - "8080"
          - --model
          - /mnt/models
        command:
          - python3
          - -m
          - vllm.entrypoints.api_server
        env:
          - name: STORAGE_URI
            value: pvc://modesaved/
        image: m.daocloud.io/ghcr.io/substratusai/vllm:latest
        resources:
          limits:
            cpu: "4"
            memory: 50Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: 50Gi
            nvidia.com/gpu: "1"