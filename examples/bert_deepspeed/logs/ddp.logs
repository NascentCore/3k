nohup: ignoring input
[2023-07-27 21:50:44,200] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-07-27 21:50:45,764] [INFO] [runner.py:452:main] Using IP address of 214.2.5.1 for node dss8440-001
[2023-07-27 21:50:45,765] [INFO] [multinode_runner.py:70:get_cmd] Running on the following workers: dss8440-001,dss8440-002,dss8440-003
[2023-07-27 21:50:45,765] [INFO] [runner.py:555:main] cmd = pdsh -S -f 1024 -w dss8440-001,dss8440-002,dss8440-003 export PYTHONPATH=/home/lanyun/workspace/bert_deepspeed; export NCCL_SOCKET_IFNAME=bond0; export NCCL_DEBUG=INFO; export NCCL_IB_DISABLE=1;  cd /home/lanyun/workspace/bert_deepspeed; /home/lanyun/anaconda3/envs/bert_deepspeed/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJkc3M4NDQwLTAwMSI6IFswLCAxLCAyLCAzLCA0LCA1LCA2XSwgImRzczg0NDAtMDAyIjogWzAsIDEsIDIsIDMsIDQsIDUsIDZdLCAiZHNzODQ0MC0wMDMiOiBbMCwgMSwgMiwgMywgNCwgNSwgNl19 --node_rank=%n --master_addr=214.2.5.1 --master_port=60000 train_bert_ds.py --checkpoint_dir './ds_experiments'
dss8440-003: [2023-07-27 21:50:48,714] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:48,753] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:48,764] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:138:main] 2 NCCL_SOCKET_IFNAME=bond0
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:138:main] 2 NCCL_DEBUG=INFO
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:138:main] 2 NCCL_IB_DISABLE=1
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:145:main] WORLD INFO DICT: {'dss8440-001': [0, 1, 2, 3, 4, 5, 6], 'dss8440-002': [0, 1, 2, 3, 4, 5, 6], 'dss8440-003': [0, 1, 2, 3, 4, 5, 6]}
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:151:main] nnodes=3, num_local_procs=7, node_rank=2
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'dss8440-001': [0, 1, 2, 3, 4, 5, 6], 'dss8440-002': [7, 8, 9, 10, 11, 12, 13], 'dss8440-003': [14, 15, 16, 17, 18, 19, 20]})
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:163:main] dist_world_size=21
dss8440-003: [2023-07-27 21:50:48,984] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6
dss8440-001: [2023-07-27 21:50:49,035] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=bond0
dss8440-001: [2023-07-27 21:50:49,035] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=INFO
dss8440-001: [2023-07-27 21:50:49,035] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1
dss8440-001: [2023-07-27 21:50:49,036] [INFO] [launch.py:145:main] WORLD INFO DICT: {'dss8440-001': [0, 1, 2, 3, 4, 5, 6], 'dss8440-002': [0, 1, 2, 3, 4, 5, 6], 'dss8440-003': [0, 1, 2, 3, 4, 5, 6]}
dss8440-001: [2023-07-27 21:50:49,036] [INFO] [launch.py:151:main] nnodes=3, num_local_procs=7, node_rank=0
dss8440-001: [2023-07-27 21:50:49,036] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'dss8440-001': [0, 1, 2, 3, 4, 5, 6], 'dss8440-002': [7, 8, 9, 10, 11, 12, 13], 'dss8440-003': [14, 15, 16, 17, 18, 19, 20]})
dss8440-001: [2023-07-27 21:50:49,036] [INFO] [launch.py:163:main] dist_world_size=21
dss8440-001: [2023-07-27 21:50:49,036] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:138:main] 1 NCCL_SOCKET_IFNAME=bond0
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:138:main] 1 NCCL_DEBUG=INFO
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:138:main] 1 NCCL_IB_DISABLE=1
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:145:main] WORLD INFO DICT: {'dss8440-001': [0, 1, 2, 3, 4, 5, 6], 'dss8440-002': [0, 1, 2, 3, 4, 5, 6], 'dss8440-003': [0, 1, 2, 3, 4, 5, 6]}
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:151:main] nnodes=3, num_local_procs=7, node_rank=1
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'dss8440-001': [0, 1, 2, 3, 4, 5, 6], 'dss8440-002': [7, 8, 9, 10, 11, 12, 13], 'dss8440-003': [14, 15, 16, 17, 18, 19, 20]})
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:163:main] dist_world_size=21
dss8440-002: [2023-07-27 21:50:49,230] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6
dss8440-003: [2023-07-27 21:50:51,290] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:51,292] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:51,307] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:51,309] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:51,313] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,356] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,366] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,369] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,388] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:51,393] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,418] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,424] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-001: [2023-07-27 21:50:51,458] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: [2023-07-27 21:50:51,463] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,670] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,735] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,740] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,760] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,767] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,778] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-002: [2023-07-27 21:50:51,796] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-003: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: 2023-07-27 21:50:52.087 | INFO     | __main__:log_dist:53 - [Rank 0] Creating Experiment Directory
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: 2023-07-27 21:50:52.146 | INFO     | __main__:log_dist:53 - [Rank 0] Experiment Directory created at ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: 2023-07-27 21:50:52.148 | INFO     | __main__:log_dist:53 - [Rank 0] Creating Datasets
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: 2023-07-27 21:50:52.238 | INFO     | __main__:log_dist:53 - [Rank 0] Dataset Creation Done
dss8440-001: 2023-07-27 21:50:52.238 | INFO     | __main__:log_dist:53 - [Rank 0] Creating Model
dss8440-003: [2023-07-27 21:50:52,514] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,514] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-003: [2023-07-27 21:50:52,514] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: [2023-07-27 21:50:52,561] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,561] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,561] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-003: [2023-07-27 21:50:52,561] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-003: [2023-07-27 21:50:52,561] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: [2023-07-27 21:50:52,576] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-001: [2023-07-27 21:50:52,581] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-001: [2023-07-27 21:50:52,581] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,581] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-003: [2023-07-27 21:50:52,589] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,589] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-003: [2023-07-27 21:50:52,589] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-003: [2023-07-27 21:50:52,589] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,589] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-003: [2023-07-27 21:50:52,590] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-003: [2023-07-27 21:50:52,594] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,595] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-003: [2023-07-27 21:50:52,595] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-003: [2023-07-27 21:50:52,619] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-003: [2023-07-27 21:50:52,621] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-003: [2023-07-27 21:50:52,621] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-001: [2023-07-27 21:50:52,641] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: [2023-07-27 21:50:52,641] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,641] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-719682cdde8dc7bf.arrow
dss8440-002: Loading cached processed dataset at /home/lanyun/workspace/bert_deepspeed/dataset/wikitext/cache-c02c629be02c517d.arrow
dss8440-001: [2023-07-27 21:50:52,665] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-001: [2023-07-27 21:50:52,666] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,666] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-001: [2023-07-27 21:50:52,688] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-001: [2023-07-27 21:50:52,688] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,688] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-001: [2023-07-27 21:50:52,701] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-001: [2023-07-27 21:50:52,701] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,701] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-001: 2023-07-27 21:50:52.717 | INFO     | __main__:log_dist:53 - [Rank 0] Model Creation Done
dss8440-001: 2023-07-27 21:50:52.718 | INFO     | __main__:log_dist:53 - [Rank 0] Creating DeepSpeed engine
dss8440-001: [2023-07-27 21:50:52,718] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-001: [2023-07-27 21:50:52,719] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,719] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-001: [2023-07-27 21:50:52,719] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
dss8440-001: [2023-07-27 21:50:52,736] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-001: [2023-07-27 21:50:52,737] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-001: [2023-07-27 21:50:52,737] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,054] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,054] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,054] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,098] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,098] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,098] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,161] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,161] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,161] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,178] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,180] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,180] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,193] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,193] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,193] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,221] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,221] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,222] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-002: [2023-07-27 21:50:53,253] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
dss8440-002: [2023-07-27 21:50:53,253] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
dss8440-002: [2023-07-27 21:50:53,254] [INFO] [comm.py:616:init_distributed] cdb=None
dss8440-001: master:13934:13934 [0] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-001: master:13934:13934 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13934:13934 [0] NCCL INFO cudaDriverVersion 11070
dss8440-001: NCCL version 2.14.3+cuda11.7
dss8440-001: master:13934:15089 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-001: master:13934:15089 [0] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13934:15089 [0] NCCL INFO Using network Socket
dss8440-002: worker1:4986:4986 [2] NCCL INFO cudaDriverVersion 11070
dss8440-001: master:13940:13940 [6] NCCL INFO cudaDriverVersion 11070
dss8440-001: master:13936:13936 [2] NCCL INFO cudaDriverVersion 11070
dss8440-001: master:13938:13938 [4] NCCL INFO cudaDriverVersion 11070
dss8440-001: master:13935:13935 [1] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4984:4984 [0] NCCL INFO cudaDriverVersion 11070
dss8440-001: master:13937:13937 [3] NCCL INFO cudaDriverVersion 11070
dss8440-001: master:13939:13939 [5] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4987:4987 [3] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61448:61448 [5] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61447:61447 [4] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4985:4985 [1] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4990:4990 [6] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4988:4988 [4] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61444:61444 [1] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61443:61443 [0] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4989:4989 [5] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61449:61449 [6] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61446:61446 [3] NCCL INFO cudaDriverVersion 11070
dss8440-003: worker2:61445:61445 [2] NCCL INFO cudaDriverVersion 11070
dss8440-002: worker1:4987:4987 [3] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-002: worker1:4987:4987 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-002: worker1:4986:4986 [2] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-002: worker1:4986:4986 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-002: worker1:4987:6128 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-003: worker2:61449:61449 [6] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-003: worker2:61449:61449 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-002: worker1:4987:6128 [3] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-002: worker1:4987:6128 [3] NCCL INFO Using network Socket
dss8440-002: worker1:4989:4989 [5] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-002: worker1:4986:6129 [2] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-002: worker1:4986:6129 [2] NCCL INFO Using network Socket
dss8440-002: worker1:4989:4989 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61449:62587 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61449:62587 [6] NCCL INFO Using network Socket
dss8440-002: worker1:4989:6131 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-002: worker1:4989:6131 [5] NCCL INFO Using network Socket
dss8440-002: worker1:4988:4988 [4] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-003: worker2:61446:61446 [3] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-002: worker1:4988:4988 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61446:61446 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61443:61443 [0] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-003: worker2:61443:61443 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61444:61444 [1] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-003: worker2:61444:61444 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-002: worker1:4988:6132 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-002: worker1:4988:6132 [4] NCCL INFO Using network Socket
dss8440-003: worker2:61446:62588 [3] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61446:62588 [3] NCCL INFO Using network Socket
dss8440-001: master:13939:13939 [5] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-001: master:13936:13936 [2] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-003: worker2:61443:62589 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-001: master:13939:13939 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13936:13936 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61444:62590 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61443:62589 [0] NCCL INFO Using network Socket
dss8440-002: worker1:4984:4984 [0] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-002: worker1:4984:4984 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61447:61447 [4] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-003: worker2:61444:62590 [1] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61444:62590 [1] NCCL INFO Using network Socket
dss8440-001: master:13940:13940 [6] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-001: master:13940:13940 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13937:13937 [3] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-003: worker2:61447:61447 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13938:13938 [4] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-001: master:13937:13937 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13938:13938 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13936:15091 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-001: master:13939:15090 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4985:4985 [1] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-002: worker1:4984:6133 [0] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-002: worker1:4984:6133 [0] NCCL INFO Using network Socket
dss8440-003: worker2:61447:62591 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4985:4985 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-001: master:13940:15092 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61447:62591 [4] NCCL INFO Using network Socket
dss8440-001: master:13938:15094 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-001: master:13937:15093 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-001: master:13935:13935 [1] NCCL INFO Bootstrap : Using bond0:214.2.5.1<0>
dss8440-002: worker1:4985:6134 [1] NCCL INFO Using network Socket
dss8440-001: master:13935:13935 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61445:61445 [2] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-003: worker2:61445:61445 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61448:61448 [5] NCCL INFO Bootstrap : Using bond0:214.2.5.3<0>
dss8440-001: master:13936:15091 [2] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13936:15091 [2] NCCL INFO Using network Socket
dss8440-001: master:13939:15090 [5] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13939:15090 [5] NCCL INFO Using network Socket
dss8440-001: master:13935:15095 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-003: worker2:61448:61448 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-003: worker2:61445:62592 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-001: master:13940:15092 [6] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13940:15092 [6] NCCL INFO Using network Socket
dss8440-001: master:13938:15094 [4] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13938:15094 [4] NCCL INFO Using network Socket
dss8440-003: worker2:61445:62592 [2] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61445:62592 [2] NCCL INFO Using network Socket
dss8440-001: master:13937:15093 [3] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13937:15093 [3] NCCL INFO Using network Socket
dss8440-003: worker2:61448:62593 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-001: master:13935:15095 [1] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.1<0>
dss8440-001: master:13935:15095 [1] NCCL INFO Using network Socket
dss8440-003: worker2:61448:62593 [5] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.3<0>
dss8440-003: worker2:61448:62593 [5] NCCL INFO Using network Socket
dss8440-002: worker1:4990:4990 [6] NCCL INFO Bootstrap : Using bond0:214.2.5.2<0>
dss8440-002: worker1:4990:4990 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dss8440-002: worker1:4990:6135 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO NET/Socket : Using [0]bond0:214.2.5.2<0>
dss8440-002: worker1:4990:6135 [6] NCCL INFO Using network Socket
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15092 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13940:15092 [6] NCCL INFO Setting affinity for GPU 6 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15094 [4] NCCL INFO Setting affinity for GPU 4 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15089 [0] NCCL INFO Setting affinity for GPU 0 to 555555,55555555,55555555
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13937:15093 [3] NCCL INFO Setting affinity for GPU 3 to 555555,55555555,55555555
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13935:15095 [1] NCCL INFO Setting affinity for GPU 1 to 555555,55555555,55555555
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15090 [5] NCCL INFO Setting affinity for GPU 5 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13936:15091 [2] NCCL INFO Setting affinity for GPU 2 to 555555,55555555,55555555
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO Setting affinity for GPU 0 to 555555,55555555,55555555
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO Setting affinity for GPU 3 to 555555,55555555,55555555
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO Setting affinity for GPU 2 to 555555,55555555,55555555
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Setting affinity for GPU 1 to 555555,55555555,55555555
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Setting affinity for GPU 5 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6135 [6] NCCL INFO Setting affinity for GPU 6 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO Setting affinity for GPU 4 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Setting affinity for GPU 2 to 555555,55555555,55555555
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO Setting affinity for GPU 3 to 555555,55555555,55555555
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Setting affinity for GPU 1 to 555555,55555555,55555555
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO Setting affinity for GPU 0 to 555555,55555555,55555555
dss8440-003: worker2:61448:62593 [5] NCCL INFO Setting affinity for GPU 5 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62587 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO Setting affinity for GPU 6 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-003: worker2:61447:62591 [4] NCCL INFO Setting affinity for GPU 4 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13940:15092 [6] NCCL INFO Trees [0] -1/-1/-1->6->5 [1] -1/-1/-1->6->5
dss8440-001: master:13939:15090 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
dss8440-001: master:13938:15094 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
dss8440-002: worker1:4984:6133 [0] NCCL INFO Trees [0] 8/-1/-1->7->15 [1] 8/0/-1->7->-1
dss8440-001: master:13937:15093 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
dss8440-002: worker1:4990:6135 [6] NCCL INFO Trees [0] -1/-1/-1->13->12 [1] -1/-1/-1->13->12
dss8440-002: worker1:4985:6134 [1] NCCL INFO Trees [0] 9/-1/-1->8->7 [1] 9/-1/-1->8->7
dss8440-002: worker1:4988:6132 [4] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10
dss8440-003: worker2:61449:62587 [6] NCCL INFO Trees [0] -1/-1/-1->20->19 [1] -1/-1/-1->20->19
dss8440-001: master:13936:15091 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dss8440-002: worker1:4989:6131 [5] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11
dss8440-003: worker2:61448:62593 [5] NCCL INFO Trees [0] 20/-1/-1->19->18 [1] 20/-1/-1->19->18
dss8440-003: worker2:61447:62591 [4] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17
dss8440-003: worker2:61446:62588 [3] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/-1/-1->17->16
dss8440-003: worker2:61445:62592 [2] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 17/-1/-1->16->15
dss8440-001: master:13935:15095 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/14/-1->1->0
dss8440-003: worker2:61444:62590 [1] NCCL INFO Trees [0] 16/7/-1->15->14 [1] 16/-1/-1->15->14
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
dss8440-003: worker2:61443:62589 [0] NCCL INFO Trees [0] 15/-1/-1->14->0 [1] 15/-1/-1->14->1
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
dss8440-001: master:13934:15089 [0] NCCL INFO Trees [0] 1/14/-1->0->-1 [1] 1/-1/-1->0->7
dss8440-002: worker1:4987:6128 [3] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
dss8440-002: worker1:4986:6129 [2] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 00/0 : 13[c1000] -> 14[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 01/0 : 13[c1000] -> 14[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 00 : 14[12000] -> 15[13000] via SHM/direct/direct
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 01 : 14[12000] -> 15[13000] via SHM/direct/direct
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 00/0 : 20[c1000] -> 0[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 00/0 : 6[c1000] -> 7[12000] [receive] via NET/Socket/0
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 01/0 : 20[c1000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 01/0 : 6[c1000] -> 7[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 00 : 0[12000] -> 1[13000] via SHM/direct/direct
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 00 : 7[12000] -> 8[13000] via SHM/direct/direct
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 01 : 7[12000] -> 8[13000] via SHM/direct/direct
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 01 : 0[12000] -> 1[13000] via SHM/direct/direct
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6132 [4] NCCL INFO Channel 00 : 11[89000] -> 12[8a000] via SHM/direct/direct
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Channel 00 : 12[8a000] -> 13[c1000] via SHM/direct/direct
dss8440-002: worker1:4987:6128 [3] NCCL INFO Channel 00 : 10[49000] -> 11[89000] via SHM/direct/direct
dss8440-002: worker1:4988:6132 [4] NCCL INFO Channel 01 : 11[89000] -> 12[8a000] via SHM/direct/direct
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Channel 00 : 18[89000] -> 19[8a000] via SHM/direct/direct
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Channel 01 : 12[8a000] -> 13[c1000] via SHM/direct/direct
dss8440-002: worker1:4987:6128 [3] NCCL INFO Channel 01 : 10[49000] -> 11[89000] via SHM/direct/direct
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO Channel 00 : 8[13000] -> 9[48000] via SHM/direct/direct
dss8440-003: worker2:61448:62593 [5] NCCL INFO Channel 00 : 19[8a000] -> 20[c1000] via SHM/direct/direct
dss8440-002: worker1:4986:6129 [2] NCCL INFO Channel 00 : 9[48000] -> 10[49000] via SHM/direct/direct
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO Channel 00 : 17[49000] -> 18[89000] via SHM/direct/direct
dss8440-003: worker2:61447:62591 [4] NCCL INFO Channel 01 : 18[89000] -> 19[8a000] via SHM/direct/direct
dss8440-003: worker2:61445:62592 [2] NCCL INFO Channel 00 : 16[48000] -> 17[49000] via SHM/direct/direct
dss8440-002: worker1:4985:6134 [1] NCCL INFO Channel 01 : 8[13000] -> 9[48000] via SHM/direct/direct
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO Channel 00 : 15[13000] -> 16[48000] via SHM/direct/direct
dss8440-002: worker1:4986:6129 [2] NCCL INFO Channel 01 : 9[48000] -> 10[49000] via SHM/direct/direct
dss8440-003: worker2:61448:62593 [5] NCCL INFO Channel 01 : 19[8a000] -> 20[c1000] via SHM/direct/direct
dss8440-003: worker2:61446:62588 [3] NCCL INFO Channel 01 : 17[49000] -> 18[89000] via SHM/direct/direct
dss8440-003: worker2:61445:62592 [2] NCCL INFO Channel 01 : 16[48000] -> 17[49000] via SHM/direct/direct
dss8440-003: worker2:61444:62590 [1] NCCL INFO Channel 01 : 15[13000] -> 16[48000] via SHM/direct/direct
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15094 [4] NCCL INFO Channel 00 : 4[89000] -> 5[8a000] via SHM/direct/direct
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15090 [5] NCCL INFO Channel 00 : 5[8a000] -> 6[c1000] via SHM/direct/direct
dss8440-001: master:13938:15094 [4] NCCL INFO Channel 01 : 4[89000] -> 5[8a000] via SHM/direct/direct
dss8440-001: master:13939:15090 [5] NCCL INFO Channel 01 : 5[8a000] -> 6[c1000] via SHM/direct/direct
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13937:15093 [3] NCCL INFO Channel 00 : 3[49000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13936:15091 [2] NCCL INFO Channel 00 : 2[48000] -> 3[49000] via SHM/direct/direct
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13937:15093 [3] NCCL INFO Channel 01 : 3[49000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13935:15095 [1] NCCL INFO Channel 00 : 1[13000] -> 2[48000] via SHM/direct/direct
dss8440-001: master:13936:15091 [2] NCCL INFO Channel 01 : 2[48000] -> 3[49000] via SHM/direct/direct
dss8440-001: master:13935:15095 [1] NCCL INFO Channel 01 : 1[13000] -> 2[48000] via SHM/direct/direct
dss8440-002: worker1:4988:6132 [4] NCCL INFO Connected all rings
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61449:62587 [6] NCCL INFO Channel 00/0 : 20[c1000] -> 0[12000] [send] via NET/Socket/0
dss8440-003: worker2:61449:62587 [6] NCCL INFO Channel 01/0 : 20[c1000] -> 0[12000] [send] via NET/Socket/0
dss8440-002: worker1:4990:6135 [6] NCCL INFO Channel 00/0 : 13[c1000] -> 14[12000] [send] via NET/Socket/0
dss8440-002: worker1:4987:6128 [3] NCCL INFO Connected all rings
dss8440-002: worker1:4990:6135 [6] NCCL INFO Channel 01/0 : 13[c1000] -> 14[12000] [send] via NET/Socket/0
dss8440-002: worker1:4986:6129 [2] NCCL INFO Connected all rings
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Connected all rings
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13940:15092 [6] NCCL INFO Channel 00/0 : 6[c1000] -> 7[12000] [send] via NET/Socket/0
dss8440-001: master:13940:15092 [6] NCCL INFO Channel 01/0 : 6[c1000] -> 7[12000] [send] via NET/Socket/0
dss8440-003: worker2:61444:62590 [1] NCCL INFO Connected all rings
dss8440-003: worker2:61445:62592 [2] NCCL INFO Connected all rings
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO Connected all rings
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61446:62588 [3] NCCL INFO Connected all rings
dss8440-003: worker2:61448:62593 [5] NCCL INFO Connected all rings
dss8440-002: worker1:4989:6131 [5] NCCL INFO Connected all rings
dss8440-002: worker1:4988:6132 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6132 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13936:15091 [2] NCCL INFO Connected all rings
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13935:15095 [1] NCCL INFO Connected all rings
dss8440-001: master:13937:15093 [3] NCCL INFO Connected all rings
dss8440-001: master:13938:15094 [4] NCCL INFO Connected all rings
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15090 [5] NCCL INFO Connected all rings
dss8440-002: worker1:4986:6129 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6129 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62591 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62592 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO Channel 00/0 : 7[12000] -> 15[13000] [receive] via NET/Socket/0
dss8440-003: worker2:61449:62587 [6] NCCL INFO Connected all rings
dss8440-002: worker1:4988:6132 [4] NCCL INFO Channel 00 : 11[89000] -> 10[49000] via SHM/direct/direct
dss8440-002: worker1:4988:6132 [4] NCCL INFO Channel 01 : 11[89000] -> 10[49000] via SHM/direct/direct
dss8440-003: worker2:61449:62587 [6] NCCL INFO Channel 00 : 20[c1000] -> 19[8a000] via SHM/direct/direct
dss8440-003: worker2:61449:62587 [6] NCCL INFO Channel 01 : 20[c1000] -> 19[8a000] via SHM/direct/direct
dss8440-002: worker1:4990:6135 [6] NCCL INFO Connected all rings
dss8440-002: worker1:4990:6135 [6] NCCL INFO Channel 00 : 13[c1000] -> 12[8a000] via SHM/direct/direct
dss8440-002: worker1:4990:6135 [6] NCCL INFO Channel 01 : 13[c1000] -> 12[8a000] via SHM/direct/direct
dss8440-001: master:13936:15091 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15091 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13940:15092 [6] NCCL INFO Connected all rings
dss8440-001: master:13940:15092 [6] NCCL INFO Channel 00 : 6[c1000] -> 5[8a000] via SHM/direct/direct
dss8440-001: master:13940:15092 [6] NCCL INFO Channel 01 : 6[c1000] -> 5[8a000] via SHM/direct/direct
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13938:15094 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15094 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Channel 01/0 : 14[12000] -> 1[13000] [receive] via NET/Socket/0
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6134 [1] NCCL INFO Channel 00 : 8[13000] -> 7[12000] via SHM/direct/direct
dss8440-002: worker1:4985:6134 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6134 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO Channel 00 : 9[48000] -> 8[13000] via SHM/direct/direct
dss8440-002: worker1:4985:6134 [1] NCCL INFO Channel 01 : 8[13000] -> 7[12000] via SHM/direct/direct
dss8440-003: worker2:61447:62591 [4] NCCL INFO Channel 00 : 18[89000] -> 17[49000] via SHM/direct/direct
dss8440-002: worker1:4987:6128 [3] NCCL INFO Channel 00 : 10[49000] -> 9[48000] via SHM/direct/direct
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4986:6129 [2] NCCL INFO Channel 01 : 9[48000] -> 8[13000] via SHM/direct/direct
dss8440-002: worker1:4987:6128 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62591 [4] NCCL INFO Channel 01 : 18[89000] -> 17[49000] via SHM/direct/direct
dss8440-002: worker1:4987:6128 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO Channel 00 : 16[48000] -> 15[13000] via SHM/direct/direct
dss8440-003: worker2:61446:62588 [3] NCCL INFO Channel 00 : 17[49000] -> 16[48000] via SHM/direct/direct
dss8440-003: worker2:61446:62588 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62588 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61445:62592 [2] NCCL INFO Channel 01 : 16[48000] -> 15[13000] via SHM/direct/direct
dss8440-002: worker1:4987:6128 [3] NCCL INFO Channel 01 : 10[49000] -> 9[48000] via SHM/direct/direct
dss8440-003: worker2:61446:62588 [3] NCCL INFO Channel 01 : 17[49000] -> 16[48000] via SHM/direct/direct
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Channel 00 : 12[8a000] -> 11[89000] via SHM/direct/direct
dss8440-002: worker1:4989:6131 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6131 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4989:6131 [5] NCCL INFO Channel 01 : 12[8a000] -> 11[89000] via SHM/direct/direct
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO Channel 00 : 19[8a000] -> 18[89000] via SHM/direct/direct
dss8440-003: worker2:61448:62593 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62593 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61448:62593 [5] NCCL INFO Channel 01 : 19[8a000] -> 18[89000] via SHM/direct/direct
dss8440-003: worker2:61443:62589 [0] NCCL INFO Connected all rings
dss8440-001: master:13934:15089 [0] NCCL INFO Connected all rings
dss8440-003: worker2:61446:62588 [3] NCCL INFO Connected all trees
dss8440-003: worker2:61446:62588 [3] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61446:62588 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15090 [5] NCCL INFO Channel 00 : 5[8a000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13939:15090 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15090 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13936:15091 [2] NCCL INFO Channel 00 : 2[48000] -> 1[13000] via SHM/direct/direct
dss8440-001: master:13939:15090 [5] NCCL INFO Channel 01 : 5[8a000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13937:15093 [3] NCCL INFO Channel 00 : 3[49000] -> 2[48000] via SHM/direct/direct
dss8440-001: master:13937:15093 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15093 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13936:15091 [2] NCCL INFO Channel 01 : 2[48000] -> 1[13000] via SHM/direct/direct
dss8440-002: worker1:4984:6133 [0] NCCL INFO Connected all rings
dss8440-001: master:13938:15094 [4] NCCL INFO Channel 00 : 4[89000] -> 3[49000] via SHM/direct/direct
dss8440-001: master:13937:15093 [3] NCCL INFO Channel 01 : 3[49000] -> 2[48000] via SHM/direct/direct
dss8440-001: master:13938:15094 [4] NCCL INFO Channel 01 : 4[89000] -> 3[49000] via SHM/direct/direct
dss8440-002: worker1:4990:6135 [6] NCCL INFO Connected all trees
dss8440-002: worker1:4990:6135 [6] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4990:6135 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4986:6129 [2] NCCL INFO Connected all trees
dss8440-002: worker1:4986:6129 [2] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4986:6129 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4989:6131 [5] NCCL INFO Connected all trees
dss8440-002: worker1:4989:6131 [5] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4989:6131 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4988:6132 [4] NCCL INFO Connected all trees
dss8440-002: worker1:4988:6132 [4] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61449:62587 [6] NCCL INFO Connected all trees
dss8440-003: worker2:61449:62587 [6] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61449:62587 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4988:6132 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61447:62591 [4] NCCL INFO Connected all trees
dss8440-003: worker2:61447:62591 [4] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61447:62591 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61448:62593 [5] NCCL INFO Connected all trees
dss8440-003: worker2:61448:62593 [5] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61448:62593 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4987:6128 [3] NCCL INFO Connected all trees
dss8440-002: worker1:4987:6128 [3] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4987:6128 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13940:15092 [6] NCCL INFO Connected all trees
dss8440-001: master:13940:15092 [6] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13940:15092 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13939:15090 [5] NCCL INFO Connected all trees
dss8440-001: master:13939:15090 [5] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13939:15090 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13938:15094 [4] NCCL INFO Connected all trees
dss8440-001: master:13938:15094 [4] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13938:15094 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13937:15093 [3] NCCL INFO Connected all trees
dss8440-001: master:13937:15093 [3] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13937:15093 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 00/0 : 14[12000] -> 0[12000] [send] via NET/Socket/0
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 00/0 : 14[12000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 01/0 : 0[12000] -> 7[12000] [send] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 01/0 : 0[12000] -> 7[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 01/0 : 14[12000] -> 1[13000] [send] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 00/0 : 7[12000] -> 15[13000] [send] via NET/Socket/0
dss8440-001: master:13935:15095 [1] NCCL INFO Channel 01/0 : 1[13000] -> 14[12000] [send] via NET/Socket/0
dss8440-003: worker2:61444:62590 [1] NCCL INFO Channel 00/0 : 15[13000] -> 7[12000] [send] via NET/Socket/0
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 01/0 : 1[13000] -> 14[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 00/0 : 15[13000] -> 7[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 01/0 : 7[12000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15089 [0] NCCL INFO Channel 00/0 : 0[12000] -> 14[12000] [send] via NET/Socket/0
dss8440-003: worker2:61443:62589 [0] NCCL INFO Channel 00/0 : 0[12000] -> 14[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO Channel 01/0 : 7[12000] -> 0[12000] [send] via NET/Socket/0
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6133 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6133 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62589 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62589 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO Channel 00 : 15[13000] -> 14[12000] via SHM/direct/direct
dss8440-003: worker2:61444:62590 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62590 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62590 [1] NCCL INFO Channel 01 : 15[13000] -> 14[12000] via SHM/direct/direct
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15095 [1] NCCL INFO Channel 00 : 1[13000] -> 0[12000] via SHM/direct/direct
dss8440-001: master:13935:15095 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15095 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15095 [1] NCCL INFO Channel 01 : 1[13000] -> 0[12000] via SHM/direct/direct
dss8440-002: worker1:4984:6133 [0] NCCL INFO Connected all trees
dss8440-002: worker1:4984:6133 [0] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4984:6133 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61443:62589 [0] NCCL INFO Connected all trees
dss8440-003: worker2:61443:62589 [0] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61443:62589 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13934:15089 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15089 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13936:15091 [2] NCCL INFO Connected all trees
dss8440-001: master:13936:15091 [2] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13936:15091 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61444:62590 [1] NCCL INFO Connected all trees
dss8440-003: worker2:61444:62590 [1] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61444:62590 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61445:62592 [2] NCCL INFO Connected all trees
dss8440-003: worker2:61445:62592 [2] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61445:62592 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13934:15089 [0] NCCL INFO Connected all trees
dss8440-001: master:13934:15089 [0] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13934:15089 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4985:6134 [1] NCCL INFO Connected all trees
dss8440-002: worker1:4985:6134 [1] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4985:6134 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13935:15095 [1] NCCL INFO Connected all trees
dss8440-001: master:13935:15095 [1] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13935:15095 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4984:6133 [0] NCCL INFO comm 0x3e188a20 rank 7 nranks 21 cudaDev 0 busId 12000 - Init COMPLETE
dss8440-003: worker2:61443:62589 [0] NCCL INFO comm 0x3dc62970 rank 14 nranks 21 cudaDev 0 busId 12000 - Init COMPLETE
dss8440-003: worker2:61444:62590 [1] NCCL INFO comm 0x3d1f0410 rank 15 nranks 21 cudaDev 1 busId 13000 - Init COMPLETE
dss8440-002: worker1:4985:6134 [1] NCCL INFO comm 0x3ddd9050 rank 8 nranks 21 cudaDev 1 busId 13000 - Init COMPLETE
dss8440-002: worker1:4990:6135 [6] NCCL INFO comm 0x3ec168f0 rank 13 nranks 21 cudaDev 6 busId c1000 - Init COMPLETE
dss8440-002: worker1:4988:6132 [4] NCCL INFO comm 0x3f6c5710 rank 11 nranks 21 cudaDev 4 busId 89000 - Init COMPLETE
dss8440-002: worker1:4987:6128 [3] NCCL INFO comm 0x3e987a80 rank 10 nranks 21 cudaDev 3 busId 49000 - Init COMPLETE
dss8440-002: worker1:4986:6129 [2] NCCL INFO comm 0x3df5d290 rank 9 nranks 21 cudaDev 2 busId 48000 - Init COMPLETE
dss8440-002: worker1:4989:6131 [5] NCCL INFO comm 0x3ede20f0 rank 12 nranks 21 cudaDev 5 busId 8a000 - Init COMPLETE
dss8440-001: master:13934:15089 [0] NCCL INFO comm 0x3e1243f0 rank 0 nranks 21 cudaDev 0 busId 12000 - Init COMPLETE
dss8440-003: worker2:61449:62587 [6] NCCL INFO comm 0x3cb70570 rank 20 nranks 21 cudaDev 6 busId c1000 - Init COMPLETE
dss8440-003: worker2:61447:62591 [4] NCCL INFO comm 0x3e2cc950 rank 18 nranks 21 cudaDev 4 busId 89000 - Init COMPLETE
dss8440-003: worker2:61446:62588 [3] NCCL INFO comm 0x3e77aea0 rank 17 nranks 21 cudaDev 3 busId 49000 - Init COMPLETE
dss8440-003: worker2:61445:62592 [2] NCCL INFO comm 0x3f03c1d0 rank 16 nranks 21 cudaDev 2 busId 48000 - Init COMPLETE
dss8440-003: worker2:61448:62593 [5] NCCL INFO comm 0x3f4c0e60 rank 19 nranks 21 cudaDev 5 busId 8a000 - Init COMPLETE
dss8440-001: master:13936:15091 [2] NCCL INFO comm 0x3e795140 rank 2 nranks 21 cudaDev 2 busId 48000 - Init COMPLETE
dss8440-001: master:13938:15094 [4] NCCL INFO comm 0x3eb64d90 rank 4 nranks 21 cudaDev 4 busId 89000 - Init COMPLETE
dss8440-001: master:13937:15093 [3] NCCL INFO comm 0x40807980 rank 3 nranks 21 cudaDev 3 busId 49000 - Init COMPLETE
dss8440-001: master:13940:15092 [6] NCCL INFO comm 0x3f11c070 rank 6 nranks 21 cudaDev 6 busId c1000 - Init COMPLETE
dss8440-001: [2023-07-27 21:50:59,540] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
dss8440-001: master:13939:15090 [5] NCCL INFO comm 0x3dcc3500 rank 5 nranks 21 cudaDev 5 busId 8a000 - Init COMPLETE
dss8440-001: master:13935:15095 [1] NCCL INFO comm 0x3f3b9480 rank 1 nranks 21 cudaDev 1 busId 13000 - Init COMPLETE
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-003: Detected CUDA files, patching ldflags
dss8440-003: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-003: Building extension module cpu_adam...
dss8440-003: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-003: ninja: no work to do.
dss8440-003: Loading extension module cpu_adam...
dss8440-003: Time to load cpu_adam op: 2.3786096572875977 seconds
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-001: Detected CUDA files, patching ldflags
dss8440-001: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-001: Building extension module cpu_adam...
dss8440-001: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-001: ninja: no work to do.
dss8440-001: Loading extension module cpu_adam...
dss8440-001: Time to load cpu_adam op: 2.397744655609131 seconds
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-003: Detected CUDA files, patching ldflags
dss8440-003: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-003: Building extension module cpu_adam...
dss8440-003: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-003: ninja: no work to do.
dss8440-003: Loading extension module cpu_adam...
dss8440-003: Time to load cpu_adam op: 2.3656198978424072 seconds
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Detected CUDA files, patching ldflags
dss8440-002: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-002: Building extension module cpu_adam...
dss8440-002: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-002: ninja: no work to do.
dss8440-002: Loading extension module cpu_adam...
dss8440-002: Time to load cpu_adam op: 2.411320686340332 seconds
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-003: Detected CUDA files, patching ldflags
dss8440-003: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-001: Detected CUDA files, patching ldflags
dss8440-001: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-003: Building extension module cpu_adam...
dss8440-003: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-001: Building extension module cpu_adam...
dss8440-001: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-003: ninja: no work to do.
dss8440-003: Loading extension module cpu_adam...
dss8440-002: Detected CUDA files, patching ldflags
dss8440-002: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-001: ninja: no work to do.
dss8440-003: Time to load cpu_adam op: 2.3622777462005615 seconds
dss8440-001: Loading extension module cpu_adam...
dss8440-002: Building extension module cpu_adam...
dss8440-002: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-001: Time to load cpu_adam op: 2.4292984008789062 seconds
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: ninja: no work to do.
dss8440-002: Loading extension module cpu_adam...
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Time to load cpu_adam op: 2.4144699573516846 seconds
dss8440-003: Detected CUDA files, patching ldflags
dss8440-003: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-003: Building extension module cpu_adam...
dss8440-003: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-001: Detected CUDA files, patching ldflags
dss8440-001: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-001: Building extension module cpu_adam...
dss8440-001: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-003: ninja: no work to do.
dss8440-003: Loading extension module cpu_adam...
dss8440-003: Time to load cpu_adam op: 2.3633620738983154 seconds
dss8440-001: ninja: no work to do.
dss8440-003: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-001: Loading extension module cpu_adam...
dss8440-001: Time to load cpu_adam op: 2.4477717876434326 seconds
dss8440-003: Detected CUDA files, patching ldflags
dss8440-003: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-003: Building extension module cpu_adam...
dss8440-003: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-003: ninja: no work to do.
dss8440-003: Loading extension module cpu_adam...
dss8440-003: Loading extension module cpu_adam...
dss8440-003: Time to load cpu_adam op: 2.3741421699523926 seconds
dss8440-003: Time to load cpu_adam op: 2.5996036529541016 seconds
dss8440-003: Loading extension module cpu_adam...
dss8440-003: Time to load cpu_adam op: 2.448988199234009 seconds
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Detected CUDA files, patching ldflags
dss8440-002: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Building extension module cpu_adam...
dss8440-002: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-002: ninja: no work to do.
dss8440-002: Loading extension module cpu_adam...
dss8440-002: Time to load cpu_adam op: 2.503223180770874 seconds
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-001: Detected CUDA files, patching ldflags
dss8440-001: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-001: Building extension module cpu_adam...
dss8440-001: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-001: ninja: no work to do.
dss8440-001: Loading extension module cpu_adam...
dss8440-001: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-001: Time to load cpu_adam op: 2.514291524887085 seconds
dss8440-001: Detected CUDA files, patching ldflags
dss8440-001: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-001: Building extension module cpu_adam...
dss8440-001: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-001: ninja: no work to do.
dss8440-001: Loading extension module cpu_adam...
dss8440-001: Time to load cpu_adam op: 2.5143649578094482 seconds
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Using /home/lanyun/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...
dss8440-002: Detected CUDA files, patching ldflags
dss8440-002: Emitting ninja build file /home/lanyun/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...
dss8440-002: Building extension module cpu_adam...
dss8440-002: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
dss8440-002: ninja: no work to do.
dss8440-002: Loading extension module cpu_adam...
dss8440-002: Time to load cpu_adam op: 2.5962073802948 seconds
dss8440-001: Loading extension module cpu_adam...
dss8440-001: Time to load cpu_adam op: 2.6342058181762695 seconds
dss8440-001: Loading extension module cpu_adam...
dss8440-001: Time to load cpu_adam op: 2.5274035930633545 seconds
dss8440-002: Loading extension module cpu_adam...
dss8440-002: Time to load cpu_adam op: 2.670999765396118 seconds
dss8440-002: Loading extension module cpu_adam...
dss8440-002: Time to load cpu_adam op: 2.6015164852142334 seconds
dss8440-002: Loading extension module cpu_adam...
dss8440-002: Time to load cpu_adam op: 2.6977596282958984 seconds
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Rank: 20 partition count [21] and sizes[(778342, False)] 
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Rank: 17 partition count [21] and sizes[(778342, False)] 
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Rank: 16 partition count [21] and sizes[(778342, False)] 
dss8440-001: Rank: 2 partition count [21] and sizes[(778342, False)] 
dss8440-002: Rank: 13 partition count [21] and sizes[(778342, False)] 
dss8440-003: Rank: 19 partition count [21] and sizes[(778342, False)] 
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-003: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-002: Rank: 12 partition count [21] and sizes[(778342, False)] 
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: Rank: 5 partition count [21] and sizes[(778342, False)] 
dss8440-001: Rank: 4 partition count [21] and sizes[(778342, False)] 
dss8440-002: Rank: 7 partition count [21] and sizes[(778342, False)] 
dss8440-001: Rank: 1 partition count [21] and sizes[(778342, False)] 
dss8440-003: Rank: 15 partition count [21] and sizes[(778342, False)] 
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-003: Rank: 14 partition count [21] and sizes[(778342, False)] 
dss8440-003: Rank: 18 partition count [21] and sizes[(778342, False)] 
dss8440-001: Rank: 6 partition count [21] and sizes[(778342, False)] 
dss8440-001: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-001: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: [2023-07-27 21:51:05,584] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
dss8440-001: Rank: 3 partition count [21] and sizes[(778342, False)] 
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
dss8440-002: Rank: 8 partition count [21] and sizes[(778342, False)] 
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
dss8440-001: [2023-07-27 21:51:05,587] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-001: Rank: 0 partition count [21] and sizes[(778342, False)] 
dss8440-001: master:13934:15385 [0] NCCL INFO Using network Socket
dss8440-001: master:13940:15388 [6] NCCL INFO Using network Socket
dss8440-001: master:13935:15389 [1] NCCL INFO Using network Socket
dss8440-003: worker2:61449:62880 [6] NCCL INFO Using network Socket
dss8440-003: worker2:61447:62882 [4] NCCL INFO Using network Socket
dss8440-003: worker2:61446:62883 [3] NCCL INFO Using network Socket
dss8440-003: worker2:61444:62884 [1] NCCL INFO Using network Socket
dss8440-003: worker2:61443:62885 [0] NCCL INFO Using network Socket
dss8440-003: worker2:61448:62881 [5] NCCL INFO Using network Socket
dss8440-003: worker2:61445:62886 [2] NCCL INFO Using network Socket
dss8440-002: worker1:4989:6418 [5] NCCL INFO Using network Socket
dss8440-002: worker1:4990:6420 [6] NCCL INFO Using network Socket
dss8440-002: worker1:4984:6419 [0] NCCL INFO Using network Socket
dss8440-002: worker1:4985:6421 [1] NCCL INFO Using network Socket
dss8440-001: master:13939:15390 [5] NCCL INFO Using network Socket
dss8440-001: master:13937:15391 [3] NCCL INFO Using network Socket
dss8440-001: master:13938:15386 [4] NCCL INFO Using network Socket
dss8440-001: master:13936:15387 [2] NCCL INFO Using network Socket
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-002: Adam Optimizer #0 is created with AVX512 arithmetic capability.
dss8440-002: Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
dss8440-002: Rank: 10 partition count [21] and sizes[(778342, False)] 
dss8440-002: worker1:4987:6422 [3] NCCL INFO Using network Socket
dss8440-002: Rank: 11 partition count [21] and sizes[(778342, False)] 
dss8440-002: worker1:4988:6423 [4] NCCL INFO Using network Socket
dss8440-002: Rank: 9 partition count [21] and sizes[(778342, False)] 
dss8440-002: worker1:4986:6424 [2] NCCL INFO Using network Socket
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO Setting affinity for GPU 5 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Setting affinity for GPU 0 to 555555,55555555,55555555
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Setting affinity for GPU 4 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO Setting affinity for GPU 3 to 555555,55555555,55555555
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Setting affinity for GPU 2 to 555555,55555555,55555555
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Setting affinity for GPU 1 to 555555,55555555,55555555
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4990:6420 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO Setting affinity for GPU 6 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13937:15391 [3] NCCL INFO Setting affinity for GPU 3 to 555555,55555555,55555555
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13936:15387 [2] NCCL INFO Setting affinity for GPU 2 to 555555,55555555,55555555
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15389 [1] NCCL INFO Setting affinity for GPU 1 to 555555,55555555,55555555
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13934:15385 [0] NCCL INFO Setting affinity for GPU 0 to 555555,55555555,55555555
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15390 [5] NCCL INFO Setting affinity for GPU 5 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15386 [4] NCCL INFO Setting affinity for GPU 4 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13940:15388 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13940:15388 [6] NCCL INFO Setting affinity for GPU 6 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61445:62886 [2] NCCL INFO Setting affinity for GPU 2 to 555555,55555555,55555555
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO Setting affinity for GPU 3 to 555555,55555555,55555555
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO Setting affinity for GPU 0 to 555555,55555555,55555555
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Setting affinity for GPU 1 to 555555,55555555,55555555
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO Setting affinity for GPU 5 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61449:62880 [6] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO Setting affinity for GPU 6 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO Setting affinity for GPU 4 to aaaaaa,aaaaaaaa,aaaaaaaa
dss8440-001: master:13938:15386 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
dss8440-001: master:13936:15387 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
dss8440-001: master:13935:15389 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/14/-1->1->0
dss8440-001: master:13937:15391 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
dss8440-001: master:13940:15388 [6] NCCL INFO Trees [0] -1/-1/-1->6->5 [1] -1/-1/-1->6->5
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
dss8440-001: master:13939:15390 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
dss8440-001: master:13934:15385 [0] NCCL INFO Trees [0] 1/14/-1->0->-1 [1] 1/-1/-1->0->7
dss8440-002: worker1:4990:6420 [6] NCCL INFO Trees [0] -1/-1/-1->13->12 [1] -1/-1/-1->13->12
dss8440-003: worker2:61446:62883 [3] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/-1/-1->17->16
dss8440-002: worker1:4989:6418 [5] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11
dss8440-003: worker2:61449:62880 [6] NCCL INFO Trees [0] -1/-1/-1->20->19 [1] -1/-1/-1->20->19
dss8440-002: worker1:4988:6423 [4] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10
dss8440-002: worker1:4987:6422 [3] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
dss8440-002: worker1:4986:6424 [2] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8
dss8440-002: worker1:4985:6421 [1] NCCL INFO Trees [0] 9/-1/-1->8->7 [1] 9/-1/-1->8->7
dss8440-002: worker1:4984:6419 [0] NCCL INFO Trees [0] 8/-1/-1->7->15 [1] 8/0/-1->7->-1
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 17/-1/-1->16->15
dss8440-003: worker2:61448:62881 [5] NCCL INFO Trees [0] 20/-1/-1->19->18 [1] 20/-1/-1->19->18
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17
dss8440-003: worker2:61444:62884 [1] NCCL INFO Trees [0] 16/7/-1->15->14 [1] 16/-1/-1->15->14
dss8440-003: worker2:61443:62885 [0] NCCL INFO Trees [0] 15/-1/-1->14->0 [1] 15/-1/-1->14->1
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 00/0 : 13[c1000] -> 14[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 01/0 : 13[c1000] -> 14[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 00/0 : 6[c1000] -> 7[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 01/0 : 6[c1000] -> 7[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 00 : 14[12000] -> 15[13000] via SHM/direct/direct
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 00 : 7[12000] -> 8[13000] via SHM/direct/direct
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 01 : 14[12000] -> 15[13000] via SHM/direct/direct
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 01 : 7[12000] -> 8[13000] via SHM/direct/direct
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 00/0 : 20[c1000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 01/0 : 20[c1000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 00 : 0[12000] -> 1[13000] via SHM/direct/direct
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 01 : 0[12000] -> 1[13000] via SHM/direct/direct
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15386 [4] NCCL INFO Channel 00 : 4[89000] -> 5[8a000] via SHM/direct/direct
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15386 [4] NCCL INFO Channel 01 : 4[89000] -> 5[8a000] via SHM/direct/direct
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Channel 00 : 11[89000] -> 12[8a000] via SHM/direct/direct
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Channel 01 : 11[89000] -> 12[8a000] via SHM/direct/direct
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13935:15389 [1] NCCL INFO Channel 00 : 1[13000] -> 2[48000] via SHM/direct/direct
dss8440-001: master:13937:15391 [3] NCCL INFO Channel 00 : 3[49000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13935:15389 [1] NCCL INFO Channel 01 : 1[13000] -> 2[48000] via SHM/direct/direct
dss8440-001: master:13936:15387 [2] NCCL INFO Channel 00 : 2[48000] -> 3[49000] via SHM/direct/direct
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-001: master:13937:15391 [3] NCCL INFO Channel 01 : 3[49000] -> 4[89000] via SHM/direct/direct
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Channel 01 : 2[48000] -> 3[49000] via SHM/direct/direct
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO Channel 00 : 10[49000] -> 11[89000] via SHM/direct/direct
dss8440-002: worker1:4986:6424 [2] NCCL INFO Channel 00 : 9[48000] -> 10[49000] via SHM/direct/direct
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO Channel 00 : 8[13000] -> 9[48000] via SHM/direct/direct
dss8440-002: worker1:4987:6422 [3] NCCL INFO Channel 01 : 10[49000] -> 11[89000] via SHM/direct/direct
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO Channel 01 : 9[48000] -> 10[49000] via SHM/direct/direct
dss8440-003: worker2:61448:62881 [5] NCCL INFO Channel 00 : 19[8a000] -> 20[c1000] via SHM/direct/direct
dss8440-002: worker1:4985:6421 [1] NCCL INFO Channel 01 : 8[13000] -> 9[48000] via SHM/direct/direct
dss8440-003: worker2:61447:62882 [4] NCCL INFO Channel 00 : 18[89000] -> 19[8a000] via SHM/direct/direct
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO Channel 00 : 15[13000] -> 16[48000] via SHM/direct/direct
dss8440-003: worker2:61445:62886 [2] NCCL INFO Channel 00 : 16[48000] -> 17[49000] via SHM/direct/direct
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO Channel 00 : 17[49000] -> 18[89000] via SHM/direct/direct
dss8440-003: worker2:61448:62881 [5] NCCL INFO Channel 01 : 19[8a000] -> 20[c1000] via SHM/direct/direct
dss8440-003: worker2:61447:62882 [4] NCCL INFO Channel 01 : 18[89000] -> 19[8a000] via SHM/direct/direct
dss8440-003: worker2:61444:62884 [1] NCCL INFO Channel 01 : 15[13000] -> 16[48000] via SHM/direct/direct
dss8440-003: worker2:61445:62886 [2] NCCL INFO Channel 01 : 16[48000] -> 17[49000] via SHM/direct/direct
dss8440-003: worker2:61446:62883 [3] NCCL INFO Channel 01 : 17[49000] -> 18[89000] via SHM/direct/direct
dss8440-002: worker1:4989:6418 [5] NCCL INFO Channel 00 : 12[8a000] -> 13[c1000] via SHM/direct/direct
dss8440-002: worker1:4989:6418 [5] NCCL INFO Channel 01 : 12[8a000] -> 13[c1000] via SHM/direct/direct
dss8440-001: master:13939:15390 [5] NCCL INFO Channel 00 : 5[8a000] -> 6[c1000] via SHM/direct/direct
dss8440-001: master:13939:15390 [5] NCCL INFO Channel 01 : 5[8a000] -> 6[c1000] via SHM/direct/direct
dss8440-001: master:13935:15389 [1] NCCL INFO Connected all rings
dss8440-001: master:13936:15387 [2] NCCL INFO Connected all rings
dss8440-001: master:13937:15391 [3] NCCL INFO Connected all rings
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4990:6420 [6] NCCL INFO Channel 00/0 : 13[c1000] -> 14[12000] [send] via NET/Socket/0
dss8440-002: worker1:4990:6420 [6] NCCL INFO Channel 01/0 : 13[c1000] -> 14[12000] [send] via NET/Socket/0
dss8440-002: worker1:4986:6424 [2] NCCL INFO Connected all rings
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO Connected all rings
dss8440-002: worker1:4985:6421 [1] NCCL INFO Connected all rings
dss8440-003: worker2:61444:62884 [1] NCCL INFO Connected all rings
dss8440-003: worker2:61445:62886 [2] NCCL INFO Connected all rings
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO Connected all rings
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Connected all rings
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Connected all rings
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-003: worker2:61449:62880 [6] NCCL INFO Channel 00/0 : 20[c1000] -> 0[12000] [send] via NET/Socket/0
dss8440-003: worker2:61449:62880 [6] NCCL INFO Channel 01/0 : 20[c1000] -> 0[12000] [send] via NET/Socket/0
dss8440-002: worker1:4989:6418 [5] NCCL INFO Connected all rings
dss8440-001: master:13938:15386 [4] NCCL INFO Connected all rings
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13940:15388 [6] NCCL INFO Channel 00/0 : 6[c1000] -> 7[12000] [send] via NET/Socket/0
dss8440-001: master:13940:15388 [6] NCCL INFO Channel 01/0 : 6[c1000] -> 7[12000] [send] via NET/Socket/0
dss8440-003: worker2:61448:62881 [5] NCCL INFO Connected all rings
dss8440-001: master:13936:15387 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4986:6424 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO Channel 00/0 : 7[12000] -> 15[13000] [receive] via NET/Socket/0
dss8440-001: master:13935:15389 [1] NCCL INFO Channel 01/0 : 14[12000] -> 1[13000] [receive] via NET/Socket/0
dss8440-003: worker2:61445:62886 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61445:62886 [2] NCCL INFO Could not enable P2P between dev 2(=48000) and dev 3(=49000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61447:62882 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4988:6423 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13938:15386 [4] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13938:15386 [4] NCCL INFO Could not enable P2P between dev 4(=89000) and dev 5(=8a000)
dss8440-001: master:13939:15390 [5] NCCL INFO Connected all rings
dss8440-002: worker1:4990:6420 [6] NCCL INFO Connected all rings
dss8440-002: worker1:4990:6420 [6] NCCL INFO Channel 00 : 13[c1000] -> 12[8a000] via SHM/direct/direct
dss8440-002: worker1:4990:6420 [6] NCCL INFO Channel 01 : 13[c1000] -> 12[8a000] via SHM/direct/direct
dss8440-003: worker2:61449:62880 [6] NCCL INFO Connected all rings
dss8440-003: worker2:61449:62880 [6] NCCL INFO Channel 00 : 20[c1000] -> 19[8a000] via SHM/direct/direct
dss8440-003: worker2:61449:62880 [6] NCCL INFO Channel 01 : 20[c1000] -> 19[8a000] via SHM/direct/direct
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO Channel 00 : 10[49000] -> 9[48000] via SHM/direct/direct
dss8440-002: worker1:4987:6422 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4987:6422 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-002: worker1:4986:6424 [2] NCCL INFO Channel 00 : 9[48000] -> 8[13000] via SHM/direct/direct
dss8440-002: worker1:4985:6421 [1] NCCL INFO Channel 00 : 8[13000] -> 7[12000] via SHM/direct/direct
dss8440-002: worker1:4985:6421 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4985:6421 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-002: worker1:4987:6422 [3] NCCL INFO Channel 01 : 10[49000] -> 9[48000] via SHM/direct/direct
dss8440-002: worker1:4986:6424 [2] NCCL INFO Channel 01 : 9[48000] -> 8[13000] via SHM/direct/direct
dss8440-002: worker1:4985:6421 [1] NCCL INFO Channel 01 : 8[13000] -> 7[12000] via SHM/direct/direct
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13936:15387 [2] NCCL INFO Channel 00 : 2[48000] -> 1[13000] via SHM/direct/direct
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61447:62882 [4] NCCL INFO Channel 00 : 18[89000] -> 17[49000] via SHM/direct/direct
dss8440-001: master:13936:15387 [2] NCCL INFO Channel 01 : 2[48000] -> 1[13000] via SHM/direct/direct
dss8440-003: worker2:61445:62886 [2] NCCL INFO Channel 00 : 16[48000] -> 15[13000] via SHM/direct/direct
dss8440-003: worker2:61447:62882 [4] NCCL INFO Channel 01 : 18[89000] -> 17[49000] via SHM/direct/direct
dss8440-001: master:13937:15391 [3] NCCL INFO Channel 00 : 3[49000] -> 2[48000] via SHM/direct/direct
dss8440-003: worker2:61445:62886 [2] NCCL INFO Channel 01 : 16[48000] -> 15[13000] via SHM/direct/direct
dss8440-001: master:13937:15391 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13937:15391 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO Connected all rings
dss8440-001: master:13937:15391 [3] NCCL INFO Channel 01 : 3[49000] -> 2[48000] via SHM/direct/direct
dss8440-001: master:13940:15388 [6] NCCL INFO Connected all rings
dss8440-001: master:13940:15388 [6] NCCL INFO Channel 00 : 6[c1000] -> 5[8a000] via SHM/direct/direct
dss8440-001: master:13940:15388 [6] NCCL INFO Channel 01 : 6[c1000] -> 5[8a000] via SHM/direct/direct
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Channel 00 : 11[89000] -> 10[49000] via SHM/direct/direct
dss8440-002: worker1:4989:6418 [5] NCCL INFO Channel 00 : 12[8a000] -> 11[89000] via SHM/direct/direct
dss8440-002: worker1:4989:6418 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4989:6418 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-002: worker1:4988:6423 [4] NCCL INFO Channel 01 : 11[89000] -> 10[49000] via SHM/direct/direct
dss8440-002: worker1:4989:6418 [5] NCCL INFO Channel 01 : 12[8a000] -> 11[89000] via SHM/direct/direct
dss8440-001: master:13938:15386 [4] NCCL INFO Channel 00 : 4[89000] -> 3[49000] via SHM/direct/direct
dss8440-001: master:13938:15386 [4] NCCL INFO Channel 01 : 4[89000] -> 3[49000] via SHM/direct/direct
dss8440-002: worker1:4986:6424 [2] NCCL INFO Connected all trees
dss8440-002: worker1:4986:6424 [2] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4986:6424 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13934:15385 [0] NCCL INFO Connected all rings
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO Channel 00 : 17[49000] -> 16[48000] via SHM/direct/direct
dss8440-003: worker2:61446:62883 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61446:62883 [3] NCCL INFO Could not enable P2P between dev 3(=49000) and dev 2(=48000)
dss8440-003: worker2:61448:62881 [5] NCCL INFO Channel 00 : 19[8a000] -> 18[89000] via SHM/direct/direct
dss8440-003: worker2:61448:62881 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61448:62881 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-003: worker2:61446:62883 [3] NCCL INFO Channel 01 : 17[49000] -> 16[48000] via SHM/direct/direct
dss8440-003: worker2:61448:62881 [5] NCCL INFO Channel 01 : 19[8a000] -> 18[89000] via SHM/direct/direct
dss8440-001: master:13937:15391 [3] NCCL INFO Connected all trees
dss8440-001: master:13937:15391 [3] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13937:15391 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4990:6420 [6] NCCL INFO Connected all trees
dss8440-002: worker1:4990:6420 [6] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4990:6420 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4989:6418 [5] NCCL INFO Connected all trees
dss8440-002: worker1:4989:6418 [5] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4989:6418 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4987:6422 [3] NCCL INFO Connected all trees
dss8440-002: worker1:4987:6422 [3] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4987:6422 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4988:6423 [4] NCCL INFO Connected all trees
dss8440-002: worker1:4988:6423 [4] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4988:6423 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4984:6419 [0] NCCL INFO Connected all rings
dss8440-003: worker2:61448:62881 [5] NCCL INFO Connected all trees
dss8440-003: worker2:61448:62881 [5] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61448:62881 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61449:62880 [6] NCCL INFO Connected all trees
dss8440-003: worker2:61449:62880 [6] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61449:62880 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61447:62882 [4] NCCL INFO Connected all trees
dss8440-003: worker2:61447:62882 [4] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61447:62882 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61446:62883 [3] NCCL INFO Connected all trees
dss8440-003: worker2:61446:62883 [3] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61446:62883 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15390 [5] NCCL INFO Channel 00 : 5[8a000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13939:15390 [5] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13939:15390 [5] NCCL INFO Could not enable P2P between dev 5(=8a000) and dev 4(=89000)
dss8440-001: master:13939:15390 [5] NCCL INFO Channel 01 : 5[8a000] -> 4[89000] via SHM/direct/direct
dss8440-001: master:13940:15388 [6] NCCL INFO Connected all trees
dss8440-001: master:13940:15388 [6] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13940:15388 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13939:15390 [5] NCCL INFO Connected all trees
dss8440-001: master:13939:15390 [5] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13939:15390 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13938:15386 [4] NCCL INFO Connected all trees
dss8440-001: master:13938:15386 [4] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13938:15386 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 00/0 : 14[12000] -> 0[12000] [send] via NET/Socket/0
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 00/0 : 14[12000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 01/0 : 0[12000] -> 7[12000] [send] via NET/Socket/0
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 01/0 : 0[12000] -> 7[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 01/0 : 14[12000] -> 1[13000] [send] via NET/Socket/0
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 00/0 : 7[12000] -> 15[13000] [send] via NET/Socket/0
dss8440-001: master:13935:15389 [1] NCCL INFO Channel 01/0 : 1[13000] -> 14[12000] [send] via NET/Socket/0
dss8440-003: worker2:61444:62884 [1] NCCL INFO Channel 00/0 : 15[13000] -> 7[12000] [send] via NET/Socket/0
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 01/0 : 1[13000] -> 14[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 01/0 : 7[12000] -> 0[12000] [receive] via NET/Socket/0
dss8440-001: master:13934:15385 [0] NCCL INFO Channel 00/0 : 0[12000] -> 14[12000] [send] via NET/Socket/0
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 00/0 : 15[13000] -> 7[12000] [receive] via NET/Socket/0
dss8440-003: worker2:61443:62885 [0] NCCL INFO Channel 00/0 : 0[12000] -> 14[12000] [receive] via NET/Socket/0
dss8440-002: worker1:4984:6419 [0] NCCL INFO Channel 01/0 : 7[12000] -> 0[12000] [send] via NET/Socket/0
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61443:62885 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61443:62885 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15389 [1] NCCL INFO Channel 00 : 1[13000] -> 0[12000] via SHM/direct/direct
dss8440-001: master:13935:15389 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13935:15389 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-001: master:13935:15389 [1] NCCL INFO Channel 01 : 1[13000] -> 0[12000] via SHM/direct/direct
dss8440-002: worker1:4984:6419 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-002: worker1:4984:6419 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO Channel 00 : 15[13000] -> 14[12000] via SHM/direct/direct
dss8440-003: worker2:61444:62884 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-003: worker2:61444:62884 [1] NCCL INFO Could not enable P2P between dev 1(=13000) and dev 0(=12000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO Channel 01 : 15[13000] -> 14[12000] via SHM/direct/direct
dss8440-001: master:13936:15387 [2] NCCL INFO Connected all trees
dss8440-001: master:13936:15387 [2] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13936:15387 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61443:62885 [0] NCCL INFO Connected all trees
dss8440-003: worker2:61443:62885 [0] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61443:62885 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4984:6419 [0] NCCL INFO Connected all trees
dss8440-002: worker1:4984:6419 [0] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4984:6419 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13934:15385 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
dss8440-001: master:13934:15385 [0] NCCL INFO Could not enable P2P between dev 0(=12000) and dev 1(=13000)
dss8440-003: worker2:61444:62884 [1] NCCL INFO Connected all trees
dss8440-003: worker2:61444:62884 [1] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61444:62884 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4985:6421 [1] NCCL INFO Connected all trees
dss8440-002: worker1:4985:6421 [1] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-002: worker1:4985:6421 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-003: worker2:61445:62886 [2] NCCL INFO Connected all trees
dss8440-003: worker2:61445:62886 [2] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-003: worker2:61445:62886 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13934:15385 [0] NCCL INFO Connected all trees
dss8440-001: master:13934:15385 [0] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13934:15385 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-001: master:13935:15389 [1] NCCL INFO Connected all trees
dss8440-001: master:13935:15389 [1] NCCL INFO threadThresholds 8/8/64 | 168/8/64 | 512 | 512
dss8440-001: master:13935:15389 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
dss8440-002: worker1:4984:6419 [0] NCCL INFO comm 0x72cf2480 rank 7 nranks 21 cudaDev 0 busId 12000 - Init COMPLETE
dss8440-002: worker1:4985:6421 [1] NCCL INFO comm 0x73007520 rank 8 nranks 21 cudaDev 1 busId 13000 - Init COMPLETE
dss8440-002: worker1:4988:6423 [4] NCCL INFO comm 0x746bd880 rank 11 nranks 21 cudaDev 4 busId 89000 - Init COMPLETE
dss8440-003: worker2:61444:62884 [1] NCCL INFO comm 0x706cd1c0 rank 15 nranks 21 cudaDev 1 busId 13000 - Init COMPLETE
dss8440-003: worker2:61443:62885 [0] NCCL INFO comm 0x73211200 rank 14 nranks 21 cudaDev 0 busId 12000 - Init COMPLETE
dss8440-002: worker1:4986:6424 [2] NCCL INFO comm 0x7539ca80 rank 9 nranks 21 cudaDev 2 busId 48000 - Init COMPLETE
dss8440-002: worker1:4990:6420 [6] NCCL INFO comm 0x73855280 rank 13 nranks 21 cudaDev 6 busId c1000 - Init COMPLETE
dss8440-002: worker1:4987:6422 [3] NCCL INFO comm 0x75dca250 rank 10 nranks 21 cudaDev 3 busId 49000 - Init COMPLETE
dss8440-002: worker1:4989:6418 [5] NCCL INFO comm 0x761bec40 rank 12 nranks 21 cudaDev 5 busId 8a000 - Init COMPLETE
dss8440-003: worker2:61445:62886 [2] NCCL INFO comm 0x764e40e0 rank 16 nranks 21 cudaDev 2 busId 48000 - Init COMPLETE
dss8440-003: worker2:61446:62883 [3] NCCL INFO comm 0x70f55260 rank 17 nranks 21 cudaDev 3 busId 49000 - Init COMPLETE
dss8440-003: worker2:61448:62881 [5] NCCL INFO comm 0x76908a00 rank 19 nranks 21 cudaDev 5 busId 8a000 - Init COMPLETE
dss8440-003: worker2:61449:62880 [6] NCCL INFO comm 0x740186b0 rank 20 nranks 21 cudaDev 6 busId c1000 - Init COMPLETE
dss8440-001: master:13934:15385 [0] NCCL INFO comm 0x7556c3c0 rank 0 nranks 21 cudaDev 0 busId 12000 - Init COMPLETE
dss8440-003: worker2:61447:62882 [4] NCCL INFO comm 0x757170c0 rank 18 nranks 21 cudaDev 4 busId 89000 - Init COMPLETE
dss8440-001: master:13936:15387 [2] NCCL INFO comm 0x73f187d0 rank 2 nranks 21 cudaDev 2 busId 48000 - Init COMPLETE
dss8440-001: master:13935:15389 [1] NCCL INFO comm 0x74680a80 rank 1 nranks 21 cudaDev 1 busId 13000 - Init COMPLETE
dss8440-001: master:13938:15386 [4] NCCL INFO comm 0x75fb6340 rank 4 nranks 21 cudaDev 4 busId 89000 - Init COMPLETE
dss8440-001: master:13937:15391 [3] NCCL INFO comm 0x74794300 rank 3 nranks 21 cudaDev 3 busId 49000 - Init COMPLETE
dss8440-001: master:13940:15388 [6] NCCL INFO comm 0x73b6a800 rank 6 nranks 21 cudaDev 6 busId c1000 - Init COMPLETE
dss8440-001: master:13939:15390 [5] NCCL INFO comm 0x7516e3d0 rank 5 nranks 21 cudaDev 5 busId 8a000 - Init COMPLETE
dss8440-001: [2023-07-27 21:51:07,109] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
dss8440-001: [2023-07-27 21:51:07,110] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB 
dss8440-001: [2023-07-27 21:51:07,110] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 30.08 GB, percent = 8.0%
dss8440-001: [2023-07-27 21:51:07,257] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
dss8440-001: [2023-07-27 21:51:07,257] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB 
dss8440-001: [2023-07-27 21:51:07,257] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 30.15 GB, percent = 8.0%
dss8440-001: [2023-07-27 21:51:07,257] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
dss8440-001: [2023-07-27 21:51:07,326] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
dss8440-001: [2023-07-27 21:51:07,327] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB 
dss8440-001: [2023-07-27 21:51:07,327] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 30.14 GB, percent = 8.0%
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [config.py:964:print]   activation_checkpointing_config  {
dss8440-001:     "partition_activations": false, 
dss8440-001:     "contiguous_memory_optimization": false, 
dss8440-001:     "cpu_checkpointing": false, 
dss8440-001:     "number_checkpoints": null, 
dss8440-001:     "synchronize_checkpoint_boundary": false, 
dss8440-001:     "profile": false
dss8440-001: }
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [config.py:964:print]   amp_enabled .................. False
dss8440-001: [2023-07-27 21:51:07,328] [INFO] [config.py:964:print]   amp_params ................... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   autotuning_config ............ {
dss8440-001:     "enabled": false, 
dss8440-001:     "start_step": null, 
dss8440-001:     "end_step": null, 
dss8440-001:     "metric_path": null, 
dss8440-001:     "arg_mappings": null, 
dss8440-001:     "metric": "throughput", 
dss8440-001:     "model_info": null, 
dss8440-001:     "results_dir": "autotuning_results", 
dss8440-001:     "exps_dir": "autotuning_exps", 
dss8440-001:     "overwrite": true, 
dss8440-001:     "fast": true, 
dss8440-001:     "start_profile_step": 3, 
dss8440-001:     "end_profile_step": 5, 
dss8440-001:     "tuner_type": "gridsearch", 
dss8440-001:     "tuner_early_stopping": 5, 
dss8440-001:     "tuner_num_trials": 50, 
dss8440-001:     "model_info_path": null, 
dss8440-001:     "mp_size": 1, 
dss8440-001:     "max_train_batch_size": null, 
dss8440-001:     "min_train_batch_size": 1, 
dss8440-001:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
dss8440-001:     "min_train_micro_batch_size_per_gpu": 1, 
dss8440-001:     "num_tuning_micro_batch_sizes": 3
dss8440-001: }
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9c8eeb0c90>
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   communication_data_type ...... None
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   disable_allgather ............ False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   dump_state ................... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   elasticity_enabled ........... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   flops_profiler_config ........ {
dss8440-001:     "enabled": false, 
dss8440-001:     "recompute_fwd_factor": 0.0, 
dss8440-001:     "profile_step": 1, 
dss8440-001:     "module_depth": -1, 
dss8440-001:     "top_modules": 1, 
dss8440-001:     "detailed": true, 
dss8440-001:     "output_file": null
dss8440-001: }
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   fp16_enabled ................. True
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   global_rank .................. 0
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
dss8440-001: [2023-07-27 21:51:07,329] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   loss_scale ................... 0
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   memory_breakdown ............. False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   mics_shard_size .............. -1
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   nebula_config ................ {
dss8440-001:     "enabled": false, 
dss8440-001:     "persistent_storage_path": null, 
dss8440-001:     "persistent_time_interval": 100, 
dss8440-001:     "num_of_version_in_retention": 2, 
dss8440-001:     "enable_nebula_load": true, 
dss8440-001:     "load_path": null
dss8440-001: }
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   optimizer_name ............... adam
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0001}
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   pld_enabled .................. False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   pld_params ................... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   prescale_gradients ........... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   scheduler_name ............... None
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   scheduler_params ............. None
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   sparse_attention ............. None
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   steps_per_print .............. 10
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   train_batch_size ............. 168
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  8
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   use_node_local_storage ....... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   world_size ................... 21
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   zero_enabled ................. True
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:964:print]   zero_optimization_stage ...... 1
dss8440-001: [2023-07-27 21:51:07,330] [INFO] [config.py:950:print_user_config]   json = {
dss8440-001:     "train_micro_batch_size_per_gpu": 8, 
dss8440-001:     "optimizer": {
dss8440-001:         "type": "Adam", 
dss8440-001:         "params": {
dss8440-001:             "lr": 0.0001
dss8440-001:         }
dss8440-001:     }, 
dss8440-001:     "fp16": {
dss8440-001:         "enabled": true
dss8440-001:     }, 
dss8440-001:     "zero_optimization": {
dss8440-001:         "stage": 1, 
dss8440-001:         "offload_optimizer": {
dss8440-001:             "device": "cpu"
dss8440-001:         }
dss8440-001:     }
dss8440-001: }
dss8440-001: 2023-07-27 21:51:07.330 | INFO     | __main__:log_dist:53 - [Rank 0] DeepSpeed engine created
dss8440-001: 2023-07-27 21:51:07.331 | INFO     | __main__:log_dist:53 - [Rank 0] Total number of model parameters: 16,345,177
dss8440-001: [2023-07-27 21:51:07,443] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
dss8440-001: [2023-07-27 21:51:07,553] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
dss8440-001: [2023-07-27 21:51:07,644] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
dss8440-001: [2023-07-27 21:51:07,731] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
dss8440-001: [2023-07-27 21:51:07,816] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
dss8440-001: [2023-07-27 21:51:07,906] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
dss8440-001: [2023-07-27 21:51:07,995] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
dss8440-001: [2023-07-27 21:51:08,078] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
dss8440-001: [2023-07-27 21:51:08,157] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
dss8440-001: [2023-07-27 21:51:08,244] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
dss8440-001: [2023-07-27 21:51:08,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:08,244] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=2202.303814025155, CurrSamplesPerSec=2206.698835024427, MemAllocated=0.07GB, MaxMemAllocated=1.36GB
dss8440-001: 2023-07-27 21:51:08.245 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.8781
dss8440-001: [2023-07-27 21:51:08,324] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
dss8440-001: [2023-07-27 21:51:08,407] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
dss8440-001: [2023-07-27 21:51:08,490] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
dss8440-001: [2023-07-27 21:51:08,576] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
dss8440-001: [2023-07-27 21:51:08,656] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
dss8440-001: [2023-07-27 21:51:08,737] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
dss8440-001: [2023-07-27 21:51:08,819] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 21:51:10,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:10,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=1035.3402330573067, CurrSamplesPerSec=278.47520315275045, MemAllocated=0.07GB, MaxMemAllocated=1.36GB
dss8440-001: 2023-07-27 21:51:10.661 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.8691
dss8440-001: [2023-07-27 21:51:16,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:16,972] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=516.6401957209479, CurrSamplesPerSec=256.56392551580905, MemAllocated=0.07GB, MaxMemAllocated=1.36GB
dss8440-001: 2023-07-27 21:51:16.972 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.7844
dss8440-001: [2023-07-27 21:51:23,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:23,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=421.7024740078669, CurrSamplesPerSec=284.4112417509233, MemAllocated=0.07GB, MaxMemAllocated=1.55GB
dss8440-001: 2023-07-27 21:51:23.124 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.6674
dss8440-001: [2023-07-27 21:51:29,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:29,352] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=379.48166745636365, CurrSamplesPerSec=282.47970106927625, MemAllocated=0.07GB, MaxMemAllocated=1.55GB
dss8440-001: 2023-07-27 21:51:29.353 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.5606
dss8440-001: [2023-07-27 21:51:35,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:35,643] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=355.33248254941515, CurrSamplesPerSec=273.7503849015262, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:51:35.643 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.4379
dss8440-001: [2023-07-27 21:51:41,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:41,863] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=340.6824361995011, CurrSamplesPerSec=287.71510840716996, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:51:41.864 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.3126
dss8440-001: [2023-07-27 21:51:48,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:48,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=329.7282577550092, CurrSamplesPerSec=259.3906760531442, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:51:48.185 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.1817
dss8440-001: [2023-07-27 21:51:54,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:51:54,548] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=321.4041096717951, CurrSamplesPerSec=283.30377788990586, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:51:54.548 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 10.0570
dss8440-001: [2023-07-27 21:52:00,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:00,850] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=315.47848567357835, CurrSamplesPerSec=259.499575751862, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:52:00.851 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.9291
dss8440-001: [2023-07-27 21:52:06,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:07,024] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=311.4858046879643, CurrSamplesPerSec=267.42494626969557, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:52:07.024 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.8059
dss8440-001: [2023-07-27 21:52:13,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:13,219] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=308.1277711930388, CurrSamplesPerSec=282.59559456949114, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:52:13.219 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.6808
dss8440-001: [2023-07-27 21:52:19,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:19,317] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=305.77057772921694, CurrSamplesPerSec=264.1521657391666, MemAllocated=0.07GB, MaxMemAllocated=1.64GB
dss8440-001: 2023-07-27 21:52:19.317 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.5522
dss8440-001: [2023-07-27 21:52:25,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:25,483] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=303.54042812451615, CurrSamplesPerSec=284.36269160317727, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:52:25.483 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.4516
dss8440-001: [2023-07-27 21:52:31,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:31,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=301.8275184880007, CurrSamplesPerSec=273.6174048241632, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:52:31.593 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.3428
dss8440-001: [2023-07-27 21:52:37,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:37,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=299.8574999196043, CurrSamplesPerSec=276.6631547330419, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:52:37.839 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.2452
dss8440-001: [2023-07-27 21:52:43,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:44,009] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=298.4250477852395, CurrSamplesPerSec=275.89772752634, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:52:44.009 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.1609
dss8440-001: [2023-07-27 21:52:50,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:50,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=297.188225708189, CurrSamplesPerSec=281.8942133207877, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:52:50.167 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.0808
dss8440-001: [2023-07-27 21:52:56,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:52:56,344] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=296.01013331332655, CurrSamplesPerSec=286.1437770375331, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:52:56.345 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 9.0037
dss8440-001: [2023-07-27 21:53:02,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:02,623] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=294.71475600964976, CurrSamplesPerSec=286.04620382732895, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:02.624 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.9389
dss8440-001: [2023-07-27 21:53:08,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:08,808] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=293.78832405594574, CurrSamplesPerSec=273.56407903134885, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:08.808 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.8714
dss8440-001: [2023-07-27 21:53:14,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:15,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=292.80263172080254, CurrSamplesPerSec=254.4843799263608, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:15.053 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.8042
dss8440-001: [2023-07-27 21:53:21,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:21,147] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=292.25196995909187, CurrSamplesPerSec=277.9573756368645, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:21.147 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.7492
dss8440-001: [2023-07-27 21:53:27,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:27,369] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=291.49173816495676, CurrSamplesPerSec=271.17155112498915, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:27.370 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.6948
dss8440-001: [2023-07-27 21:53:33,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:33,497] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=290.9753894153472, CurrSamplesPerSec=281.7356317013577, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:33.497 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.6384
dss8440-001: [2023-07-27 21:53:39,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:39,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=290.3997410409645, CurrSamplesPerSec=280.00560771728715, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:39.674 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.5856
dss8440-001: [2023-07-27 21:53:45,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:45,889] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=289.79191081612, CurrSamplesPerSec=268.6779314360274, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:45.890 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.5425
dss8440-001: [2023-07-27 21:53:52,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:52,061] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=289.324239061685, CurrSamplesPerSec=283.8315611392849, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:52.062 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.4992
dss8440-001: [2023-07-27 21:53:58,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:53:58,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=288.7040424477439, CurrSamplesPerSec=269.6316962799529, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:53:58.339 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.4604
dss8440-001: [2023-07-27 21:54:04,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:04,533] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=288.266921970989, CurrSamplesPerSec=272.15301433717644, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:04.533 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.4273
dss8440-001: [2023-07-27 21:54:10,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:10,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=287.9969459248909, CurrSamplesPerSec=281.5915190426314, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:10.638 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.3874
dss8440-001: [2023-07-27 21:54:16,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:16,698] [INFO] [timer.py:215:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=287.81609170528174, CurrSamplesPerSec=287.493598293259, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:16.698 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.3598
dss8440-001: [2023-07-27 21:54:22,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:22,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=287.33122859658414, CurrSamplesPerSec=266.07072079385875, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:22.978 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.3264
dss8440-001: [2023-07-27 21:54:29,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:29,128] [INFO] [timer.py:215:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=287.0536347174576, CurrSamplesPerSec=279.74848472216433, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:29.128 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.3013
dss8440-001: [2023-07-27 21:54:35,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:35,412] [INFO] [timer.py:215:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=286.61110286942966, CurrSamplesPerSec=281.90921285885116, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:35.412 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.2751
dss8440-001: [2023-07-27 21:54:41,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:41,585] [INFO] [timer.py:215:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=286.34831415956745, CurrSamplesPerSec=296.86844088714673, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:41.585 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.2480
dss8440-001: [2023-07-27 21:54:47,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:47,829] [INFO] [timer.py:215:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=285.9883100437429, CurrSamplesPerSec=266.1442832333307, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:47.830 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.2141
dss8440-001: [2023-07-27 21:54:54,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:54:54,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=285.68153198686133, CurrSamplesPerSec=273.9397136210393, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:54:54.059 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.1889
dss8440-001: [2023-07-27 21:55:00,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:00,274] [INFO] [timer.py:215:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=285.38767588738466, CurrSamplesPerSec=279.8804888831163, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:00.274 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.1648
dss8440-001: [2023-07-27 21:55:06,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:06,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=285.0562245799857, CurrSamplesPerSec=258.7189575504344, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:06.538 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.1435
dss8440-001: [2023-07-27 21:55:12,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:12,717] [INFO] [timer.py:215:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=284.84987668473747, CurrSamplesPerSec=278.24560090031395, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:12.718 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.1220
dss8440-001: [2023-07-27 21:55:18,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:18,970] [INFO] [timer.py:215:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=284.5711613636221, CurrSamplesPerSec=277.5465970912563, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:18.971 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.1009
dss8440-001: [2023-07-27 21:55:25,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:25,124] [INFO] [timer.py:215:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=284.41284996789716, CurrSamplesPerSec=283.1579431275507, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:25.124 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.0845
dss8440-001: [2023-07-27 21:55:31,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:31,344] [INFO] [timer.py:215:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=284.18280622401073, CurrSamplesPerSec=280.04767275770877, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:31.345 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.0664
dss8440-001: [2023-07-27 21:55:37,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:37,520] [INFO] [timer.py:215:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=284.02584703878864, CurrSamplesPerSec=286.51551261022206, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:37.520 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.0455
dss8440-001: [2023-07-27 21:55:43,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:43,656] [INFO] [timer.py:215:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=283.90689924625036, CurrSamplesPerSec=277.2537082285728, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:43.657 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.0219
dss8440-001: [2023-07-27 21:55:49,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:49,784] [INFO] [timer.py:215:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=283.813698042303, CurrSamplesPerSec=277.8342992801432, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:49.785 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 8.0061
dss8440-001: [2023-07-27 21:55:56,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:55:56,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=283.45675844655, CurrSamplesPerSec=270.7874383214203, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:55:56.175 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9909
dss8440-001: [2023-07-27 21:56:02,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:02,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=283.25615082781394, CurrSamplesPerSec=258.4836863697991, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:02.424 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9746
dss8440-001: [2023-07-27 21:56:08,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:08,678] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=283.05878583574173, CurrSamplesPerSec=284.0111695095605, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:08.678 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9604
dss8440-001: [2023-07-27 21:56:15,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:15,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=282.6773358480516, CurrSamplesPerSec=288.57396088395046, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:15.123 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9449
dss8440-001: [2023-07-27 21:56:21,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:21,260] [INFO] [timer.py:215:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=282.5945802525084, CurrSamplesPerSec=281.87188613807916, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:21.260 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9342
dss8440-001: [2023-07-27 21:56:27,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:27,604] [INFO] [timer.py:215:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=282.33199893106615, CurrSamplesPerSec=273.56726524774456, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:27.604 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9197
dss8440-001: [2023-07-27 21:56:33,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:33,809] [INFO] [timer.py:215:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=282.20208037166117, CurrSamplesPerSec=277.3407895109717, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:33.809 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.9041
dss8440-001: [2023-07-27 21:56:39,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:39,966] [INFO] [timer.py:215:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=282.1221590112411, CurrSamplesPerSec=284.3061280646981, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:39.967 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8911
dss8440-001: [2023-07-27 21:56:46,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:46,162] [INFO] [timer.py:215:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=282.0192489596095, CurrSamplesPerSec=272.5112905826922, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:46.162 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8793
dss8440-001: [2023-07-27 21:56:52,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:52,365] [INFO] [timer.py:215:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=281.90075328369875, CurrSamplesPerSec=270.0905710192057, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:52.365 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8704
dss8440-001: [2023-07-27 21:56:58,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:56:58,434] [INFO] [timer.py:215:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=281.8963991337667, CurrSamplesPerSec=292.8201821640773, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:56:58.434 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8570
dss8440-001: [2023-07-27 21:57:04,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:04,648] [INFO] [timer.py:215:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=281.79213615841473, CurrSamplesPerSec=278.596534954888, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:04.649 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8487
dss8440-001: [2023-07-27 21:57:10,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:10,856] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=281.68113023607265, CurrSamplesPerSec=279.2633193842315, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:10.856 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8369
dss8440-001: [2023-07-27 21:57:16,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:17,005] [INFO] [timer.py:215:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=281.62119299956913, CurrSamplesPerSec=279.97000697696967, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:17.005 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8255
dss8440-001: [2023-07-27 21:57:23,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:23,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=281.5513375770817, CurrSamplesPerSec=296.3996633213325, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:23.167 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8144
dss8440-001: [2023-07-27 21:57:29,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:29,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=281.46267567616627, CurrSamplesPerSec=269.0901908758568, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:29.363 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.8032
dss8440-001: [2023-07-27 21:57:35,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:35,657] [INFO] [timer.py:215:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=281.3075351812097, CurrSamplesPerSec=271.5849098054814, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:35.657 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7953
dss8440-001: [2023-07-27 21:57:41,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:41,805] [INFO] [timer.py:215:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=281.2591858306173, CurrSamplesPerSec=275.5485987415324, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:41.805 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7875
dss8440-001: [2023-07-27 21:57:47,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:47,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=281.2199585597918, CurrSamplesPerSec=292.4748860949269, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:47.942 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7802
dss8440-001: [2023-07-27 21:57:54,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:57:54,128] [INFO] [timer.py:215:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=281.14331148818076, CurrSamplesPerSec=262.26937803228105, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:57:54.129 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7696
dss8440-001: [2023-07-27 21:58:00,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:00,299] [INFO] [timer.py:215:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=281.08390528734344, CurrSamplesPerSec=272.3214046380696, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:00.299 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7582
dss8440-001: [2023-07-27 21:58:06,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:06,469] [INFO] [timer.py:215:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=281.0281418633132, CurrSamplesPerSec=277.2689816909777, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:06.470 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7475
dss8440-001: [2023-07-27 21:58:12,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:12,739] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=280.913125664436, CurrSamplesPerSec=275.83573471125334, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:12.740 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7416
dss8440-001: [2023-07-27 21:58:18,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:19,022] [INFO] [timer.py:215:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=280.78622226378474, CurrSamplesPerSec=257.68201730086724, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:19.022 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7328
dss8440-001: [2023-07-27 21:58:25,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:25,277] [INFO] [timer.py:215:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=280.67781094369354, CurrSamplesPerSec=260.25098391462103, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:25.277 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7235
dss8440-001: [2023-07-27 21:58:31,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:31,421] [INFO] [timer.py:215:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=280.64579685412315, CurrSamplesPerSec=276.62872460204176, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:31.421 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7153
dss8440-001: [2023-07-27 21:58:37,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:37,647] [INFO] [timer.py:215:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=280.5645269059098, CurrSamplesPerSec=266.77605810410523, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:37.647 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.7096
dss8440-001: [2023-07-27 21:58:43,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:43,840] [INFO] [timer.py:215:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=280.50500029511215, CurrSamplesPerSec=262.1440975238458, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:43.841 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6987
dss8440-001: [2023-07-27 21:58:49,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:50,026] [INFO] [timer.py:215:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=280.45573878608644, CurrSamplesPerSec=271.79211449902624, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:50.027 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6920
dss8440-001: [2023-07-27 21:58:56,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:58:56,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=280.3665833429439, CurrSamplesPerSec=278.24812798157654, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:58:56.274 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6849
dss8440-001: [2023-07-27 21:59:02,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:02,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=280.29899652712277, CurrSamplesPerSec=269.1731442284111, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:02.495 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6789
dss8440-001: [2023-07-27 21:59:08,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:08,717] [INFO] [timer.py:215:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=280.2353990038522, CurrSamplesPerSec=290.7610293450148, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:08.718 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6710
dss8440-001: [2023-07-27 21:59:14,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:14,872] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=280.2098528828432, CurrSamplesPerSec=275.81684000391425, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:14.872 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6656
dss8440-001: [2023-07-27 21:59:20,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:21,037] [INFO] [timer.py:215:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=280.17809319228525, CurrSamplesPerSec=274.46671836785254, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:21.038 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6582
dss8440-001: [2023-07-27 21:59:27,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:27,233] [INFO] [timer.py:215:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=280.1273337507138, CurrSamplesPerSec=271.6733888161918, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:27.233 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6529
dss8440-001: [2023-07-27 21:59:33,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:33,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=280.11725476080767, CurrSamplesPerSec=288.32505168951997, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:33.364 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6463
dss8440-001: [2023-07-27 21:59:39,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:39,529] [INFO] [timer.py:215:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=280.0873027209644, CurrSamplesPerSec=292.05363909085105, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:39.530 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6400
dss8440-001: [2023-07-27 21:59:45,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:45,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=280.0508142586989, CurrSamplesPerSec=277.5909884172135, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:45.702 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6325
dss8440-001: [2023-07-27 21:59:51,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:51,929] [INFO] [timer.py:215:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=279.9866850092851, CurrSamplesPerSec=279.7270513584948, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:51.929 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6270
dss8440-001: [2023-07-27 21:59:58,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 21:59:58,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=279.9322142979009, CurrSamplesPerSec=270.9945739018872, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 21:59:58.141 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6214
dss8440-001: [2023-07-27 22:00:04,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:04,206] [INFO] [timer.py:215:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=279.95429337718724, CurrSamplesPerSec=286.9306212731025, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:04.206 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6118
dss8440-001: [2023-07-27 22:00:10,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:10,503] [INFO] [timer.py:215:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=279.8603135189171, CurrSamplesPerSec=269.03224638884654, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:10.504 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.6053
dss8440-001: [2023-07-27 22:00:16,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:16,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=279.8551717029122, CurrSamplesPerSec=278.5235250242794, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:16.626 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5983
dss8440-001: [2023-07-27 22:00:22,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:22,897] [INFO] [timer.py:215:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=279.7713936675012, CurrSamplesPerSec=266.4418618632926, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:22.897 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5908
dss8440-001: [2023-07-27 22:00:29,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:29,103] [INFO] [timer.py:215:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=279.7280692764334, CurrSamplesPerSec=287.9715560670378, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:29.103 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5842
dss8440-001: [2023-07-27 22:00:35,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:35,354] [INFO] [timer.py:215:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=279.65461182043134, CurrSamplesPerSec=257.4516374180122, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:35.354 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5799
dss8440-001: [2023-07-27 22:00:41,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:41,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=279.63712349380927, CurrSamplesPerSec=264.12077895499124, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:41.520 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5746
dss8440-001: [2023-07-27 22:00:47,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:47,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=279.6246328168129, CurrSamplesPerSec=300.90525352045313, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:47.661 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5716
dss8440-001: [2023-07-27 22:00:53,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:53,786] [INFO] [timer.py:215:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=279.6189777711566, CurrSamplesPerSec=270.3251575777339, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:53.786 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5645
dss8440-001: [2023-07-27 22:00:59,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:00:59,918] [INFO] [timer.py:215:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=279.6131170091498, CurrSamplesPerSec=275.1264258670189, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:00:59.918 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5584
dss8440-001: [2023-07-27 22:01:05,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:06,066] [INFO] [timer.py:215:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=279.59549167175936, CurrSamplesPerSec=283.2364771491254, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:06.066 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5533
dss8440-001: [2023-07-27 22:01:12,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:12,269] [INFO] [timer.py:215:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=279.5571110979684, CurrSamplesPerSec=270.9216393794328, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:12.270 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5471
dss8440-001: [2023-07-27 22:01:18,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:18,474] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=279.5200738534143, CurrSamplesPerSec=259.6826183960216, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:18.474 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5433
dss8440-001: [2023-07-27 22:01:18,475] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-002: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-002:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001:   warnings.warn(
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-001:   warnings.warn(
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001:   warnings.warn(
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001:   warnings.warn(
dss8440-003: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-003:   warnings.warn(
dss8440-001: /home/lanyun/anaconda3/envs/bert_deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
dss8440-001:   warnings.warn(
dss8440-001: [2023-07-27 22:01:18,480] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 22:01:18,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 22:01:18,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:01:18,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:01:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:01:18,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,565] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,565] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,563] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,563] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,567] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,567] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,567] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-002: [2023-07-27 22:01:18,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:01:18,565] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step1000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:01:18,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-003: [2023-07-27 22:01:18,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:01:18,567] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step1000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:01:18,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
dss8440-001: 2023-07-27 22:01:18.568 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 22:01:24,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=17, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:24,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=279.4770549203267, CurrSamplesPerSec=289.87314896290087, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:24.779 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5381
dss8440-001: [2023-07-27 22:01:29,241] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 22:01:30,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:30,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=279.6783938877907, CurrSamplesPerSec=264.2812566807013, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:30.461 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5320
dss8440-001: [2023-07-27 22:01:36,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:36,550] [INFO] [timer.py:215:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=279.69485336446746, CurrSamplesPerSec=283.79349505808227, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:36.551 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5274
dss8440-001: [2023-07-27 22:01:42,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:42,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=279.62696090463913, CurrSamplesPerSec=263.3304565323898, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:42.820 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5210
dss8440-001: [2023-07-27 22:01:48,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:49,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=279.57630074846065, CurrSamplesPerSec=289.00513500300224, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:49.049 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5177
dss8440-001: [2023-07-27 22:01:55,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:01:55,269] [INFO] [timer.py:215:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=279.5296791135844, CurrSamplesPerSec=274.61347456784927, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:01:55.269 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5128
dss8440-001: [2023-07-27 22:02:01,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:01,476] [INFO] [timer.py:215:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=279.4909295223803, CurrSamplesPerSec=266.36189208958075, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:01.476 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5103
dss8440-001: [2023-07-27 22:02:07,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:07,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=279.48122523036517, CurrSamplesPerSec=265.7752733214296, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:07.628 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.5035
dss8440-001: [2023-07-27 22:02:13,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:13,862] [INFO] [timer.py:215:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=279.43217930296214, CurrSamplesPerSec=281.9139498867172, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:13.862 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4997
dss8440-001: [2023-07-27 22:02:19,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:19,983] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=279.43635862907314, CurrSamplesPerSec=288.3242258562657, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:19.983 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4938
dss8440-001: [2023-07-27 22:02:26,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:26,338] [INFO] [timer.py:215:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=279.33542148964614, CurrSamplesPerSec=276.1276896900082, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:26.338 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4894
dss8440-001: [2023-07-27 22:02:32,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:32,587] [INFO] [timer.py:215:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=279.2862536463033, CurrSamplesPerSec=265.4572781555501, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:32.588 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4859
dss8440-001: [2023-07-27 22:02:38,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:38,872] [INFO] [timer.py:215:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=279.2265457049432, CurrSamplesPerSec=264.5680257958708, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:38.872 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4806
dss8440-001: [2023-07-27 22:02:45,021] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:45,074] [INFO] [timer.py:215:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=279.1966989801175, CurrSamplesPerSec=284.3882845323378, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:45.074 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4755
dss8440-001: [2023-07-27 22:02:51,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:51,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=279.1497364423611, CurrSamplesPerSec=273.29998754982495, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:51.315 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4728
dss8440-001: [2023-07-27 22:02:57,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:02:57,510] [INFO] [timer.py:215:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=279.1238244335918, CurrSamplesPerSec=276.272653384486, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:02:57.511 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4685
dss8440-001: [2023-07-27 22:03:03,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:03,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=279.0964021243946, CurrSamplesPerSec=286.35320317236875, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:03.704 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4641
dss8440-001: [2023-07-27 22:03:09,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:09,925] [INFO] [timer.py:215:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=279.058846866823, CurrSamplesPerSec=278.3293486401168, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:09.925 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4609
dss8440-001: [2023-07-27 22:03:16,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:16,227] [INFO] [timer.py:215:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=278.99610635082695, CurrSamplesPerSec=262.4227265194245, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:16.227 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4584
dss8440-001: [2023-07-27 22:03:22,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:22,448] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=278.9586059710908, CurrSamplesPerSec=267.76507425979605, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:22.449 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4538
dss8440-001: [2023-07-27 22:03:28,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:28,635] [INFO] [timer.py:215:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=278.94190562010255, CurrSamplesPerSec=283.5039330446968, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:28.636 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4502
dss8440-001: [2023-07-27 22:03:34,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:34,785] [INFO] [timer.py:215:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=278.93391072770123, CurrSamplesPerSec=280.936543559447, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:34.785 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4482
dss8440-001: [2023-07-27 22:03:40,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:40,960] [INFO] [timer.py:215:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=278.91742559717113, CurrSamplesPerSec=276.5524006507179, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:40.960 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4463
dss8440-001: [2023-07-27 22:03:47,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:47,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=278.8829349103601, CurrSamplesPerSec=276.13872712765874, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:47.195 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4421
dss8440-001: [2023-07-27 22:03:53,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:53,423] [INFO] [timer.py:215:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=278.8451510748326, CurrSamplesPerSec=267.51510213812975, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:53.423 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4382
dss8440-001: [2023-07-27 22:03:59,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:03:59,653] [INFO] [timer.py:215:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=278.80949150433617, CurrSamplesPerSec=270.9894672071254, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:03:59.653 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4349
dss8440-001: [2023-07-27 22:04:05,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:06,008] [INFO] [timer.py:215:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=278.7292830633098, CurrSamplesPerSec=281.0733713818902, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:06.008 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4316
dss8440-001: [2023-07-27 22:04:12,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:12,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=278.64962252596587, CurrSamplesPerSec=287.8656763428995, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:12.373 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4285
dss8440-001: [2023-07-27 22:04:18,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:18,690] [INFO] [timer.py:215:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=278.58436816524835, CurrSamplesPerSec=292.6230654773646, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:18.690 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4247
dss8440-001: [2023-07-27 22:04:24,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:24,904] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=278.55621812363074, CurrSamplesPerSec=273.99627565155686, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:24.904 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4212
dss8440-001: [2023-07-27 22:04:30,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:31,061] [INFO] [timer.py:215:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=278.5505060802586, CurrSamplesPerSec=266.8792197270999, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:31.061 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4154
dss8440-001: [2023-07-27 22:04:37,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:37,199] [INFO] [timer.py:215:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=278.55345488975405, CurrSamplesPerSec=279.5085741531063, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:37.200 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4117
dss8440-001: [2023-07-27 22:04:43,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:43,371] [INFO] [timer.py:215:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=278.54536750456316, CurrSamplesPerSec=285.16168237536573, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:43.371 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4081
dss8440-001: [2023-07-27 22:04:49,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:49,559] [INFO] [timer.py:215:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=278.5314781836576, CurrSamplesPerSec=275.06810779075175, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:49.560 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4037
dss8440-001: [2023-07-27 22:04:55,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:04:55,811] [INFO] [timer.py:215:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=278.4933681458182, CurrSamplesPerSec=271.2706215243892, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:04:55.811 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.4011
dss8440-001: [2023-07-27 22:05:02,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:02,132] [INFO] [timer.py:215:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=278.43614733620086, CurrSamplesPerSec=286.704600827104, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:02.132 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3974
dss8440-001: [2023-07-27 22:05:08,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:08,286] [INFO] [timer.py:215:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=278.43490108614276, CurrSamplesPerSec=285.5398939198633, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:08.286 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3932
dss8440-001: [2023-07-27 22:05:14,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:14,584] [INFO] [timer.py:215:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=278.38522260675, CurrSamplesPerSec=274.9092815810336, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:14.584 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3909
dss8440-001: [2023-07-27 22:05:20,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:20,849] [INFO] [timer.py:215:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=278.34358241107753, CurrSamplesPerSec=259.67036924299254, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:20.850 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3864
dss8440-001: [2023-07-27 22:05:26,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:26,994] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=278.3436462227142, CurrSamplesPerSec=264.9958602098777, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:26.995 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3837
dss8440-001: [2023-07-27 22:05:33,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:33,235] [INFO] [timer.py:215:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=278.31277112681767, CurrSamplesPerSec=287.9693200236377, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:33.236 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3790
dss8440-001: [2023-07-27 22:05:39,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:39,539] [INFO] [timer.py:215:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=278.2645423421563, CurrSamplesPerSec=277.4606975705757, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:39.539 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3759
dss8440-001: [2023-07-27 22:05:45,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:45,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=278.2519663183424, CurrSamplesPerSec=267.2654949324689, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:45.720 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3716
dss8440-001: [2023-07-27 22:05:51,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:51,952] [INFO] [timer.py:215:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=278.2230907074975, CurrSamplesPerSec=268.85497217753874, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:51.953 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3690
dss8440-001: [2023-07-27 22:05:58,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:05:58,187] [INFO] [timer.py:215:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=278.193475899998, CurrSamplesPerSec=261.9352703314546, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:05:58.187 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3659
dss8440-001: [2023-07-27 22:06:04,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:04,305] [INFO] [timer.py:215:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=278.20312652051217, CurrSamplesPerSec=278.45550496909556, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:04.306 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3627
dss8440-001: [2023-07-27 22:06:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:10,448] [INFO] [timer.py:215:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=278.2035076183587, CurrSamplesPerSec=292.5147097344655, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:10.448 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3601
dss8440-001: [2023-07-27 22:06:16,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:16,632] [INFO] [timer.py:215:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=278.1920633478603, CurrSamplesPerSec=288.3230461026803, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:16.632 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3568
dss8440-001: [2023-07-27 22:06:22,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:22,817] [INFO] [timer.py:215:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=278.1800824969588, CurrSamplesPerSec=275.5728451802338, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:22.817 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3535
dss8440-001: [2023-07-27 22:06:29,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:29,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=278.06133789128137, CurrSamplesPerSec=273.3507714723962, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:29.349 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3502
dss8440-001: [2023-07-27 22:06:35,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:35,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=278.05303499114234, CurrSamplesPerSec=271.72315085497326, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:35.526 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3458
dss8440-001: [2023-07-27 22:06:41,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:41,811] [INFO] [timer.py:215:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=278.014320219276, CurrSamplesPerSec=270.11883658962654, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:41.811 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3415
dss8440-001: [2023-07-27 22:06:47,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:47,965] [INFO] [timer.py:215:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=278.01456985672297, CurrSamplesPerSec=295.78776782877094, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:47.966 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3386
dss8440-001: [2023-07-27 22:06:54,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:06:54,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=277.9954927676432, CurrSamplesPerSec=286.71708336300287, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:06:54.176 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3367
dss8440-001: [2023-07-27 22:07:00,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:00,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=277.9835583701912, CurrSamplesPerSec=293.39455908357905, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:00.373 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3328
dss8440-001: [2023-07-27 22:07:06,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:06,604] [INFO] [timer.py:215:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=277.9649040187966, CurrSamplesPerSec=286.9203398857601, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:06.605 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3304
dss8440-001: [2023-07-27 22:07:12,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:12,813] [INFO] [timer.py:215:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=277.9520354604139, CurrSamplesPerSec=265.26231242948637, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:12.814 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3267
dss8440-001: [2023-07-27 22:07:19,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:19,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=277.9192316410829, CurrSamplesPerSec=270.26678372653095, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:19.081 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3237
dss8440-001: [2023-07-27 22:07:25,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:25,340] [INFO] [timer.py:215:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=277.887647227424, CurrSamplesPerSec=276.63947630943255, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:25.340 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3214
dss8440-001: [2023-07-27 22:07:31,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:31,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=277.87583496555806, CurrSamplesPerSec=292.22550666673857, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:31.538 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3194
dss8440-001: [2023-07-27 22:07:37,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:37,846] [INFO] [timer.py:215:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=277.83321811577395, CurrSamplesPerSec=269.16913415423755, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:37.847 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3169
dss8440-001: [2023-07-27 22:07:44,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:44,075] [INFO] [timer.py:215:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=277.8127393211373, CurrSamplesPerSec=272.4689303483453, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:44.075 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3136
dss8440-001: [2023-07-27 22:07:50,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:50,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=277.8259905379866, CurrSamplesPerSec=290.10538918309817, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:50.185 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3105
dss8440-001: [2023-07-27 22:07:56,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:07:56,314] [INFO] [timer.py:215:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=277.8320892200694, CurrSamplesPerSec=278.05993115615905, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:07:56.314 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3079
dss8440-001: [2023-07-27 22:08:02,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:02,512] [INFO] [timer.py:215:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=277.81902701571374, CurrSamplesPerSec=274.0471055300772, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:02.512 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3059
dss8440-001: [2023-07-27 22:08:08,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:08,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=277.83487642099715, CurrSamplesPerSec=277.6871454000977, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:08.614 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3036
dss8440-001: [2023-07-27 22:08:14,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:14,826] [INFO] [timer.py:215:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=277.8177423700682, CurrSamplesPerSec=264.95550516135313, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:14.826 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.3012
dss8440-001: [2023-07-27 22:08:20,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:21,071] [INFO] [timer.py:215:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=277.7920614253556, CurrSamplesPerSec=271.6704560438162, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:21.071 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2977
dss8440-001: [2023-07-27 22:08:27,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:27,358] [INFO] [timer.py:215:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=277.75727207053995, CurrSamplesPerSec=273.7441103549443, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:27.358 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2958
dss8440-001: [2023-07-27 22:08:33,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:33,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=277.7544375232595, CurrSamplesPerSec=278.9068406765868, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:33.526 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2931
dss8440-001: [2023-07-27 22:08:39,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:39,741] [INFO] [timer.py:215:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=277.73858087673716, CurrSamplesPerSec=268.7115379115228, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:39.742 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2905
dss8440-001: [2023-07-27 22:08:45,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:46,031] [INFO] [timer.py:215:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=277.70282664799214, CurrSamplesPerSec=262.0100707450879, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:46.031 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2865
dss8440-001: [2023-07-27 22:08:52,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:52,264] [INFO] [timer.py:215:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=277.6836545916888, CurrSamplesPerSec=267.87501045431804, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:52.264 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2842
dss8440-001: [2023-07-27 22:08:58,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:08:58,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=277.6629094449703, CurrSamplesPerSec=276.5541372850763, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:08:58.509 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2819
dss8440-001: [2023-07-27 22:09:04,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:04,597] [INFO] [timer.py:215:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=277.6816345564838, CurrSamplesPerSec=276.298869188756, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:04.597 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2782
dss8440-001: [2023-07-27 22:09:10,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:10,816] [INFO] [timer.py:215:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=277.6667049381474, CurrSamplesPerSec=284.1456239974257, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:10.816 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2756
dss8440-001: [2023-07-27 22:09:17,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:17,121] [INFO] [timer.py:215:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=277.6295178454908, CurrSamplesPerSec=284.0228461976881, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:17.122 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2716
dss8440-001: [2023-07-27 22:09:23,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:23,313] [INFO] [timer.py:215:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=277.62289584949707, CurrSamplesPerSec=277.0914770676186, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:23.314 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2688
dss8440-001: [2023-07-27 22:09:29,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:29,528] [INFO] [timer.py:215:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=277.6095449028454, CurrSamplesPerSec=261.33655947676544, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:29.529 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2663
dss8440-001: [2023-07-27 22:09:35,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:35,828] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=277.5790232388383, CurrSamplesPerSec=281.86906730082666, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:35.828 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2640
dss8440-001: [2023-07-27 22:09:41,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:42,011] [INFO] [timer.py:215:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=277.57669700037565, CurrSamplesPerSec=261.24044626898706, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:42.012 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2605
dss8440-001: [2023-07-27 22:09:48,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:48,168] [INFO] [timer.py:215:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=277.5772131662554, CurrSamplesPerSec=275.93424504329664, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:48.168 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2559
dss8440-001: [2023-07-27 22:09:54,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:09:54,272] [INFO] [timer.py:215:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=277.5909955361042, CurrSamplesPerSec=299.98891905434766, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:09:54.272 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2534
dss8440-001: [2023-07-27 22:10:00,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:00,544] [INFO] [timer.py:215:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=277.56589687747606, CurrSamplesPerSec=273.54421995555083, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:00.545 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2514
dss8440-001: [2023-07-27 22:10:06,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:06,680] [INFO] [timer.py:215:stop] epoch=0/micro_step=1850/global_step=1850, RunningAvgSamplesPerSec=277.572786793426, CurrSamplesPerSec=288.8512416851714, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:06.681 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2471
dss8440-001: [2023-07-27 22:10:12,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:12,732] [INFO] [timer.py:215:stop] epoch=0/micro_step=1860/global_step=1860, RunningAvgSamplesPerSec=277.6017626112427, CurrSamplesPerSec=291.1939553053279, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:12.732 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2452
dss8440-001: [2023-07-27 22:10:18,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:18,917] [INFO] [timer.py:215:stop] epoch=0/micro_step=1870/global_step=1870, RunningAvgSamplesPerSec=277.5985853943314, CurrSamplesPerSec=295.0783932569119, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:18.918 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2420
dss8440-001: [2023-07-27 22:10:25,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:25,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=1880/global_step=1880, RunningAvgSamplesPerSec=277.56205700167027, CurrSamplesPerSec=272.47019464226264, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:25.228 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2400
dss8440-001: [2023-07-27 22:10:31,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:31,421] [INFO] [timer.py:215:stop] epoch=0/micro_step=1890/global_step=1890, RunningAvgSamplesPerSec=277.55520225649616, CurrSamplesPerSec=276.38491797800276, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:31.421 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2379
dss8440-001: [2023-07-27 22:10:37,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:37,598] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=1900, RunningAvgSamplesPerSec=277.55046806766194, CurrSamplesPerSec=284.00521705765044, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:37.599 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2357
dss8440-001: [2023-07-27 22:10:43,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:43,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=1910/global_step=1910, RunningAvgSamplesPerSec=277.5293509228888, CurrSamplesPerSec=273.15579627839253, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:43.852 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2344
dss8440-001: [2023-07-27 22:10:49,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:49,956] [INFO] [timer.py:215:stop] epoch=0/micro_step=1920/global_step=1920, RunningAvgSamplesPerSec=277.54325332993017, CurrSamplesPerSec=282.97077539533575, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:49.957 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2330
dss8440-001: [2023-07-27 22:10:56,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:10:56,225] [INFO] [timer.py:215:stop] epoch=0/micro_step=1930/global_step=1930, RunningAvgSamplesPerSec=277.51682086981117, CurrSamplesPerSec=291.1862540058391, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:10:56.226 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2306
dss8440-001: [2023-07-27 22:11:02,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:02,382] [INFO] [timer.py:215:stop] epoch=0/micro_step=1940/global_step=1940, RunningAvgSamplesPerSec=277.521637932518, CurrSamplesPerSec=271.2730234968289, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:02.383 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2287
dss8440-001: [2023-07-27 22:11:08,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:08,478] [INFO] [timer.py:215:stop] epoch=0/micro_step=1950/global_step=1950, RunningAvgSamplesPerSec=277.5383689840988, CurrSamplesPerSec=275.1235254889528, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:08.479 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2269
dss8440-001: [2023-07-27 22:11:14,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:14,708] [INFO] [timer.py:215:stop] epoch=0/micro_step=1960/global_step=1960, RunningAvgSamplesPerSec=277.5226064974335, CurrSamplesPerSec=269.77632139628855, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:14.708 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2250
dss8440-001: [2023-07-27 22:11:20,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:20,946] [INFO] [timer.py:215:stop] epoch=0/micro_step=1970/global_step=1970, RunningAvgSamplesPerSec=277.50650488554277, CurrSamplesPerSec=282.9936179723297, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:20.947 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2237
dss8440-001: [2023-07-27 22:11:27,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:27,238] [INFO] [timer.py:215:stop] epoch=0/micro_step=1980/global_step=1980, RunningAvgSamplesPerSec=277.47779496643517, CurrSamplesPerSec=282.0089496148928, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:27.239 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2217
dss8440-001: [2023-07-27 22:11:33,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:33,514] [INFO] [timer.py:215:stop] epoch=0/micro_step=1990/global_step=1990, RunningAvgSamplesPerSec=277.4543870836909, CurrSamplesPerSec=283.6016055554505, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:33.514 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2196
dss8440-001: [2023-07-27 22:11:39,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:39,689] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=277.4522457502699, CurrSamplesPerSec=266.748891674241, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:39.689 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2169
dss8440-001: [2023-07-27 22:11:39,690] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
dss8440-001: [2023-07-27 22:11:39,694] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 22:11:39,694] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 22:11:39,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:11:39,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:11:39,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,769] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,773] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,773] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step2000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-003: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-002: [2023-07-27 22:11:39,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,770] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:11:39,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: [2023-07-27 22:11:39,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:11:39,774] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step2000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:11:39,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
dss8440-001: 2023-07-27 22:11:39.774 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 22:11:45,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=18, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:46,068] [INFO] [timer.py:215:stop] epoch=0/micro_step=2010/global_step=2010, RunningAvgSamplesPerSec=277.4249855996715, CurrSamplesPerSec=265.2621127140724, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:46.069 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2149
dss8440-001: [2023-07-27 22:11:51,037] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 22:11:51,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:51,677] [INFO] [timer.py:215:stop] epoch=0/micro_step=2020/global_step=2020, RunningAvgSamplesPerSec=277.5501704308228, CurrSamplesPerSec=266.62050672961846, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:51.677 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2137
dss8440-001: [2023-07-27 22:11:57,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:11:57,840] [INFO] [timer.py:215:stop] epoch=0/micro_step=2030/global_step=2030, RunningAvgSamplesPerSec=277.55066250295295, CurrSamplesPerSec=265.4506780143725, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:11:57.840 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2125
dss8440-001: [2023-07-27 22:12:03,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:04,030] [INFO] [timer.py:215:stop] epoch=0/micro_step=2040/global_step=2040, RunningAvgSamplesPerSec=277.54535766332936, CurrSamplesPerSec=265.045498827567, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:04.030 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2107
dss8440-001: [2023-07-27 22:12:10,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:10,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=2050/global_step=2050, RunningAvgSamplesPerSec=277.53716550730024, CurrSamplesPerSec=281.1828398768394, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:10.230 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2084
dss8440-001: [2023-07-27 22:12:16,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:16,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=2060/global_step=2060, RunningAvgSamplesPerSec=277.53831658317904, CurrSamplesPerSec=286.58706166573126, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:16.382 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2071
dss8440-001: [2023-07-27 22:12:22,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:22,642] [INFO] [timer.py:215:stop] epoch=0/micro_step=2070/global_step=2070, RunningAvgSamplesPerSec=277.5195415730136, CurrSamplesPerSec=257.03176442651244, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:22.642 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2059
dss8440-001: [2023-07-27 22:12:28,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:28,907] [INFO] [timer.py:215:stop] epoch=0/micro_step=2080/global_step=2080, RunningAvgSamplesPerSec=277.49604776442806, CurrSamplesPerSec=280.709799423875, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:28.908 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2031
dss8440-001: [2023-07-27 22:12:35,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:35,164] [INFO] [timer.py:215:stop] epoch=0/micro_step=2090/global_step=2090, RunningAvgSamplesPerSec=277.4803894843956, CurrSamplesPerSec=276.0275274208712, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:35.164 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.2016
dss8440-001: [2023-07-27 22:12:41,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:41,364] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=2100, RunningAvgSamplesPerSec=277.47480651860883, CurrSamplesPerSec=283.38957289857717, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:41.365 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1991
dss8440-001: [2023-07-27 22:12:47,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:47,582] [INFO] [timer.py:215:stop] epoch=0/micro_step=2110/global_step=2110, RunningAvgSamplesPerSec=277.46233715415406, CurrSamplesPerSec=284.98165365804533, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:47.583 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1969
dss8440-001: [2023-07-27 22:12:53,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:12:53,901] [INFO] [timer.py:215:stop] epoch=0/micro_step=2120/global_step=2120, RunningAvgSamplesPerSec=277.4311652894816, CurrSamplesPerSec=280.8140607272106, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:12:53.901 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1948
dss8440-001: [2023-07-27 22:13:00,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:00,189] [INFO] [timer.py:215:stop] epoch=0/micro_step=2130/global_step=2130, RunningAvgSamplesPerSec=277.4037880220884, CurrSamplesPerSec=263.86501242478846, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:00.190 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1935
dss8440-001: [2023-07-27 22:13:06,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:06,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=2140/global_step=2140, RunningAvgSamplesPerSec=277.3817452811139, CurrSamplesPerSec=259.19850979163124, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:06.458 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1912
dss8440-001: [2023-07-27 22:13:12,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:12,608] [INFO] [timer.py:215:stop] epoch=0/micro_step=2150/global_step=2150, RunningAvgSamplesPerSec=277.38342212468564, CurrSamplesPerSec=273.0639153716364, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:12.608 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1895
dss8440-001: [2023-07-27 22:13:18,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:18,783] [INFO] [timer.py:215:stop] epoch=0/micro_step=2160/global_step=2160, RunningAvgSamplesPerSec=277.3810747154441, CurrSamplesPerSec=271.9502970391031, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:18.783 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1870
dss8440-001: [2023-07-27 22:13:24,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:25,060] [INFO] [timer.py:215:stop] epoch=0/micro_step=2170/global_step=2170, RunningAvgSamplesPerSec=277.35616874851854, CurrSamplesPerSec=263.7808544090562, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:25.061 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1846
dss8440-001: [2023-07-27 22:13:31,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:31,235] [INFO] [timer.py:215:stop] epoch=0/micro_step=2180/global_step=2180, RunningAvgSamplesPerSec=277.35375649542516, CurrSamplesPerSec=290.07003985234746, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:31.236 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1828
dss8440-001: [2023-07-27 22:13:37,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:37,495] [INFO] [timer.py:215:stop] epoch=0/micro_step=2190/global_step=2190, RunningAvgSamplesPerSec=277.33331693201416, CurrSamplesPerSec=283.50781127725617, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:37.495 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1812
dss8440-001: [2023-07-27 22:13:43,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:43,688] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=2200, RunningAvgSamplesPerSec=277.3263389547624, CurrSamplesPerSec=277.19263120213054, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:43.689 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1793
dss8440-001: [2023-07-27 22:13:49,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:49,900] [INFO] [timer.py:215:stop] epoch=0/micro_step=2210/global_step=2210, RunningAvgSamplesPerSec=277.3178033281427, CurrSamplesPerSec=274.969892910339, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:49.900 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1777
dss8440-001: [2023-07-27 22:13:56,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:13:56,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=2220/global_step=2220, RunningAvgSamplesPerSec=277.3201158158388, CurrSamplesPerSec=281.90086705422, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:13:56.058 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1761
dss8440-001: [2023-07-27 22:14:02,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:02,305] [INFO] [timer.py:215:stop] epoch=0/micro_step=2230/global_step=2230, RunningAvgSamplesPerSec=277.30495883268526, CurrSamplesPerSec=290.1828056400578, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:02.305 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1747
dss8440-001: [2023-07-27 22:14:08,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:08,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=2240/global_step=2240, RunningAvgSamplesPerSec=277.2961439182154, CurrSamplesPerSec=266.0232082189578, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:08.520 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1726
dss8440-001: [2023-07-27 22:14:14,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:14,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=2250/global_step=2250, RunningAvgSamplesPerSec=277.29506456189995, CurrSamplesPerSec=267.4027212237082, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:14.698 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1708
dss8440-001: [2023-07-27 22:14:20,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:20,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=2260/global_step=2260, RunningAvgSamplesPerSec=277.2811352777127, CurrSamplesPerSec=269.3276744598487, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:20.946 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1691
dss8440-001: [2023-07-27 22:14:27,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:27,119] [INFO] [timer.py:215:stop] epoch=0/micro_step=2270/global_step=2270, RunningAvgSamplesPerSec=277.2817657547027, CurrSamplesPerSec=280.5076503924319, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:27.120 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1677
dss8440-001: [2023-07-27 22:14:33,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:33,398] [INFO] [timer.py:215:stop] epoch=0/micro_step=2280/global_step=2280, RunningAvgSamplesPerSec=277.2608555895511, CurrSamplesPerSec=272.4124705171925, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:33.398 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1663
dss8440-001: [2023-07-27 22:14:39,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:39,575] [INFO] [timer.py:215:stop] epoch=0/micro_step=2290/global_step=2290, RunningAvgSamplesPerSec=277.2579070232903, CurrSamplesPerSec=268.68049260925966, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:39.576 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1648
dss8440-001: [2023-07-27 22:14:45,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:45,711] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=2300, RunningAvgSamplesPerSec=277.263807790406, CurrSamplesPerSec=276.85229304630883, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:45.712 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1632
dss8440-001: [2023-07-27 22:14:51,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:51,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=2310/global_step=2310, RunningAvgSamplesPerSec=277.2692230584668, CurrSamplesPerSec=273.31578261436204, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:51.853 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1612
dss8440-001: [2023-07-27 22:14:57,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:14:58,057] [INFO] [timer.py:215:stop] epoch=0/micro_step=2320/global_step=2320, RunningAvgSamplesPerSec=277.2623394200615, CurrSamplesPerSec=272.4970636620346, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:14:58.058 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1600
dss8440-001: [2023-07-27 22:15:04,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:04,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=2330/global_step=2330, RunningAvgSamplesPerSec=277.2344297594808, CurrSamplesPerSec=267.43885148752344, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:04.373 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1584
dss8440-001: [2023-07-27 22:15:10,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:10,656] [INFO] [timer.py:215:stop] epoch=0/micro_step=2340/global_step=2340, RunningAvgSamplesPerSec=277.2108588470003, CurrSamplesPerSec=271.9569094677365, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:10.657 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1558
dss8440-001: [2023-07-27 22:15:16,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:16,835] [INFO] [timer.py:215:stop] epoch=0/micro_step=2350/global_step=2350, RunningAvgSamplesPerSec=277.20934030925036, CurrSamplesPerSec=287.0876207448258, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:16.836 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1542
dss8440-001: [2023-07-27 22:15:22,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:23,059] [INFO] [timer.py:215:stop] epoch=0/micro_step=2360/global_step=2360, RunningAvgSamplesPerSec=277.19994382803, CurrSamplesPerSec=271.6648001616164, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:23.060 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1517
dss8440-001: [2023-07-27 22:15:29,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:29,343] [INFO] [timer.py:215:stop] epoch=0/micro_step=2370/global_step=2370, RunningAvgSamplesPerSec=277.1785744184629, CurrSamplesPerSec=274.48746008664176, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:29.343 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1505
dss8440-001: [2023-07-27 22:15:35,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:35,665] [INFO] [timer.py:215:stop] epoch=0/micro_step=2380/global_step=2380, RunningAvgSamplesPerSec=277.14982228564634, CurrSamplesPerSec=273.66628735300947, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:35.665 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1484
dss8440-001: [2023-07-27 22:15:41,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:41,848] [INFO] [timer.py:215:stop] epoch=0/micro_step=2390/global_step=2390, RunningAvgSamplesPerSec=277.14680991166085, CurrSamplesPerSec=290.05224902299614, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:41.849 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1460
dss8440-001: [2023-07-27 22:15:48,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:48,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=2400, RunningAvgSamplesPerSec=277.13723952325057, CurrSamplesPerSec=291.92514313649133, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:48.081 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1444
dss8440-001: [2023-07-27 22:15:54,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:15:54,251] [INFO] [timer.py:215:stop] epoch=0/micro_step=2410/global_step=2410, RunningAvgSamplesPerSec=277.13538085114425, CurrSamplesPerSec=284.98211468393225, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:15:54.251 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1425
dss8440-001: [2023-07-27 22:16:00,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:00,561] [INFO] [timer.py:215:stop] epoch=0/micro_step=2420/global_step=2420, RunningAvgSamplesPerSec=277.1092065125281, CurrSamplesPerSec=272.0281971686244, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:00.562 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1408
dss8440-001: [2023-07-27 22:16:06,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:06,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=2430/global_step=2430, RunningAvgSamplesPerSec=277.1023761719514, CurrSamplesPerSec=290.3110876730389, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:06.771 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1398
dss8440-001: [2023-07-27 22:16:12,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:13,009] [INFO] [timer.py:215:stop] epoch=0/micro_step=2440/global_step=2440, RunningAvgSamplesPerSec=277.09000697523106, CurrSamplesPerSec=267.04600293027744, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:13.010 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1380
dss8440-001: [2023-07-27 22:16:19,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:19,070] [INFO] [timer.py:215:stop] epoch=0/micro_step=2450/global_step=2450, RunningAvgSamplesPerSec=277.1112784150586, CurrSamplesPerSec=286.2971246084275, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:19.070 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1368
dss8440-001: [2023-07-27 22:16:25,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:25,284] [INFO] [timer.py:215:stop] epoch=0/micro_step=2460/global_step=2460, RunningAvgSamplesPerSec=277.1035981047499, CurrSamplesPerSec=272.453654390891, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:25.284 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1357
dss8440-001: [2023-07-27 22:16:31,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:31,517] [INFO] [timer.py:215:stop] epoch=0/micro_step=2470/global_step=2470, RunningAvgSamplesPerSec=277.0914502243417, CurrSamplesPerSec=263.35742324661805, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:31.517 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1339
dss8440-001: [2023-07-27 22:16:37,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:37,780] [INFO] [timer.py:215:stop] epoch=0/micro_step=2480/global_step=2480, RunningAvgSamplesPerSec=277.07491606185147, CurrSamplesPerSec=287.15032823033545, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:37.781 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1319
dss8440-001: [2023-07-27 22:16:44,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:44,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=2490/global_step=2490, RunningAvgSamplesPerSec=277.04573231742114, CurrSamplesPerSec=261.35090501035364, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:44.108 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1306
dss8440-001: [2023-07-27 22:16:50,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:50,274] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=2500, RunningAvgSamplesPerSec=277.0474833468584, CurrSamplesPerSec=274.8515915532183, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:50.275 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1290
dss8440-001: [2023-07-27 22:16:56,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:16:56,544] [INFO] [timer.py:215:stop] epoch=0/micro_step=2510/global_step=2510, RunningAvgSamplesPerSec=277.0287476274999, CurrSamplesPerSec=280.3931280426127, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:16:56.545 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1276
dss8440-001: [2023-07-27 22:17:02,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:02,725] [INFO] [timer.py:215:stop] epoch=0/micro_step=2520/global_step=2520, RunningAvgSamplesPerSec=277.0269972970327, CurrSamplesPerSec=273.0183154412525, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:02.726 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1269
dss8440-001: [2023-07-27 22:17:08,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:09,026] [INFO] [timer.py:215:stop] epoch=0/micro_step=2530/global_step=2530, RunningAvgSamplesPerSec=277.00453372948596, CurrSamplesPerSec=267.11635909980544, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:09.027 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1250
dss8440-001: [2023-07-27 22:17:15,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:15,287] [INFO] [timer.py:215:stop] epoch=0/micro_step=2540/global_step=2540, RunningAvgSamplesPerSec=276.98945256862885, CurrSamplesPerSec=283.4985721332225, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:15.288 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1233
dss8440-001: [2023-07-27 22:17:21,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:21,509] [INFO] [timer.py:215:stop] epoch=0/micro_step=2550/global_step=2550, RunningAvgSamplesPerSec=276.9787466097676, CurrSamplesPerSec=269.87509828621023, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:21.509 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1211
dss8440-001: [2023-07-27 22:17:27,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:27,684] [INFO] [timer.py:215:stop] epoch=0/micro_step=2560/global_step=2560, RunningAvgSamplesPerSec=276.9787971740664, CurrSamplesPerSec=281.8581307460184, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:27.684 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1193
dss8440-001: [2023-07-27 22:17:33,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:33,923] [INFO] [timer.py:215:stop] epoch=0/micro_step=2570/global_step=2570, RunningAvgSamplesPerSec=276.96757150541004, CurrSamplesPerSec=259.0998657513786, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:33.923 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1180
dss8440-001: [2023-07-27 22:17:40,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:40,210] [INFO] [timer.py:215:stop] epoch=0/micro_step=2580/global_step=2580, RunningAvgSamplesPerSec=276.9471067376792, CurrSamplesPerSec=289.6739053310142, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:40.210 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1168
dss8440-001: [2023-07-27 22:17:46,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:46,485] [INFO] [timer.py:215:stop] epoch=0/micro_step=2590/global_step=2590, RunningAvgSamplesPerSec=276.9290075857518, CurrSamplesPerSec=265.16309092174856, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:46.486 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1150
dss8440-001: [2023-07-27 22:17:52,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:52,751] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=2600, RunningAvgSamplesPerSec=276.9131974643948, CurrSamplesPerSec=270.1170762898011, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:52.752 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1139
dss8440-001: [2023-07-27 22:17:58,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:17:59,028] [INFO] [timer.py:215:stop] epoch=0/micro_step=2610/global_step=2610, RunningAvgSamplesPerSec=276.89615196263514, CurrSamplesPerSec=271.22447014192034, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:17:59.028 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1126
dss8440-001: [2023-07-27 22:18:05,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:05,321] [INFO] [timer.py:215:stop] epoch=0/micro_step=2620/global_step=2620, RunningAvgSamplesPerSec=276.8745292856025, CurrSamplesPerSec=275.7284471677345, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:05.322 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1106
dss8440-001: [2023-07-27 22:18:11,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:11,616] [INFO] [timer.py:215:stop] epoch=0/micro_step=2630/global_step=2630, RunningAvgSamplesPerSec=276.854085560549, CurrSamplesPerSec=269.6254027775918, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:11.616 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1084
dss8440-001: [2023-07-27 22:18:17,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:17,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=2640/global_step=2640, RunningAvgSamplesPerSec=276.83919293732083, CurrSamplesPerSec=280.0445563953641, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:17.887 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1069
dss8440-001: [2023-07-27 22:18:23,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:24,100] [INFO] [timer.py:215:stop] epoch=0/micro_step=2650/global_step=2650, RunningAvgSamplesPerSec=276.83149600568396, CurrSamplesPerSec=259.66558473144715, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:24.100 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1059
dss8440-001: [2023-07-27 22:18:30,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:30,316] [INFO] [timer.py:215:stop] epoch=0/micro_step=2660/global_step=2660, RunningAvgSamplesPerSec=276.82383069491084, CurrSamplesPerSec=270.3165502565855, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:30.317 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1050
dss8440-001: [2023-07-27 22:18:36,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:36,486] [INFO] [timer.py:215:stop] epoch=0/micro_step=2670/global_step=2670, RunningAvgSamplesPerSec=276.8251032762321, CurrSamplesPerSec=275.3056158756379, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:36.486 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1040
dss8440-001: [2023-07-27 22:18:42,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:42,629] [INFO] [timer.py:215:stop] epoch=0/micro_step=2680/global_step=2680, RunningAvgSamplesPerSec=276.830768296732, CurrSamplesPerSec=273.8042089286734, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:42.630 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1025
dss8440-001: [2023-07-27 22:18:48,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:48,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=2690/global_step=2690, RunningAvgSamplesPerSec=276.8375993959119, CurrSamplesPerSec=293.4541860892228, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:48.771 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.1007
dss8440-001: [2023-07-27 22:18:54,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:18:54,901] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=2700, RunningAvgSamplesPerSec=276.846189190205, CurrSamplesPerSec=264.95769697174796, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:18:54.901 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0995
dss8440-001: [2023-07-27 22:19:01,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:01,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=2710/global_step=2710, RunningAvgSamplesPerSec=276.8379700585795, CurrSamplesPerSec=268.38515295418915, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:01.123 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0982
dss8440-001: [2023-07-27 22:19:07,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:07,279] [INFO] [timer.py:215:stop] epoch=0/micro_step=2720/global_step=2720, RunningAvgSamplesPerSec=276.841198132598, CurrSamplesPerSec=285.18095580370095, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:07.279 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0966
dss8440-001: [2023-07-27 22:19:13,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:13,496] [INFO] [timer.py:215:stop] epoch=0/micro_step=2730/global_step=2730, RunningAvgSamplesPerSec=276.83490134486556, CurrSamplesPerSec=266.4252395071699, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:13.497 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0955
dss8440-001: [2023-07-27 22:19:19,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:19,796] [INFO] [timer.py:215:stop] epoch=0/micro_step=2740/global_step=2740, RunningAvgSamplesPerSec=276.81502901367224, CurrSamplesPerSec=275.67246198096467, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:19.796 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0945
dss8440-001: [2023-07-27 22:19:25,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:26,036] [INFO] [timer.py:215:stop] epoch=0/micro_step=2750/global_step=2750, RunningAvgSamplesPerSec=276.8029743035388, CurrSamplesPerSec=269.530726192876, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:26.037 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0932
dss8440-001: [2023-07-27 22:19:31,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:32,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=2760/global_step=2760, RunningAvgSamplesPerSec=276.82896375149375, CurrSamplesPerSec=280.1182546749777, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:32.059 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0921
dss8440-001: [2023-07-27 22:19:38,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:38,218] [INFO] [timer.py:215:stop] epoch=0/micro_step=2770/global_step=2770, RunningAvgSamplesPerSec=276.83230352218294, CurrSamplesPerSec=275.1482343716687, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:38.219 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0908
dss8440-001: [2023-07-27 22:19:44,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:44,356] [INFO] [timer.py:215:stop] epoch=0/micro_step=2780/global_step=2780, RunningAvgSamplesPerSec=276.83857639324293, CurrSamplesPerSec=278.83752988634285, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:44.357 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0896
dss8440-001: [2023-07-27 22:19:50,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:50,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=2790/global_step=2790, RunningAvgSamplesPerSec=276.8335756185226, CurrSamplesPerSec=292.84208688973837, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:50.557 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0886
dss8440-001: [2023-07-27 22:19:56,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:19:56,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=2800, RunningAvgSamplesPerSec=276.8170604654765, CurrSamplesPerSec=286.12902062634305, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:19:56.838 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0881
dss8440-001: [2023-07-27 22:20:02,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:03,055] [INFO] [timer.py:215:stop] epoch=0/micro_step=2810/global_step=2810, RunningAvgSamplesPerSec=276.80961985336006, CurrSamplesPerSec=285.3504894923655, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:03.055 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0867
dss8440-001: [2023-07-27 22:20:09,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:09,280] [INFO] [timer.py:215:stop] epoch=0/micro_step=2820/global_step=2820, RunningAvgSamplesPerSec=276.80105191315664, CurrSamplesPerSec=284.142530343043, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:09.281 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0853
dss8440-001: [2023-07-27 22:20:15,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:15,529] [INFO] [timer.py:215:stop] epoch=0/micro_step=2830/global_step=2830, RunningAvgSamplesPerSec=276.7894782241877, CurrSamplesPerSec=258.38672253580467, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:15.529 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0843
dss8440-001: [2023-07-27 22:20:21,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:21,782] [INFO] [timer.py:215:stop] epoch=0/micro_step=2840/global_step=2840, RunningAvgSamplesPerSec=276.77658693046334, CurrSamplesPerSec=276.467440749108, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:21.782 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0834
dss8440-001: [2023-07-27 22:20:27,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:27,976] [INFO] [timer.py:215:stop] epoch=0/micro_step=2850/global_step=2850, RunningAvgSamplesPerSec=276.7758005360331, CurrSamplesPerSec=264.65258752888707, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:27.976 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0818
dss8440-001: [2023-07-27 22:20:34,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:34,204] [INFO] [timer.py:215:stop] epoch=0/micro_step=2860/global_step=2860, RunningAvgSamplesPerSec=276.7677556443837, CurrSamplesPerSec=280.75688641077886, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:34.205 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0803
dss8440-001: [2023-07-27 22:20:40,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:40,358] [INFO] [timer.py:215:stop] epoch=0/micro_step=2870/global_step=2870, RunningAvgSamplesPerSec=276.77030091410523, CurrSamplesPerSec=256.45383512620464, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:40.358 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0782
dss8440-001: [2023-07-27 22:20:46,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:46,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=2880/global_step=2880, RunningAvgSamplesPerSec=276.75894684482546, CurrSamplesPerSec=268.9822328359667, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:46.606 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0774
dss8440-001: [2023-07-27 22:20:52,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:52,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=2890/global_step=2890, RunningAvgSamplesPerSec=276.75170107588616, CurrSamplesPerSec=256.34878183563757, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:52.833 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0764
dss8440-001: [2023-07-27 22:20:59,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:20:59,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=2900, RunningAvgSamplesPerSec=276.74039406356746, CurrSamplesPerSec=280.99132593001406, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:20:59.088 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0758
dss8440-001: [2023-07-27 22:21:05,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:05,163] [INFO] [timer.py:215:stop] epoch=0/micro_step=2910/global_step=2910, RunningAvgSamplesPerSec=276.7569279472558, CurrSamplesPerSec=292.774314550692, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:05.164 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0739
dss8440-001: [2023-07-27 22:21:11,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:11,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=2920/global_step=2920, RunningAvgSamplesPerSec=276.758786664642, CurrSamplesPerSec=274.8138595063949, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:11.332 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0731
dss8440-001: [2023-07-27 22:21:17,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:17,521] [INFO] [timer.py:215:stop] epoch=0/micro_step=2930/global_step=2930, RunningAvgSamplesPerSec=276.75703120824767, CurrSamplesPerSec=267.0518729629349, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:17.521 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0727
dss8440-001: [2023-07-27 22:21:23,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:23,634] [INFO] [timer.py:215:stop] epoch=0/micro_step=2940/global_step=2940, RunningAvgSamplesPerSec=276.7677630231376, CurrSamplesPerSec=268.9260796172522, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:23.635 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0712
dss8440-001: [2023-07-27 22:21:29,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:29,857] [INFO] [timer.py:215:stop] epoch=0/micro_step=2950/global_step=2950, RunningAvgSamplesPerSec=276.7608114672504, CurrSamplesPerSec=278.79637911299426, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:29.857 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0697
dss8440-001: [2023-07-27 22:21:35,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:36,025] [INFO] [timer.py:215:stop] epoch=0/micro_step=2960/global_step=2960, RunningAvgSamplesPerSec=276.76275184776335, CurrSamplesPerSec=262.14468266844443, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:36.025 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0686
dss8440-001: [2023-07-27 22:21:42,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:42,129] [INFO] [timer.py:215:stop] epoch=0/micro_step=2970/global_step=2970, RunningAvgSamplesPerSec=276.77416986071745, CurrSamplesPerSec=266.8968086077482, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:42.129 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0668
dss8440-001: [2023-07-27 22:21:48,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:48,296] [INFO] [timer.py:215:stop] epoch=0/micro_step=2980/global_step=2980, RunningAvgSamplesPerSec=276.7765228059457, CurrSamplesPerSec=280.9105602120853, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:48.296 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0655
dss8440-001: [2023-07-27 22:21:54,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:21:54,649] [INFO] [timer.py:215:stop] epoch=0/micro_step=2990/global_step=2990, RunningAvgSamplesPerSec=276.7525902188814, CurrSamplesPerSec=275.3229345302077, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:21:54.650 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0641
dss8440-001: [2023-07-27 22:22:00,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:00,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=276.7542096239925, CurrSamplesPerSec=277.9253631031077, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:00.834 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0625
dss8440-001: [2023-07-27 22:22:00,835] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
dss8440-001: [2023-07-27 22:22:00,838] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 22:22:00,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 22:22:00,896] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:22:00,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:22:00,915] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,915] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:22:00,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: [2023-07-27 22:22:00,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,916] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,918] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,918] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,918] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:22:00,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,919] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,916] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:22:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: [2023-07-27 22:22:00,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,919] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-003: [2023-07-27 22:22:00,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:22:00,920] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step3000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:22:00,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: [2023-07-27 22:22:00,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:22:00,921] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step3000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:22:00,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
dss8440-001: 2023-07-27 22:22:00.921 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 22:22:07,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=19, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:07,134] [INFO] [timer.py:215:stop] epoch=0/micro_step=3010/global_step=3010, RunningAvgSamplesPerSec=276.75131686179384, CurrSamplesPerSec=273.7361346516099, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:07.134 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0616
dss8440-001: [2023-07-27 22:22:12,971] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 22:22:12,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:12,972] [INFO] [timer.py:215:stop] epoch=0/micro_step=3020/global_step=3020, RunningAvgSamplesPerSec=276.80445063058767, CurrSamplesPerSec=2211.7551461125586, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:12.973 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0604
dss8440-001: [2023-07-27 22:22:19,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:19,120] [INFO] [timer.py:215:stop] epoch=0/micro_step=3030/global_step=3030, RunningAvgSamplesPerSec=276.8098207889357, CurrSamplesPerSec=297.5426037144526, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:19.120 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0588
dss8440-001: [2023-07-27 22:22:25,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:25,364] [INFO] [timer.py:215:stop] epoch=0/micro_step=3040/global_step=3040, RunningAvgSamplesPerSec=276.8010916798308, CurrSamplesPerSec=273.7950594785933, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:25.364 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0581
dss8440-001: [2023-07-27 22:22:31,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:31,436] [INFO] [timer.py:215:stop] epoch=0/micro_step=3050/global_step=3050, RunningAvgSamplesPerSec=276.81758277610504, CurrSamplesPerSec=278.20671161290016, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:31.436 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0568
dss8440-001: [2023-07-27 22:22:37,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:37,655] [INFO] [timer.py:215:stop] epoch=0/micro_step=3060/global_step=3060, RunningAvgSamplesPerSec=276.81231391471096, CurrSamplesPerSec=269.0471410811323, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:37.655 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0556
dss8440-001: [2023-07-27 22:22:43,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:43,855] [INFO] [timer.py:215:stop] epoch=0/micro_step=3070/global_step=3070, RunningAvgSamplesPerSec=276.80926945903883, CurrSamplesPerSec=287.41444456862325, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:43.855 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0550
dss8440-001: [2023-07-27 22:22:49,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:50,034] [INFO] [timer.py:215:stop] epoch=0/micro_step=3080/global_step=3080, RunningAvgSamplesPerSec=276.81014305846935, CurrSamplesPerSec=266.0797631634595, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:50.035 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0536
dss8440-001: [2023-07-27 22:22:56,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:22:56,356] [INFO] [timer.py:215:stop] epoch=0/micro_step=3090/global_step=3090, RunningAvgSamplesPerSec=276.7901388938156, CurrSamplesPerSec=267.530845035985, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:22:56.356 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0524
dss8440-001: [2023-07-27 22:23:02,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:02,665] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=3100, RunningAvgSamplesPerSec=276.77175625986456, CurrSamplesPerSec=262.3372394844417, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:02.665 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0512
dss8440-001: [2023-07-27 22:23:08,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:08,861] [INFO] [timer.py:215:stop] epoch=0/micro_step=3110/global_step=3110, RunningAvgSamplesPerSec=276.7696130834075, CurrSamplesPerSec=266.59306938222363, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:08.862 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0502
dss8440-001: [2023-07-27 22:23:15,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:15,152] [INFO] [timer.py:215:stop] epoch=0/micro_step=3120/global_step=3120, RunningAvgSamplesPerSec=276.75406306599905, CurrSamplesPerSec=268.6363448170956, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:15.153 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0486
dss8440-001: [2023-07-27 22:23:21,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:21,371] [INFO] [timer.py:215:stop] epoch=0/micro_step=3130/global_step=3130, RunningAvgSamplesPerSec=276.74788979425284, CurrSamplesPerSec=270.3987051126637, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:21.371 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0478
dss8440-001: [2023-07-27 22:23:27,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:27,570] [INFO] [timer.py:215:stop] epoch=0/micro_step=3140/global_step=3140, RunningAvgSamplesPerSec=276.74489620324573, CurrSamplesPerSec=266.24232918704325, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:27.571 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0471
dss8440-001: [2023-07-27 22:23:33,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:33,740] [INFO] [timer.py:215:stop] epoch=0/micro_step=3150/global_step=3150, RunningAvgSamplesPerSec=276.7463877372937, CurrSamplesPerSec=296.89270677043373, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:33.740 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0457
dss8440-001: [2023-07-27 22:23:39,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:39,933] [INFO] [timer.py:215:stop] epoch=0/micro_step=3160/global_step=3160, RunningAvgSamplesPerSec=276.74319855387023, CurrSamplesPerSec=268.7354159118559, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:39.934 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0449
dss8440-001: [2023-07-27 22:23:45,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:46,041] [INFO] [timer.py:215:stop] epoch=0/micro_step=3170/global_step=3170, RunningAvgSamplesPerSec=276.75495045709016, CurrSamplesPerSec=271.52818792023896, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:46.042 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0441
dss8440-001: [2023-07-27 22:23:52,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:52,253] [INFO] [timer.py:215:stop] epoch=0/micro_step=3180/global_step=3180, RunningAvgSamplesPerSec=276.7502870958557, CurrSamplesPerSec=277.25785371627154, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:52.253 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0428
dss8440-001: [2023-07-27 22:23:58,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:23:58,512] [INFO] [timer.py:215:stop] epoch=0/micro_step=3190/global_step=3190, RunningAvgSamplesPerSec=276.73917004017346, CurrSamplesPerSec=260.25925053167794, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:23:58.513 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0407
dss8440-001: [2023-07-27 22:24:04,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:04,737] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=3200, RunningAvgSamplesPerSec=276.73408746736817, CurrSamplesPerSec=289.65628212754046, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:04.738 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0392
dss8440-001: [2023-07-27 22:24:10,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:10,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=3210/global_step=3210, RunningAvgSamplesPerSec=276.7427367242772, CurrSamplesPerSec=280.5061987443676, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:10.853 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0382
dss8440-001: [2023-07-27 22:24:17,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:17,136] [INFO] [timer.py:215:stop] epoch=0/micro_step=3220/global_step=3220, RunningAvgSamplesPerSec=276.7307250089977, CurrSamplesPerSec=260.5667375791849, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:17.136 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0372
dss8440-001: [2023-07-27 22:24:23,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:23,425] [INFO] [timer.py:215:stop] epoch=0/micro_step=3230/global_step=3230, RunningAvgSamplesPerSec=276.7159327671491, CurrSamplesPerSec=267.3153791029511, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:23.426 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0363
dss8440-001: [2023-07-27 22:24:29,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:29,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=3240/global_step=3240, RunningAvgSamplesPerSec=276.7125822494847, CurrSamplesPerSec=272.47398759441364, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:29.628 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0358
dss8440-001: [2023-07-27 22:24:35,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:35,787] [INFO] [timer.py:215:stop] epoch=0/micro_step=3250/global_step=3250, RunningAvgSamplesPerSec=276.7170790808346, CurrSamplesPerSec=285.80418458917654, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:35.787 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0347
dss8440-001: [2023-07-27 22:24:42,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:42,067] [INFO] [timer.py:215:stop] epoch=0/micro_step=3260/global_step=3260, RunningAvgSamplesPerSec=276.7031222078164, CurrSamplesPerSec=261.0129439097245, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:42.068 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0339
dss8440-001: [2023-07-27 22:24:48,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:48,341] [INFO] [timer.py:215:stop] epoch=0/micro_step=3270/global_step=3270, RunningAvgSamplesPerSec=276.6900666631452, CurrSamplesPerSec=268.3351753116264, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:48.341 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0324
dss8440-001: [2023-07-27 22:24:54,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:24:54,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=3280/global_step=3280, RunningAvgSamplesPerSec=276.6889989151202, CurrSamplesPerSec=280.24793306914376, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:24:54.526 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0312
dss8440-001: [2023-07-27 22:25:00,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:00,804] [INFO] [timer.py:215:stop] epoch=0/micro_step=3290/global_step=3290, RunningAvgSamplesPerSec=276.6761646531044, CurrSamplesPerSec=267.7407579799613, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:00.805 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0296
dss8440-001: [2023-07-27 22:25:06,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:06,940] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=3300, RunningAvgSamplesPerSec=276.6827538444545, CurrSamplesPerSec=279.8542559195327, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:06.940 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0289
dss8440-001: [2023-07-27 22:25:13,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:13,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=3310/global_step=3310, RunningAvgSamplesPerSec=276.6805977139336, CurrSamplesPerSec=259.0277649321518, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:13.148 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0280
dss8440-001: [2023-07-27 22:25:19,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:19,399] [INFO] [timer.py:215:stop] epoch=0/micro_step=3320/global_step=3320, RunningAvgSamplesPerSec=276.67132604647367, CurrSamplesPerSec=273.4248077296186, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:19.400 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0268
dss8440-001: [2023-07-27 22:25:25,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:25,706] [INFO] [timer.py:215:stop] epoch=0/micro_step=3330/global_step=3330, RunningAvgSamplesPerSec=276.65526893805935, CurrSamplesPerSec=266.5492015361011, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:25.707 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0256
dss8440-001: [2023-07-27 22:25:31,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:31,892] [INFO] [timer.py:215:stop] epoch=0/micro_step=3340/global_step=3340, RunningAvgSamplesPerSec=276.65582775415083, CurrSamplesPerSec=278.21253332838484, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:31.893 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0248
dss8440-001: [2023-07-27 22:25:38,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:38,113] [INFO] [timer.py:215:stop] epoch=0/micro_step=3350/global_step=3350, RunningAvgSamplesPerSec=276.6510094090118, CurrSamplesPerSec=284.0645237755455, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:38.114 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0240
dss8440-001: [2023-07-27 22:25:44,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:44,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=3360/global_step=3360, RunningAvgSamplesPerSec=276.6365980835246, CurrSamplesPerSec=284.2916752737038, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:44.405 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0227
dss8440-001: [2023-07-27 22:25:50,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:50,572] [INFO] [timer.py:215:stop] epoch=0/micro_step=3370/global_step=3370, RunningAvgSamplesPerSec=276.63877848977495, CurrSamplesPerSec=288.08681815580104, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:50.573 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0218
dss8440-001: [2023-07-27 22:25:56,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:25:56,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=3380/global_step=3380, RunningAvgSamplesPerSec=276.6461245364677, CurrSamplesPerSec=272.1096095237948, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:25:56.704 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0213
dss8440-001: [2023-07-27 22:26:02,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:02,937] [INFO] [timer.py:215:stop] epoch=0/micro_step=3390/global_step=3390, RunningAvgSamplesPerSec=276.63955058429735, CurrSamplesPerSec=266.05896663214327, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:02.937 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0208
dss8440-001: [2023-07-27 22:26:09,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:09,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=3400, RunningAvgSamplesPerSec=276.62193078691615, CurrSamplesPerSec=268.35878206226795, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:09.243 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0189
dss8440-001: [2023-07-27 22:26:15,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:15,413] [INFO] [timer.py:215:stop] epoch=0/micro_step=3410/global_step=3410, RunningAvgSamplesPerSec=276.62338800032734, CurrSamplesPerSec=266.2680845098204, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:15.414 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0187
dss8440-001: [2023-07-27 22:26:21,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:21,706] [INFO] [timer.py:215:stop] epoch=0/micro_step=3420/global_step=3420, RunningAvgSamplesPerSec=276.60830195780346, CurrSamplesPerSec=288.4705901618137, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:21.707 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0174
dss8440-001: [2023-07-27 22:26:27,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:27,917] [INFO] [timer.py:215:stop] epoch=0/micro_step=3430/global_step=3430, RunningAvgSamplesPerSec=276.60599641901297, CurrSamplesPerSec=280.43486317398157, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:27.918 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0166
dss8440-001: [2023-07-27 22:26:33,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:34,035] [INFO] [timer.py:215:stop] epoch=0/micro_step=3440/global_step=3440, RunningAvgSamplesPerSec=276.61575999386406, CurrSamplesPerSec=290.92309051589245, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:34.036 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0156
dss8440-001: [2023-07-27 22:26:40,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:40,220] [INFO] [timer.py:215:stop] epoch=0/micro_step=3450/global_step=3450, RunningAvgSamplesPerSec=276.6191129269013, CurrSamplesPerSec=269.9000105716505, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:40.221 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0140
dss8440-001: [2023-07-27 22:26:46,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:46,405] [INFO] [timer.py:215:stop] epoch=0/micro_step=3460/global_step=3460, RunningAvgSamplesPerSec=276.61877968230704, CurrSamplesPerSec=274.12813131203103, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:46.406 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0128
dss8440-001: [2023-07-27 22:26:52,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:52,585] [INFO] [timer.py:215:stop] epoch=0/micro_step=3470/global_step=3470, RunningAvgSamplesPerSec=276.6186980442867, CurrSamplesPerSec=286.4175693390158, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:52.585 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0114
dss8440-001: [2023-07-27 22:26:58,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:26:58,777] [INFO] [timer.py:215:stop] epoch=0/micro_step=3480/global_step=3480, RunningAvgSamplesPerSec=276.61826300336133, CurrSamplesPerSec=278.3981870060868, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:26:58.778 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0103
dss8440-001: [2023-07-27 22:27:04,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:04,878] [INFO] [timer.py:215:stop] epoch=0/micro_step=3490/global_step=3490, RunningAvgSamplesPerSec=276.6285824085645, CurrSamplesPerSec=297.10752012183787, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:04.879 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0088
dss8440-001: [2023-07-27 22:27:11,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:11,118] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=3500, RunningAvgSamplesPerSec=276.62179909736784, CurrSamplesPerSec=264.7085611418362, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:11.119 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0083
dss8440-001: [2023-07-27 22:27:17,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:17,303] [INFO] [timer.py:215:stop] epoch=0/micro_step=3510/global_step=3510, RunningAvgSamplesPerSec=276.6213123386296, CurrSamplesPerSec=275.88055251041436, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:17.304 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0068
dss8440-001: [2023-07-27 22:27:23,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:23,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=3520/global_step=3520, RunningAvgSamplesPerSec=276.6182096786392, CurrSamplesPerSec=285.68298100847306, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:23.509 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0058
dss8440-001: [2023-07-27 22:27:29,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:29,709] [INFO] [timer.py:215:stop] epoch=0/micro_step=3530/global_step=3530, RunningAvgSamplesPerSec=276.61496228668284, CurrSamplesPerSec=293.46628550866603, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:29.710 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0054
dss8440-001: [2023-07-27 22:27:35,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:35,801] [INFO] [timer.py:215:stop] epoch=0/micro_step=3540/global_step=3540, RunningAvgSamplesPerSec=276.6270570759221, CurrSamplesPerSec=287.9627297819284, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:35.801 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0040
dss8440-001: [2023-07-27 22:27:41,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:42,020] [INFO] [timer.py:215:stop] epoch=0/micro_step=3550/global_step=3550, RunningAvgSamplesPerSec=276.6213807209554, CurrSamplesPerSec=263.01808502736213, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:42.020 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0022
dss8440-001: [2023-07-27 22:27:48,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:48,142] [INFO] [timer.py:215:stop] epoch=0/micro_step=3560/global_step=3560, RunningAvgSamplesPerSec=276.62874007692574, CurrSamplesPerSec=280.95020914479693, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:48.143 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0013
dss8440-001: [2023-07-27 22:27:54,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:27:54,213] [INFO] [timer.py:215:stop] epoch=0/micro_step=3570/global_step=3570, RunningAvgSamplesPerSec=276.6420760912918, CurrSamplesPerSec=282.43633626387543, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:27:54.213 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 7.0005
dss8440-001: [2023-07-27 22:28:00,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:00,416] [INFO] [timer.py:215:stop] epoch=0/micro_step=3580/global_step=3580, RunningAvgSamplesPerSec=276.63995954913366, CurrSamplesPerSec=285.8339797940791, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:00.417 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9998
dss8440-001: [2023-07-27 22:28:06,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:06,668] [INFO] [timer.py:215:stop] epoch=0/micro_step=3590/global_step=3590, RunningAvgSamplesPerSec=276.63152329139683, CurrSamplesPerSec=280.69380910004213, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:06.668 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9990
dss8440-001: [2023-07-27 22:28:12,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:12,887] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=3600, RunningAvgSamplesPerSec=276.62648805055574, CurrSamplesPerSec=277.39133919548834, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:12.887 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9982
dss8440-001: [2023-07-27 22:28:18,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:19,082] [INFO] [timer.py:215:stop] epoch=0/micro_step=3610/global_step=3610, RunningAvgSamplesPerSec=276.62475499054085, CurrSamplesPerSec=274.76574474083486, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:19.083 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9967
dss8440-001: [2023-07-27 22:28:25,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:25,226] [INFO] [timer.py:215:stop] epoch=0/micro_step=3620/global_step=3620, RunningAvgSamplesPerSec=276.6303208167404, CurrSamplesPerSec=280.37248825121685, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:25.227 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9956
dss8440-001: [2023-07-27 22:28:31,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:31,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=3630/global_step=3630, RunningAvgSamplesPerSec=276.6278734444434, CurrSamplesPerSec=274.35921104938905, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:31.415 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9941
dss8440-001: [2023-07-27 22:28:37,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:37,636] [INFO] [timer.py:215:stop] epoch=0/micro_step=3640/global_step=3640, RunningAvgSamplesPerSec=276.6222898051535, CurrSamplesPerSec=280.4020542990099, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:37.637 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9933
dss8440-001: [2023-07-27 22:28:43,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:43,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=3650/global_step=3650, RunningAvgSamplesPerSec=276.6135122770423, CurrSamplesPerSec=271.1084302026956, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:43.886 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9923
dss8440-001: [2023-07-27 22:28:50,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:50,153] [INFO] [timer.py:215:stop] epoch=0/micro_step=3660/global_step=3660, RunningAvgSamplesPerSec=276.60217923501926, CurrSamplesPerSec=261.9788013139036, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:50.153 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9914
dss8440-001: [2023-07-27 22:28:56,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:28:56,431] [INFO] [timer.py:215:stop] epoch=0/micro_step=3670/global_step=3670, RunningAvgSamplesPerSec=276.589022753241, CurrSamplesPerSec=284.99951950222373, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:28:56.431 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9899
dss8440-001: [2023-07-27 22:29:02,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:02,744] [INFO] [timer.py:215:stop] epoch=0/micro_step=3680/global_step=3680, RunningAvgSamplesPerSec=276.57268357944133, CurrSamplesPerSec=280.9769841041984, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:02.745 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9887
dss8440-001: [2023-07-27 22:29:08,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:08,932] [INFO] [timer.py:215:stop] epoch=0/micro_step=3690/global_step=3690, RunningAvgSamplesPerSec=276.572485496755, CurrSamplesPerSec=265.615579817767, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:08.933 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9878
dss8440-001: [2023-07-27 22:29:15,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:15,169] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=3700, RunningAvgSamplesPerSec=276.56604336265974, CurrSamplesPerSec=285.9565880638481, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:15.170 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9868
dss8440-001: [2023-07-27 22:29:21,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:21,288] [INFO] [timer.py:215:stop] epoch=0/micro_step=3710/global_step=3710, RunningAvgSamplesPerSec=276.572816235493, CurrSamplesPerSec=285.9770136587682, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:21.289 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9856
dss8440-001: [2023-07-27 22:29:27,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:27,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=3720/global_step=3720, RunningAvgSamplesPerSec=276.573246753745, CurrSamplesPerSec=283.8136127957235, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:27.463 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9845
dss8440-001: [2023-07-27 22:29:33,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:33,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=3730/global_step=3730, RunningAvgSamplesPerSec=276.5715200573266, CurrSamplesPerSec=276.3797145139901, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:33.659 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9838
dss8440-001: [2023-07-27 22:29:39,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:39,946] [INFO] [timer.py:215:stop] epoch=0/micro_step=3740/global_step=3740, RunningAvgSamplesPerSec=276.559007805967, CurrSamplesPerSec=278.52550668898635, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:39.947 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9833
dss8440-001: [2023-07-27 22:29:46,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:46,237] [INFO] [timer.py:215:stop] epoch=0/micro_step=3750/global_step=3750, RunningAvgSamplesPerSec=276.5463203145442, CurrSamplesPerSec=267.91432436563395, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:46.238 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9822
dss8440-001: [2023-07-27 22:29:52,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:52,501] [INFO] [timer.py:215:stop] epoch=0/micro_step=3760/global_step=3760, RunningAvgSamplesPerSec=276.5365183091301, CurrSamplesPerSec=279.36684103230164, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:52.501 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9814
dss8440-001: [2023-07-27 22:29:58,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:29:58,727] [INFO] [timer.py:215:stop] epoch=0/micro_step=3770/global_step=3770, RunningAvgSamplesPerSec=276.5321669018692, CurrSamplesPerSec=279.27416618214806, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:29:58.727 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9806
dss8440-001: [2023-07-27 22:30:04,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:04,905] [INFO] [timer.py:215:stop] epoch=0/micro_step=3780/global_step=3780, RunningAvgSamplesPerSec=276.5326470793914, CurrSamplesPerSec=288.4015207482101, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:04.906 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9792
dss8440-001: [2023-07-27 22:30:11,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:11,113] [INFO] [timer.py:215:stop] epoch=0/micro_step=3790/global_step=3790, RunningAvgSamplesPerSec=276.531069976692, CurrSamplesPerSec=285.13952692230805, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:11.114 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9784
dss8440-001: [2023-07-27 22:30:17,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:17,165] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=3800, RunningAvgSamplesPerSec=276.5471732180115, CurrSamplesPerSec=284.8332808516504, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:17.165 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9777
dss8440-001: [2023-07-27 22:30:23,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:23,340] [INFO] [timer.py:215:stop] epoch=0/micro_step=3810/global_step=3810, RunningAvgSamplesPerSec=276.54785651127844, CurrSamplesPerSec=286.7408848544877, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:23.341 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9770
dss8440-001: [2023-07-27 22:30:29,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:29,588] [INFO] [timer.py:215:stop] epoch=0/micro_step=3820/global_step=3820, RunningAvgSamplesPerSec=276.54021255768197, CurrSamplesPerSec=274.58597257737307, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:29.588 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9761
dss8440-001: [2023-07-27 22:30:35,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:35,793] [INFO] [timer.py:215:stop] epoch=0/micro_step=3830/global_step=3830, RunningAvgSamplesPerSec=276.5378581297384, CurrSamplesPerSec=272.40404568345616, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:35.793 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9754
dss8440-001: [2023-07-27 22:30:41,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:41,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=3840/global_step=3840, RunningAvgSamplesPerSec=276.5365465280165, CurrSamplesPerSec=283.54694173518834, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:41.993 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9748
dss8440-001: [2023-07-27 22:30:48,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:48,125] [INFO] [timer.py:215:stop] epoch=0/micro_step=3850/global_step=3850, RunningAvgSamplesPerSec=276.54249341202, CurrSamplesPerSec=293.59271687909103, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:48.125 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9738
dss8440-001: [2023-07-27 22:30:54,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:30:54,212] [INFO] [timer.py:215:stop] epoch=0/micro_step=3860/global_step=3860, RunningAvgSamplesPerSec=276.55261170824593, CurrSamplesPerSec=273.68361296216244, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:30:54.212 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9724
dss8440-001: [2023-07-27 22:31:00,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:00,312] [INFO] [timer.py:215:stop] epoch=0/micro_step=3870/global_step=3870, RunningAvgSamplesPerSec=276.5616275844125, CurrSamplesPerSec=277.85160886594497, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:00.313 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9719
dss8440-001: [2023-07-27 22:31:06,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:06,637] [INFO] [timer.py:215:stop] epoch=0/micro_step=3880/global_step=3880, RunningAvgSamplesPerSec=276.54450985795273, CurrSamplesPerSec=270.62364144367314, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:06.638 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9707
dss8440-001: [2023-07-27 22:31:12,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:12,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=3890/global_step=3890, RunningAvgSamplesPerSec=276.5434070047355, CurrSamplesPerSec=262.04943602940904, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:12.835 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9692
dss8440-001: [2023-07-27 22:31:18,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:19,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=3900, RunningAvgSamplesPerSec=276.5319907056848, CurrSamplesPerSec=268.0886259148165, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:19.112 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9684
dss8440-001: [2023-07-27 22:31:25,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:25,275] [INFO] [timer.py:215:stop] epoch=0/micro_step=3910/global_step=3910, RunningAvgSamplesPerSec=276.5355314527611, CurrSamplesPerSec=280.42336803456567, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:25.275 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9669
dss8440-001: [2023-07-27 22:31:31,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=3920, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:31,513] [INFO] [timer.py:215:stop] epoch=0/micro_step=3920/global_step=3920, RunningAvgSamplesPerSec=276.52978412132757, CurrSamplesPerSec=281.21796412395344, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:31.514 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9662
dss8440-001: [2023-07-27 22:31:37,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:37,624] [INFO] [timer.py:215:stop] epoch=0/micro_step=3930/global_step=3930, RunningAvgSamplesPerSec=276.5372442414149, CurrSamplesPerSec=277.4101226065194, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:37.624 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9654
dss8440-001: [2023-07-27 22:31:43,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:43,767] [INFO] [timer.py:215:stop] epoch=0/micro_step=3940/global_step=3940, RunningAvgSamplesPerSec=276.5408955580031, CurrSamplesPerSec=288.79358873159083, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:43.767 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9643
dss8440-001: [2023-07-27 22:31:49,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:49,871] [INFO] [timer.py:215:stop] epoch=0/micro_step=3950/global_step=3950, RunningAvgSamplesPerSec=276.5501104095309, CurrSamplesPerSec=280.79492543808266, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:49.871 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9638
dss8440-001: [2023-07-27 22:31:56,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:31:56,059] [INFO] [timer.py:215:stop] epoch=0/micro_step=3960/global_step=3960, RunningAvgSamplesPerSec=276.5497616543304, CurrSamplesPerSec=291.4456749268225, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:31:56.060 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9628
dss8440-001: [2023-07-27 22:32:02,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:02,233] [INFO] [timer.py:215:stop] epoch=0/micro_step=3970/global_step=3970, RunningAvgSamplesPerSec=276.5506375055428, CurrSamplesPerSec=287.03721723714847, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:02.234 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9616
dss8440-001: [2023-07-27 22:32:08,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:08,398] [INFO] [timer.py:215:stop] epoch=0/micro_step=3980/global_step=3980, RunningAvgSamplesPerSec=276.55220002567097, CurrSamplesPerSec=268.1261660834664, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:08.399 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9610
dss8440-001: [2023-07-27 22:32:14,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:14,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=3990/global_step=3990, RunningAvgSamplesPerSec=276.55941307713675, CurrSamplesPerSec=269.8980463655428, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:14.517 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9593
dss8440-001: [2023-07-27 22:32:20,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:20,797] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=276.5483783061878, CurrSamplesPerSec=271.6663712163826, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:20.798 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9584
dss8440-001: [2023-07-27 22:32:20,799] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
dss8440-001: [2023-07-27 22:32:20,802] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 22:32:20,802] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 22:32:20,884] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:32:20,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-003: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,906] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-003: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,907] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,904] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:32:20,904] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,906] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:32:20,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,907] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: [2023-07-27 22:32:20,907] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:32:20,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:32:20,907] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step4000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:32:20,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step4000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-002: [2023-07-27 22:32:20,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
dss8440-001: 2023-07-27 22:32:20.907 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 22:32:26,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:27,071] [INFO] [timer.py:215:stop] epoch=0/micro_step=4010/global_step=4010, RunningAvgSamplesPerSec=276.5495445455723, CurrSamplesPerSec=268.06139546986435, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:27.071 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9567
dss8440-001: [2023-07-27 22:32:33,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=20, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:33,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=4020/global_step=4020, RunningAvgSamplesPerSec=276.5518932360357, CurrSamplesPerSec=276.4863162157199, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:33.228 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9553
dss8440-001: [2023-07-27 22:32:33,307] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 22:32:38,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:38,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=4030/global_step=4030, RunningAvgSamplesPerSec=276.61663245078535, CurrSamplesPerSec=275.4212941664364, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:38.835 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9547
dss8440-001: [2023-07-27 22:32:44,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:45,035] [INFO] [timer.py:215:stop] epoch=0/micro_step=4040/global_step=4040, RunningAvgSamplesPerSec=276.6147495891382, CurrSamplesPerSec=271.1477599546242, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:45.036 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9536
dss8440-001: [2023-07-27 22:32:51,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:51,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=4050/global_step=4050, RunningAvgSamplesPerSec=276.61173995317967, CurrSamplesPerSec=263.7791757460777, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:51.242 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9525
dss8440-001: [2023-07-27 22:32:57,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:32:57,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=4060/global_step=4060, RunningAvgSamplesPerSec=276.61310153586186, CurrSamplesPerSec=285.9963974611761, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:32:57.415 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9516
dss8440-001: [2023-07-27 22:33:03,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:03,689] [INFO] [timer.py:215:stop] epoch=0/micro_step=4070/global_step=4070, RunningAvgSamplesPerSec=276.6033032935972, CurrSamplesPerSec=272.2909927271139, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:03.689 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9506
dss8440-001: [2023-07-27 22:33:09,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:09,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=4080/global_step=4080, RunningAvgSamplesPerSec=276.6070930391483, CurrSamplesPerSec=284.01174187385504, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:09.834 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9496
dss8440-001: [2023-07-27 22:33:15,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:16,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=4090/global_step=4090, RunningAvgSamplesPerSec=276.6047821347509, CurrSamplesPerSec=287.28203288334277, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:16.034 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9487
dss8440-001: [2023-07-27 22:33:22,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:22,305] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=4100, RunningAvgSamplesPerSec=276.59492050688505, CurrSamplesPerSec=272.63032797984994, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:22.305 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9483
dss8440-001: [2023-07-27 22:33:28,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:28,554] [INFO] [timer.py:215:stop] epoch=0/micro_step=4110/global_step=4110, RunningAvgSamplesPerSec=276.58641269634285, CurrSamplesPerSec=263.59386431693673, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:28.555 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9478
dss8440-001: [2023-07-27 22:33:34,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:34,739] [INFO] [timer.py:215:stop] epoch=0/micro_step=4120/global_step=4120, RunningAvgSamplesPerSec=276.5858629291331, CurrSamplesPerSec=276.2204532545255, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:34.740 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9467
dss8440-001: [2023-07-27 22:33:40,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:40,929] [INFO] [timer.py:215:stop] epoch=0/micro_step=4130/global_step=4130, RunningAvgSamplesPerSec=276.58513927850214, CurrSamplesPerSec=282.41199893069944, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:40.930 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9452
dss8440-001: [2023-07-27 22:33:47,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:47,136] [INFO] [timer.py:215:stop] epoch=0/micro_step=4140/global_step=4140, RunningAvgSamplesPerSec=276.5818571136506, CurrSamplesPerSec=293.1259274543116, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:47.137 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9446
dss8440-001: [2023-07-27 22:33:53,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:53,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=4150/global_step=4150, RunningAvgSamplesPerSec=276.57922650710935, CurrSamplesPerSec=267.7539838605059, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:53.340 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9439
dss8440-001: [2023-07-27 22:33:59,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=4160, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:33:59,632] [INFO] [timer.py:215:stop] epoch=0/micro_step=4160/global_step=4160, RunningAvgSamplesPerSec=276.56631908505733, CurrSamplesPerSec=282.09983465780573, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:33:59.633 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9428
dss8440-001: [2023-07-27 22:34:05,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=4170, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:05,721] [INFO] [timer.py:215:stop] epoch=0/micro_step=4170/global_step=4170, RunningAvgSamplesPerSec=276.5766297485419, CurrSamplesPerSec=289.59545092507886, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:05.721 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9421
dss8440-001: [2023-07-27 22:34:11,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=4180, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:11,943] [INFO] [timer.py:215:stop] epoch=0/micro_step=4180/global_step=4180, RunningAvgSamplesPerSec=276.5712524107121, CurrSamplesPerSec=255.3727275264173, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:11.944 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9413
dss8440-001: [2023-07-27 22:34:18,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=4190, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:18,128] [INFO] [timer.py:215:stop] epoch=0/micro_step=4190/global_step=4190, RunningAvgSamplesPerSec=276.57105529434415, CurrSamplesPerSec=285.29156870405944, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:18.129 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9401
dss8440-001: [2023-07-27 22:34:24,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=4200, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:24,360] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=4200, RunningAvgSamplesPerSec=276.5651674442934, CurrSamplesPerSec=278.86191717523735, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:24.360 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9394
dss8440-001: [2023-07-27 22:34:30,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=4210, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:30,515] [INFO] [timer.py:215:stop] epoch=0/micro_step=4210/global_step=4210, RunningAvgSamplesPerSec=276.5675360358214, CurrSamplesPerSec=270.56700866252993, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:30.516 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9386
dss8440-001: [2023-07-27 22:34:36,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=4220, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:36,769] [INFO] [timer.py:215:stop] epoch=0/micro_step=4220/global_step=4220, RunningAvgSamplesPerSec=276.55939263593416, CurrSamplesPerSec=281.3730420692073, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:36.770 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9374
dss8440-001: [2023-07-27 22:34:42,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=4230, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:43,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=4230/global_step=4230, RunningAvgSamplesPerSec=276.54863798353506, CurrSamplesPerSec=284.29534568459104, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:43.054 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9369
dss8440-001: [2023-07-27 22:34:49,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=4240, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:49,293] [INFO] [timer.py:215:stop] epoch=0/micro_step=4240/global_step=4240, RunningAvgSamplesPerSec=276.54295480513804, CurrSamplesPerSec=282.19608986470956, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:49.293 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9362
dss8440-001: [2023-07-27 22:34:55,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=4250, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:34:55,563] [INFO] [timer.py:215:stop] epoch=0/micro_step=4250/global_step=4250, RunningAvgSamplesPerSec=276.5330960386263, CurrSamplesPerSec=282.1695340402758, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:34:55.563 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9353
dss8440-001: [2023-07-27 22:35:01,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=4260, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:01,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=4260/global_step=4260, RunningAvgSamplesPerSec=276.538789494325, CurrSamplesPerSec=268.639314862936, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:01.697 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9345
dss8440-001: [2023-07-27 22:35:07,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=4270, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:07,794] [INFO] [timer.py:215:stop] epoch=0/micro_step=4270/global_step=4270, RunningAvgSamplesPerSec=276.5468777857468, CurrSamplesPerSec=268.27775977777634, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:07.794 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9328
dss8440-001: [2023-07-27 22:35:13,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=4280, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:13,881] [INFO] [timer.py:215:stop] epoch=0/micro_step=4280/global_step=4280, RunningAvgSamplesPerSec=276.5567448147317, CurrSamplesPerSec=284.2100331383324, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:13.882 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9320
dss8440-001: [2023-07-27 22:35:20,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=4290, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:20,166] [INFO] [timer.py:215:stop] epoch=0/micro_step=4290/global_step=4290, RunningAvgSamplesPerSec=276.54616997609054, CurrSamplesPerSec=288.1926249119647, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:20.167 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9311
dss8440-001: [2023-07-27 22:35:26,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=4300, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:26,379] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=4300, RunningAvgSamplesPerSec=276.5423122991434, CurrSamplesPerSec=271.434157164869, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:26.380 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9302
dss8440-001: [2023-07-27 22:35:32,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=4310, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:32,600] [INFO] [timer.py:215:stop] epoch=0/micro_step=4310/global_step=4310, RunningAvgSamplesPerSec=276.53780708708587, CurrSamplesPerSec=290.6200031345118, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:32.601 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9289
dss8440-001: [2023-07-27 22:35:38,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=4320, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:38,816] [INFO] [timer.py:215:stop] epoch=0/micro_step=4320/global_step=4320, RunningAvgSamplesPerSec=276.53427537898114, CurrSamplesPerSec=278.0786954857398, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:38.816 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9277
dss8440-001: [2023-07-27 22:35:45,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=4330, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:45,057] [INFO] [timer.py:215:stop] epoch=0/micro_step=4330/global_step=4330, RunningAvgSamplesPerSec=276.5286780189825, CurrSamplesPerSec=262.03657280385795, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:45.058 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9271
dss8440-001: [2023-07-27 22:35:51,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=4340, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:51,271] [INFO] [timer.py:215:stop] epoch=0/micro_step=4340/global_step=4340, RunningAvgSamplesPerSec=276.52491857859945, CurrSamplesPerSec=278.63355007900594, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:51.272 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9265
dss8440-001: [2023-07-27 22:35:57,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=4350, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:35:57,454] [INFO] [timer.py:215:stop] epoch=0/micro_step=4350/global_step=4350, RunningAvgSamplesPerSec=276.52436624044276, CurrSamplesPerSec=284.359593220339, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:35:57.454 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9256
dss8440-001: [2023-07-27 22:36:03,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=4360, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:03,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=4360/global_step=4360, RunningAvgSamplesPerSec=276.5168319161523, CurrSamplesPerSec=279.496600512075, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:03.704 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9249
dss8440-001: [2023-07-27 22:36:09,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=4370, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:09,839] [INFO] [timer.py:215:stop] epoch=0/micro_step=4370/global_step=4370, RunningAvgSamplesPerSec=276.52137130664465, CurrSamplesPerSec=293.48339750200336, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:09.840 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9243
dss8440-001: [2023-07-27 22:36:15,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=4380, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:16,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=4380/global_step=4380, RunningAvgSamplesPerSec=276.51943094966924, CurrSamplesPerSec=278.6862254523134, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:16.046 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9233
dss8440-001: [2023-07-27 22:36:22,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=4390, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:22,205] [INFO] [timer.py:215:stop] epoch=0/micro_step=4390/global_step=4390, RunningAvgSamplesPerSec=276.5215936105024, CurrSamplesPerSec=289.432249637513, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:22.205 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9229
dss8440-001: [2023-07-27 22:36:28,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=4400, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:28,402] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=4400, RunningAvgSamplesPerSec=276.51965549907885, CurrSamplesPerSec=274.7021176811818, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:28.403 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9219
dss8440-001: [2023-07-27 22:36:34,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=4410, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:34,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=4410/global_step=4410, RunningAvgSamplesPerSec=276.5262527671504, CurrSamplesPerSec=292.06308110617414, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:34.516 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9209
dss8440-001: [2023-07-27 22:36:40,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=4420, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:40,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=4420/global_step=4420, RunningAvgSamplesPerSec=276.50968991753416, CurrSamplesPerSec=269.00235925768135, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:40.852 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9203
dss8440-001: [2023-07-27 22:36:46,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=4430, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:46,967] [INFO] [timer.py:215:stop] epoch=0/micro_step=4430/global_step=4430, RunningAvgSamplesPerSec=276.5160530129849, CurrSamplesPerSec=286.2879354034784, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:46.968 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9191
dss8440-001: [2023-07-27 22:36:53,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=4440, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:53,203] [INFO] [timer.py:215:stop] epoch=0/micro_step=4440/global_step=4440, RunningAvgSamplesPerSec=276.510749532633, CurrSamplesPerSec=281.8483223930124, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:53.204 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9181
dss8440-001: [2023-07-27 22:36:59,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=4450, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:36:59,445] [INFO] [timer.py:215:stop] epoch=0/micro_step=4450/global_step=4450, RunningAvgSamplesPerSec=276.50359623206134, CurrSamplesPerSec=267.703020535814, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:36:59.445 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9176
dss8440-001: [2023-07-27 22:37:05,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=4460, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:05,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=4460/global_step=4460, RunningAvgSamplesPerSec=276.51364007033715, CurrSamplesPerSec=264.11721499686274, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:05.527 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9165
dss8440-001: [2023-07-27 22:37:11,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=4470, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:11,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=4470/global_step=4470, RunningAvgSamplesPerSec=276.5227757234444, CurrSamplesPerSec=282.1888571449167, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:11.613 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9154
dss8440-001: [2023-07-27 22:37:17,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=4480, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:17,815] [INFO] [timer.py:215:stop] epoch=0/micro_step=4480/global_step=4480, RunningAvgSamplesPerSec=276.52060037903453, CurrSamplesPerSec=270.43129043412586, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:17.816 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9149
dss8440-001: [2023-07-27 22:37:23,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=4490, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:24,093] [INFO] [timer.py:215:stop] epoch=0/micro_step=4490/global_step=4490, RunningAvgSamplesPerSec=276.51083139742803, CurrSamplesPerSec=286.7883829721768, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:24.093 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9144
dss8440-001: [2023-07-27 22:37:30,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=4500, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:30,358] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=4500, RunningAvgSamplesPerSec=276.5020472818747, CurrSamplesPerSec=267.79804564221996, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:30.358 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9139
dss8440-001: [2023-07-27 22:37:36,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=4510, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:36,609] [INFO] [timer.py:215:stop] epoch=0/micro_step=4510/global_step=4510, RunningAvgSamplesPerSec=276.4945881472847, CurrSamplesPerSec=266.0165799063676, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:36.610 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9133
dss8440-001: [2023-07-27 22:37:42,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=4520, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:42,850] [INFO] [timer.py:215:stop] epoch=0/micro_step=4520/global_step=4520, RunningAvgSamplesPerSec=276.48808201182123, CurrSamplesPerSec=292.12846565233616, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:42.850 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9122
dss8440-001: [2023-07-27 22:37:48,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=4530, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:49,061] [INFO] [timer.py:215:stop] epoch=0/micro_step=4530/global_step=4530, RunningAvgSamplesPerSec=276.4850832825829, CurrSamplesPerSec=262.9556603847516, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:49.062 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9112
dss8440-001: [2023-07-27 22:37:55,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=4540, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:37:55,229] [INFO] [timer.py:215:stop] epoch=0/micro_step=4540/global_step=4540, RunningAvgSamplesPerSec=276.48596871318665, CurrSamplesPerSec=281.1012912076843, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:37:55.230 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9105
dss8440-001: [2023-07-27 22:38:01,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=4550, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:01,488] [INFO] [timer.py:215:stop] epoch=0/micro_step=4550/global_step=4550, RunningAvgSamplesPerSec=276.4780599451767, CurrSamplesPerSec=280.2188452555598, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:01.488 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9096
dss8440-001: [2023-07-27 22:38:07,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=4560, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:07,808] [INFO] [timer.py:215:stop] epoch=0/micro_step=4560/global_step=4560, RunningAvgSamplesPerSec=276.46404209567686, CurrSamplesPerSec=283.26414515616165, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:07.809 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9089
dss8440-001: [2023-07-27 22:38:13,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=4570, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:13,979] [INFO] [timer.py:215:stop] epoch=0/micro_step=4570/global_step=4570, RunningAvgSamplesPerSec=276.4655268486339, CurrSamplesPerSec=263.4357958426346, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:13.979 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9082
dss8440-001: [2023-07-27 22:38:19,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=4580, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:20,018] [INFO] [timer.py:215:stop] epoch=0/micro_step=4580/global_step=4580, RunningAvgSamplesPerSec=276.4793198042619, CurrSamplesPerSec=286.8541127016171, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:20.019 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9076
dss8440-001: [2023-07-27 22:38:26,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=4590, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:26,212] [INFO] [timer.py:215:stop] epoch=0/micro_step=4590/global_step=4590, RunningAvgSamplesPerSec=276.4778883397053, CurrSamplesPerSec=273.2375672097736, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:26.212 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9070
dss8440-001: [2023-07-27 22:38:32,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=4600, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:32,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=4600, RunningAvgSamplesPerSec=276.47509002633996, CurrSamplesPerSec=276.82923477463527, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:32.416 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9062
dss8440-001: [2023-07-27 22:38:38,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=4610, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:38,579] [INFO] [timer.py:215:stop] epoch=0/micro_step=4610/global_step=4610, RunningAvgSamplesPerSec=276.4766197678896, CurrSamplesPerSec=281.8779750189614, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:38.580 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9051
dss8440-001: [2023-07-27 22:38:44,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=4620, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:44,746] [INFO] [timer.py:215:stop] epoch=0/micro_step=4620/global_step=4620, RunningAvgSamplesPerSec=276.4784241337191, CurrSamplesPerSec=282.26346237505015, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:44.747 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9043
dss8440-001: [2023-07-27 22:38:50,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=4630, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:50,948] [INFO] [timer.py:215:stop] epoch=0/micro_step=4630/global_step=4630, RunningAvgSamplesPerSec=276.4772308603418, CurrSamplesPerSec=269.7876832566123, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:50.948 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9039
dss8440-001: [2023-07-27 22:38:56,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=4640, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:38:57,045] [INFO] [timer.py:215:stop] epoch=0/micro_step=4640/global_step=4640, RunningAvgSamplesPerSec=276.4853476477858, CurrSamplesPerSec=289.94077768176766, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:38:57.046 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9030
dss8440-001: [2023-07-27 22:39:03,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=4650, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:03,279] [INFO] [timer.py:215:stop] epoch=0/micro_step=4650/global_step=4650, RunningAvgSamplesPerSec=276.48072030390745, CurrSamplesPerSec=284.74212208410734, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:03.279 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9022
dss8440-001: [2023-07-27 22:39:09,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=4660, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:09,461] [INFO] [timer.py:215:stop] epoch=0/micro_step=4660/global_step=4660, RunningAvgSamplesPerSec=276.479963391616, CurrSamplesPerSec=276.65185809875703, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:09.462 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9013
dss8440-001: [2023-07-27 22:39:15,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=4670, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:15,763] [INFO] [timer.py:215:stop] epoch=0/micro_step=4670/global_step=4670, RunningAvgSamplesPerSec=276.4680773397581, CurrSamplesPerSec=272.53163239021853, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:15.763 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.9004
dss8440-001: [2023-07-27 22:39:21,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=4680, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:22,011] [INFO] [timer.py:215:stop] epoch=0/micro_step=4680/global_step=4680, RunningAvgSamplesPerSec=276.46096787835853, CurrSamplesPerSec=281.64329255497523, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:22.012 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8993
dss8440-001: [2023-07-27 22:39:28,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=4690, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:28,126] [INFO] [timer.py:215:stop] epoch=0/micro_step=4690/global_step=4690, RunningAvgSamplesPerSec=276.4672915072403, CurrSamplesPerSec=283.3989189189189, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:28.126 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8984
dss8440-001: [2023-07-27 22:39:34,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=4700, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:34,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=4700/global_step=4700, RunningAvgSamplesPerSec=276.4638053590528, CurrSamplesPerSec=288.3283550698456, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:34.339 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8977
dss8440-001: [2023-07-27 22:39:40,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=4710, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:40,553] [INFO] [timer.py:215:stop] epoch=0/micro_step=4710/global_step=4710, RunningAvgSamplesPerSec=276.46150764046297, CurrSamplesPerSec=258.2850029745312, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:40.554 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8967
dss8440-001: [2023-07-27 22:39:46,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=4720, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:46,790] [INFO] [timer.py:215:stop] epoch=0/micro_step=4720/global_step=4720, RunningAvgSamplesPerSec=276.45626758945144, CurrSamplesPerSec=259.23369670187043, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:46.790 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8957
dss8440-001: [2023-07-27 22:39:52,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=4730, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:52,933] [INFO] [timer.py:215:stop] epoch=0/micro_step=4730/global_step=4730, RunningAvgSamplesPerSec=276.4597489977431, CurrSamplesPerSec=277.484626062507, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:52.933 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8948
dss8440-001: [2023-07-27 22:39:59,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=4740, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:39:59,071] [INFO] [timer.py:215:stop] epoch=0/micro_step=4740/global_step=4740, RunningAvgSamplesPerSec=276.4634189308445, CurrSamplesPerSec=284.1460823223228, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:39:59.072 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8943
dss8440-001: [2023-07-27 22:40:05,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=4750, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:05,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=4750/global_step=4750, RunningAvgSamplesPerSec=276.464518904842, CurrSamplesPerSec=289.49384483158127, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:05.243 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8937
dss8440-001: [2023-07-27 22:40:11,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=4760, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:11,383] [INFO] [timer.py:215:stop] epoch=0/micro_step=4760/global_step=4760, RunningAvgSamplesPerSec=276.468167638453, CurrSamplesPerSec=283.0345392695456, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:11.383 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8929
dss8440-001: [2023-07-27 22:40:17,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=4770, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:17,488] [INFO] [timer.py:215:stop] epoch=0/micro_step=4770/global_step=4770, RunningAvgSamplesPerSec=276.4751874526888, CurrSamplesPerSec=291.9893769768474, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:17.489 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8918
dss8440-001: [2023-07-27 22:40:23,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=4780, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:23,643] [INFO] [timer.py:215:stop] epoch=0/micro_step=4780/global_step=4780, RunningAvgSamplesPerSec=276.4784706050001, CurrSamplesPerSec=284.6714911616238, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:23.644 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8910
dss8440-001: [2023-07-27 22:40:29,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=4790, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:29,904] [INFO] [timer.py:215:stop] epoch=0/micro_step=4790/global_step=4790, RunningAvgSamplesPerSec=276.4709536601613, CurrSamplesPerSec=258.3370839903491, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:29.904 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8906
dss8440-001: [2023-07-27 22:40:36,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=4800, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:36,143] [INFO] [timer.py:215:stop] epoch=0/micro_step=4800/global_step=4800, RunningAvgSamplesPerSec=276.4651663313877, CurrSamplesPerSec=273.9064903240653, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:36.143 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8902
dss8440-001: [2023-07-27 22:40:42,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=4810, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:42,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=4810/global_step=4810, RunningAvgSamplesPerSec=276.4631879022131, CurrSamplesPerSec=267.4139855113439, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:42.349 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8898
dss8440-001: [2023-07-27 22:40:48,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=4820, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:48,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=4820/global_step=4820, RunningAvgSamplesPerSec=276.4528226637254, CurrSamplesPerSec=263.19826178744995, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:48.638 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8890
dss8440-001: [2023-07-27 22:40:54,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=4830, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:40:54,847] [INFO] [timer.py:215:stop] epoch=0/micro_step=4830/global_step=4830, RunningAvgSamplesPerSec=276.4506934223089, CurrSamplesPerSec=270.05247880451407, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:40:54.847 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8881
dss8440-001: [2023-07-27 22:41:00,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=4840, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:01,026] [INFO] [timer.py:215:stop] epoch=0/micro_step=4840/global_step=4840, RunningAvgSamplesPerSec=276.45134211669034, CurrSamplesPerSec=298.27904106144615, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:01.027 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8871
dss8440-001: [2023-07-27 22:41:07,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=4850, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:07,305] [INFO] [timer.py:215:stop] epoch=0/micro_step=4850/global_step=4850, RunningAvgSamplesPerSec=276.44241820229087, CurrSamplesPerSec=283.18218159122904, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:07.305 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8862
dss8440-001: [2023-07-27 22:41:13,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=4860, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:13,551] [INFO] [timer.py:215:stop] epoch=0/micro_step=4860/global_step=4860, RunningAvgSamplesPerSec=276.4366072451863, CurrSamplesPerSec=275.5280195571488, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:13.552 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8857
dss8440-001: [2023-07-27 22:41:19,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=4870, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:19,821] [INFO] [timer.py:215:stop] epoch=0/micro_step=4870/global_step=4870, RunningAvgSamplesPerSec=276.42862909624995, CurrSamplesPerSec=275.76891304534996, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:19.822 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8850
dss8440-001: [2023-07-27 22:41:25,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=4880, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:25,924] [INFO] [timer.py:215:stop] epoch=0/micro_step=4880/global_step=4880, RunningAvgSamplesPerSec=276.4356082431297, CurrSamplesPerSec=262.47746746703865, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:25.925 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8843
dss8440-001: [2023-07-27 22:41:32,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=4890, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:32,075] [INFO] [timer.py:215:stop] epoch=0/micro_step=4890/global_step=4890, RunningAvgSamplesPerSec=276.4394415128561, CurrSamplesPerSec=282.016963207961, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:32.075 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8839
dss8440-001: [2023-07-27 22:41:38,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=4900, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:38,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=4900/global_step=4900, RunningAvgSamplesPerSec=276.4420975414604, CurrSamplesPerSec=268.6532442299891, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:38.229 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8830
dss8440-001: [2023-07-27 22:41:44,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=4910, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:44,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=4910/global_step=4910, RunningAvgSamplesPerSec=276.43922041383723, CurrSamplesPerSec=290.2859722914547, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:44.442 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8824
dss8440-001: [2023-07-27 22:41:50,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=4920, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:50,594] [INFO] [timer.py:215:stop] epoch=0/micro_step=4920/global_step=4920, RunningAvgSamplesPerSec=276.44173514113294, CurrSamplesPerSec=274.274632315019, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:50.594 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8818
dss8440-001: [2023-07-27 22:41:56,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=4930, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:41:56,736] [INFO] [timer.py:215:stop] epoch=0/micro_step=4930/global_step=4930, RunningAvgSamplesPerSec=276.4449019221429, CurrSamplesPerSec=282.723833895657, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:41:56.736 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8812
dss8440-001: [2023-07-27 22:42:02,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=4940, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:02,943] [INFO] [timer.py:215:stop] epoch=0/micro_step=4940/global_step=4940, RunningAvgSamplesPerSec=276.4423844500116, CurrSamplesPerSec=283.2543525686094, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:02.944 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8800
dss8440-001: [2023-07-27 22:42:08,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=4950, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:09,105] [INFO] [timer.py:215:stop] epoch=0/micro_step=4950/global_step=4950, RunningAvgSamplesPerSec=276.44440708296685, CurrSamplesPerSec=271.5014050090219, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:09.106 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8796
dss8440-001: [2023-07-27 22:42:15,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=4960, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:15,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=4960/global_step=4960, RunningAvgSamplesPerSec=276.43647222992945, CurrSamplesPerSec=274.4342221846604, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:15.382 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8788
dss8440-001: [2023-07-27 22:42:21,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=4970, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:21,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=4970/global_step=4970, RunningAvgSamplesPerSec=276.43023572267845, CurrSamplesPerSec=276.6916176517362, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:21.626 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8774
dss8440-001: [2023-07-27 22:42:27,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=4980, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:27,855] [INFO] [timer.py:215:stop] epoch=0/micro_step=4980/global_step=4980, RunningAvgSamplesPerSec=276.42623419931857, CurrSamplesPerSec=272.92959265249965, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:27.856 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8765
dss8440-001: [2023-07-27 22:42:33,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=4990, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:33,991] [INFO] [timer.py:215:stop] epoch=0/micro_step=4990/global_step=4990, RunningAvgSamplesPerSec=276.43081502361514, CurrSamplesPerSec=283.17432924016185, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:33.991 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8756
dss8440-001: [2023-07-27 22:42:40,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:40,238] [INFO] [timer.py:215:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=276.4250908926268, CurrSamplesPerSec=262.0726320064625, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:40.238 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8752
dss8440-001: [2023-07-27 22:42:40,239] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
dss8440-001: [2023-07-27 22:42:40,242] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 22:42:40,242] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 22:42:40,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:42:40,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:42:40,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:42:40,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,316] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:42:40,315] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:42:40,315] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:42:40,316] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:42:40,316] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:42:40,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step5000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:42:40,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-003: [2023-07-27 22:42:40,319] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:42:40,319] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step5000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:42:40,319] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
dss8440-001: 2023-07-27 22:42:40.319 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 22:42:46,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=5010, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:46,520] [INFO] [timer.py:215:stop] epoch=0/micro_step=5010/global_step=5010, RunningAvgSamplesPerSec=276.424013820856, CurrSamplesPerSec=273.0072087164796, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:46.520 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8743
dss8440-001: [2023-07-27 22:42:52,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=5020, skipped=21, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:52,796] [INFO] [timer.py:215:stop] epoch=0/micro_step=5020/global_step=5020, RunningAvgSamplesPerSec=276.41538545399925, CurrSamplesPerSec=277.23058304566746, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:52.797 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8738
dss8440-001: [2023-07-27 22:42:53,524] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 22:42:58,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=5030, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:42:58,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=5030/global_step=5030, RunningAvgSamplesPerSec=276.46238822905184, CurrSamplesPerSec=280.2108220937062, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:42:58.461 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8736
dss8440-001: [2023-07-27 22:43:04,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=5040, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:04,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=5040/global_step=5040, RunningAvgSamplesPerSec=276.46533035698764, CurrSamplesPerSec=294.23314568034857, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:04.614 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8728
dss8440-001: [2023-07-27 22:43:10,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=5050, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:10,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=5050/global_step=5050, RunningAvgSamplesPerSec=276.4621718210596, CurrSamplesPerSec=267.87572329817505, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:10.839 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8717
dss8440-001: [2023-07-27 22:43:16,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=5060, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:16,968] [INFO] [timer.py:215:stop] epoch=0/micro_step=5060/global_step=5060, RunningAvgSamplesPerSec=276.4670868528259, CurrSamplesPerSec=295.25520121882744, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:16.969 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8709
dss8440-001: [2023-07-27 22:43:23,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=5070, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:23,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=5070/global_step=5070, RunningAvgSamplesPerSec=276.4676438458505, CurrSamplesPerSec=277.4841889754229, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:23.140 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8701
dss8440-001: [2023-07-27 22:43:29,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=5080, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:29,388] [INFO] [timer.py:215:stop] epoch=0/micro_step=5080/global_step=5080, RunningAvgSamplesPerSec=276.4614880813479, CurrSamplesPerSec=268.73336613159734, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:29.388 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8691
dss8440-001: [2023-07-27 22:43:35,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=5090, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:35,568] [INFO] [timer.py:215:stop] epoch=0/micro_step=5090/global_step=5090, RunningAvgSamplesPerSec=276.46152297485907, CurrSamplesPerSec=283.89720052408586, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:35.568 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8680
dss8440-001: [2023-07-27 22:43:41,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=5100, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:41,753] [INFO] [timer.py:215:stop] epoch=0/micro_step=5100/global_step=5100, RunningAvgSamplesPerSec=276.4610986914496, CurrSamplesPerSec=276.89885635895155, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:41.753 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8674
dss8440-001: [2023-07-27 22:43:47,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=5110, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:48,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=5110/global_step=5110, RunningAvgSamplesPerSec=276.45079153175993, CurrSamplesPerSec=290.9687403219373, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:48.049 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8668
dss8440-001: [2023-07-27 22:43:54,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=5120, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:43:54,301] [INFO] [timer.py:215:stop] epoch=0/micro_step=5120/global_step=5120, RunningAvgSamplesPerSec=276.44460832825933, CurrSamplesPerSec=279.9575488564795, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:43:54.301 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8660
dss8440-001: [2023-07-27 22:44:00,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=5130, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:00,591] [INFO] [timer.py:215:stop] epoch=0/micro_step=5130/global_step=5130, RunningAvgSamplesPerSec=276.43469714439476, CurrSamplesPerSec=270.08342791041565, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:00.592 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8651
dss8440-001: [2023-07-27 22:44:06,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=5140, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:06,730] [INFO] [timer.py:215:stop] epoch=0/micro_step=5140/global_step=5140, RunningAvgSamplesPerSec=276.43830581573974, CurrSamplesPerSec=278.4079766951287, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:06.731 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8643
dss8440-001: [2023-07-27 22:44:12,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=5150, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:12,956] [INFO] [timer.py:215:stop] epoch=0/micro_step=5150/global_step=5150, RunningAvgSamplesPerSec=276.43450196898715, CurrSamplesPerSec=270.3502567518209, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:12.957 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8632
dss8440-001: [2023-07-27 22:44:19,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=5160, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:19,098] [INFO] [timer.py:215:stop] epoch=0/micro_step=5160/global_step=5160, RunningAvgSamplesPerSec=276.43838885448656, CurrSamplesPerSec=294.29606395766007, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:19.098 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8623
dss8440-001: [2023-07-27 22:44:25,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=5170, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:25,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=5170/global_step=5170, RunningAvgSamplesPerSec=276.4432966076178, CurrSamplesPerSec=284.17908560099437, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:25.229 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8617
dss8440-001: [2023-07-27 22:44:31,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=5180, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:31,499] [INFO] [timer.py:215:stop] epoch=0/micro_step=5180/global_step=5180, RunningAvgSamplesPerSec=276.43585231827836, CurrSamplesPerSec=258.64868251768786, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:31.499 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8609
dss8440-001: [2023-07-27 22:44:37,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=5190, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:37,543] [INFO] [timer.py:215:stop] epoch=0/micro_step=5190/global_step=5190, RunningAvgSamplesPerSec=276.44753670203175, CurrSamplesPerSec=284.88786591763176, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:37.543 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8601
dss8440-001: [2023-07-27 22:44:43,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=5200, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:43,752] [INFO] [timer.py:215:stop] epoch=0/micro_step=5200/global_step=5200, RunningAvgSamplesPerSec=276.44463782368916, CurrSamplesPerSec=289.03583849930413, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:43.753 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8593
dss8440-001: [2023-07-27 22:44:49,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=5210, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:49,863] [INFO] [timer.py:215:stop] epoch=0/micro_step=5210/global_step=5210, RunningAvgSamplesPerSec=276.4505418913728, CurrSamplesPerSec=275.7934141615136, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:49.864 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8590
dss8440-001: [2023-07-27 22:44:55,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=5220, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:44:56,019] [INFO] [timer.py:215:stop] epoch=0/micro_step=5220/global_step=5220, RunningAvgSamplesPerSec=276.4529481727287, CurrSamplesPerSec=274.9501510650643, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:44:56.019 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8583
dss8440-001: [2023-07-27 22:45:02,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=5230, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:02,275] [INFO] [timer.py:215:stop] epoch=0/micro_step=5230/global_step=5230, RunningAvgSamplesPerSec=276.4462819317089, CurrSamplesPerSec=277.4375378470809, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:02.276 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8572
dss8440-001: [2023-07-27 22:45:08,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=5240, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:08,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=5240/global_step=5240, RunningAvgSamplesPerSec=276.4434866020586, CurrSamplesPerSec=267.9993062718502, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:08.495 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8567
dss8440-001: [2023-07-27 22:45:14,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=5250, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:14,755] [INFO] [timer.py:215:stop] epoch=0/micro_step=5250/global_step=5250, RunningAvgSamplesPerSec=276.4365018097567, CurrSamplesPerSec=291.81416143965004, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:14.755 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8560
dss8440-001: [2023-07-27 22:45:20,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=5260, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:20,927] [INFO] [timer.py:215:stop] epoch=0/micro_step=5260/global_step=5260, RunningAvgSamplesPerSec=276.43703915363545, CurrSamplesPerSec=270.13167711768796, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:20.927 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8549
dss8440-001: [2023-07-27 22:45:26,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=5270, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:27,031] [INFO] [timer.py:215:stop] epoch=0/micro_step=5270/global_step=5270, RunningAvgSamplesPerSec=276.44380719763427, CurrSamplesPerSec=290.1757552097489, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:27.031 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8542
dss8440-001: [2023-07-27 22:45:33,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=5280, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:33,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=5280/global_step=5280, RunningAvgSamplesPerSec=276.44726904727094, CurrSamplesPerSec=269.6403632230883, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:33.175 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8535
dss8440-001: [2023-07-27 22:45:39,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=5290, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:39,342] [INFO] [timer.py:215:stop] epoch=0/micro_step=5290/global_step=5290, RunningAvgSamplesPerSec=276.44899666514215, CurrSamplesPerSec=259.11291867013995, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:39.342 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8531
dss8440-001: [2023-07-27 22:45:45,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=5300, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:45,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=5300/global_step=5300, RunningAvgSamplesPerSec=276.44330457851606, CurrSamplesPerSec=269.5707338907284, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:45.593 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8524
dss8440-001: [2023-07-27 22:45:51,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=5310, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:51,780] [INFO] [timer.py:215:stop] epoch=0/micro_step=5310/global_step=5310, RunningAvgSamplesPerSec=276.44291647819045, CurrSamplesPerSec=273.76867844170414, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:51.781 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8512
dss8440-001: [2023-07-27 22:45:57,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=5320, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:45:57,874] [INFO] [timer.py:215:stop] epoch=0/micro_step=5320/global_step=5320, RunningAvgSamplesPerSec=276.45120730903324, CurrSamplesPerSec=277.3326028510064, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:45:57.875 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8505
dss8440-001: [2023-07-27 22:46:03,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=5330, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:04,052] [INFO] [timer.py:215:stop] epoch=0/micro_step=5330/global_step=5330, RunningAvgSamplesPerSec=276.4528528920215, CurrSamplesPerSec=274.25787226887616, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:04.052 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8500
dss8440-001: [2023-07-27 22:46:10,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=5340, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:10,284] [INFO] [timer.py:215:stop] epoch=0/micro_step=5340/global_step=5340, RunningAvgSamplesPerSec=276.4486151801692, CurrSamplesPerSec=267.3557461846463, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:10.284 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8495
dss8440-001: [2023-07-27 22:46:16,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=5350, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:16,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=5350/global_step=5350, RunningAvgSamplesPerSec=276.4465243800058, CurrSamplesPerSec=284.95537764793943, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:16.491 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8490
dss8440-001: [2023-07-27 22:46:22,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=5360, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:22,714] [INFO] [timer.py:215:stop] epoch=0/micro_step=5360/global_step=5360, RunningAvgSamplesPerSec=276.44280242639144, CurrSamplesPerSec=281.4294560268392, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:22.715 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8485
dss8440-001: [2023-07-27 22:46:28,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=5370, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:28,999] [INFO] [timer.py:215:stop] epoch=0/micro_step=5370/global_step=5370, RunningAvgSamplesPerSec=276.4353938738135, CurrSamplesPerSec=276.9557762541643, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:29.000 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8476
dss8440-001: [2023-07-27 22:46:35,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=5380, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:35,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=5380/global_step=5380, RunningAvgSamplesPerSec=276.4361097664674, CurrSamplesPerSec=272.412575830912, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:35.178 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8469
dss8440-001: [2023-07-27 22:46:41,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=5390, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:41,352] [INFO] [timer.py:215:stop] epoch=0/micro_step=5390/global_step=5390, RunningAvgSamplesPerSec=276.4365056718653, CurrSamplesPerSec=282.25351273187556, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:41.352 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8464
dss8440-001: [2023-07-27 22:46:47,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=5400, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:47,569] [INFO] [timer.py:215:stop] epoch=0/micro_step=5400/global_step=5400, RunningAvgSamplesPerSec=276.43313906716276, CurrSamplesPerSec=278.6807145433491, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:47.569 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8456
dss8440-001: [2023-07-27 22:46:53,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=5410, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:53,681] [INFO] [timer.py:215:stop] epoch=0/micro_step=5410/global_step=5410, RunningAvgSamplesPerSec=276.43967481290707, CurrSamplesPerSec=272.3782478714744, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:53.682 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8448
dss8440-001: [2023-07-27 22:46:59,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=5420, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:46:59,845] [INFO] [timer.py:215:stop] epoch=0/micro_step=5420/global_step=5420, RunningAvgSamplesPerSec=276.4409463394895, CurrSamplesPerSec=264.75013413287047, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:46:59.846 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8440
dss8440-001: [2023-07-27 22:47:06,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=5430, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:06,137] [INFO] [timer.py:215:stop] epoch=0/micro_step=5430/global_step=5430, RunningAvgSamplesPerSec=276.4334231658861, CurrSamplesPerSec=271.78624388990823, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:06.138 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8434
dss8440-001: [2023-07-27 22:47:12,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=5440, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:12,385] [INFO] [timer.py:215:stop] epoch=0/micro_step=5440/global_step=5440, RunningAvgSamplesPerSec=276.42771793350676, CurrSamplesPerSec=272.5443870930311, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:12.385 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8430
dss8440-001: [2023-07-27 22:47:18,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=5450, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:18,542] [INFO] [timer.py:215:stop] epoch=0/micro_step=5450/global_step=5450, RunningAvgSamplesPerSec=276.42970005452855, CurrSamplesPerSec=288.8190384299304, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:18.542 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8427
dss8440-001: [2023-07-27 22:47:24,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=5460, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:24,810] [INFO] [timer.py:215:stop] epoch=0/micro_step=5460/global_step=5460, RunningAvgSamplesPerSec=276.42213857572943, CurrSamplesPerSec=274.37299207147123, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:24.811 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8421
dss8440-001: [2023-07-27 22:47:30,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=5470, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:31,089] [INFO] [timer.py:215:stop] epoch=0/micro_step=5470/global_step=5470, RunningAvgSamplesPerSec=276.4138814293022, CurrSamplesPerSec=283.46869161811065, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:31.089 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8420
dss8440-001: [2023-07-27 22:47:37,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=5480, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:37,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=5480/global_step=5480, RunningAvgSamplesPerSec=276.4169566146785, CurrSamplesPerSec=288.56616119485136, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:37.231 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8415
dss8440-001: [2023-07-27 22:47:43,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=5490, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:43,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=5490/global_step=5490, RunningAvgSamplesPerSec=276.4152322723233, CurrSamplesPerSec=288.7254293124854, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:43.439 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8406
dss8440-001: [2023-07-27 22:47:49,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=5500, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:49,594] [INFO] [timer.py:215:stop] epoch=0/micro_step=5500/global_step=5500, RunningAvgSamplesPerSec=276.4183417299076, CurrSamplesPerSec=280.33958243652165, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:49.595 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8397
dss8440-001: [2023-07-27 22:47:55,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=5510, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:47:55,757] [INFO] [timer.py:215:stop] epoch=0/micro_step=5510/global_step=5510, RunningAvgSamplesPerSec=276.420561179262, CurrSamplesPerSec=269.09378754423204, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:47:55.757 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8391
dss8440-001: [2023-07-27 22:48:01,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=5520, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:02,043] [INFO] [timer.py:215:stop] epoch=0/micro_step=5520/global_step=5520, RunningAvgSamplesPerSec=276.4117358276624, CurrSamplesPerSec=265.18574361877006, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:02.044 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8384
dss8440-001: [2023-07-27 22:48:08,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=5530, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:08,168] [INFO] [timer.py:215:stop] epoch=0/micro_step=5530/global_step=5530, RunningAvgSamplesPerSec=276.41700096011573, CurrSamplesPerSec=270.6669893268617, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:08.168 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8377
dss8440-001: [2023-07-27 22:48:14,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=5540, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:14,394] [INFO] [timer.py:215:stop] epoch=0/micro_step=5540/global_step=5540, RunningAvgSamplesPerSec=276.41363940809566, CurrSamplesPerSec=267.5733093142173, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:14.395 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8370
dss8440-001: [2023-07-27 22:48:20,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=5550, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:20,679] [INFO] [timer.py:215:stop] epoch=0/micro_step=5550/global_step=5550, RunningAvgSamplesPerSec=276.4055408664083, CurrSamplesPerSec=271.88597384096687, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:20.680 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8363
dss8440-001: [2023-07-27 22:48:26,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=5560, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:26,888] [INFO] [timer.py:215:stop] epoch=0/micro_step=5560/global_step=5560, RunningAvgSamplesPerSec=276.4040509708841, CurrSamplesPerSec=275.5684266223657, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:26.888 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8359
dss8440-001: [2023-07-27 22:48:32,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=5570, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:33,026] [INFO] [timer.py:215:stop] epoch=0/micro_step=5570/global_step=5570, RunningAvgSamplesPerSec=276.4081932341266, CurrSamplesPerSec=283.01987444421684, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:33.026 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8355
dss8440-001: [2023-07-27 22:48:39,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=5580, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:39,211] [INFO] [timer.py:215:stop] epoch=0/micro_step=5580/global_step=5580, RunningAvgSamplesPerSec=276.4079671917148, CurrSamplesPerSec=284.8628739556812, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:39.211 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8347
dss8440-001: [2023-07-27 22:48:45,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=5590, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:45,380] [INFO] [timer.py:215:stop] epoch=0/micro_step=5590/global_step=5590, RunningAvgSamplesPerSec=276.40955173250853, CurrSamplesPerSec=278.39191756689434, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:45.381 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8339
dss8440-001: [2023-07-27 22:48:51,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=5600, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:51,675] [INFO] [timer.py:215:stop] epoch=0/micro_step=5600/global_step=5600, RunningAvgSamplesPerSec=276.4003817710493, CurrSamplesPerSec=279.8537001896026, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:51.675 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8333
dss8440-001: [2023-07-27 22:48:57,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=5610, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:48:57,861] [INFO] [timer.py:215:stop] epoch=0/micro_step=5610/global_step=5610, RunningAvgSamplesPerSec=276.40006709973886, CurrSamplesPerSec=264.20218217812874, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:48:57.861 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8326
dss8440-001: [2023-07-27 22:49:04,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=5620, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:04,121] [INFO] [timer.py:215:stop] epoch=0/micro_step=5620/global_step=5620, RunningAvgSamplesPerSec=276.393918451185, CurrSamplesPerSec=266.48508969780033, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:04.122 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8322
dss8440-001: [2023-07-27 22:49:10,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=5630, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:10,352] [INFO] [timer.py:215:stop] epoch=0/micro_step=5630/global_step=5630, RunningAvgSamplesPerSec=276.390313515872, CurrSamplesPerSec=277.27476422658015, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:10.352 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8314
dss8440-001: [2023-07-27 22:49:16,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=5640, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:16,468] [INFO] [timer.py:215:stop] epoch=0/micro_step=5640/global_step=5640, RunningAvgSamplesPerSec=276.39599644986157, CurrSamplesPerSec=286.92793402119617, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:16.469 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8307
dss8440-001: [2023-07-27 22:49:22,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=5650, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:22,585] [INFO] [timer.py:215:stop] epoch=0/micro_step=5650/global_step=5650, RunningAvgSamplesPerSec=276.4012559486819, CurrSamplesPerSec=265.61808293969904, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:22.585 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8299
dss8440-001: [2023-07-27 22:49:28,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=5660, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:28,582] [INFO] [timer.py:215:stop] epoch=0/micro_step=5660/global_step=5660, RunningAvgSamplesPerSec=276.41669485059936, CurrSamplesPerSec=292.35996830125424, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:28.582 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8294
dss8440-001: [2023-07-27 22:49:34,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=5670, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:34,839] [INFO] [timer.py:215:stop] epoch=0/micro_step=5670/global_step=5670, RunningAvgSamplesPerSec=276.41172461007, CurrSamplesPerSec=289.92061696734254, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:34.840 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8288
dss8440-001: [2023-07-27 22:49:40,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=5680, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:41,065] [INFO] [timer.py:215:stop] epoch=0/micro_step=5680/global_step=5680, RunningAvgSamplesPerSec=276.40813905617034, CurrSamplesPerSec=257.59667126188697, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:41.065 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8284
dss8440-001: [2023-07-27 22:49:47,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=5690, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:47,364] [INFO] [timer.py:215:stop] epoch=0/micro_step=5690/global_step=5690, RunningAvgSamplesPerSec=276.3989630525708, CurrSamplesPerSec=277.3195052003084, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:47.364 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8279
dss8440-001: [2023-07-27 22:49:53,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=5700, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:53,645] [INFO] [timer.py:215:stop] epoch=0/micro_step=5700/global_step=5700, RunningAvgSamplesPerSec=276.39155643461305, CurrSamplesPerSec=280.6248367668547, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:53.645 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8276
dss8440-001: [2023-07-27 22:49:59,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=5710, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:49:59,910] [INFO] [timer.py:215:stop] epoch=0/micro_step=5710/global_step=5710, RunningAvgSamplesPerSec=276.3848025243994, CurrSamplesPerSec=277.8933579423109, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:49:59.911 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8269
dss8440-001: [2023-07-27 22:50:05,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=5720, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:06,025] [INFO] [timer.py:215:stop] epoch=0/micro_step=5720/global_step=5720, RunningAvgSamplesPerSec=276.3900876811157, CurrSamplesPerSec=295.7890094612276, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:06.026 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8261
dss8440-001: [2023-07-27 22:50:12,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=5730, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:12,329] [INFO] [timer.py:215:stop] epoch=0/micro_step=5730/global_step=5730, RunningAvgSamplesPerSec=276.3804011781813, CurrSamplesPerSec=275.5504305456707, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:12.329 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8254
dss8440-001: [2023-07-27 22:50:18,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=5740, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:18,548] [INFO] [timer.py:215:stop] epoch=0/micro_step=5740/global_step=5740, RunningAvgSamplesPerSec=276.3776218972136, CurrSamplesPerSec=276.1175187149682, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:18.549 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8245
dss8440-001: [2023-07-27 22:50:24,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=5750, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:24,744] [INFO] [timer.py:215:stop] epoch=0/micro_step=5750/global_step=5750, RunningAvgSamplesPerSec=276.37662801078426, CurrSamplesPerSec=273.1316556751397, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:24.745 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8242
dss8440-001: [2023-07-27 22:50:30,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=5760, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:30,933] [INFO] [timer.py:215:stop] epoch=0/micro_step=5760/global_step=5760, RunningAvgSamplesPerSec=276.3767009382111, CurrSamplesPerSec=281.08054703243994, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:30.934 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8234
dss8440-001: [2023-07-27 22:50:37,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=5770, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:37,115] [INFO] [timer.py:215:stop] epoch=0/micro_step=5770/global_step=5770, RunningAvgSamplesPerSec=276.37679731146625, CurrSamplesPerSec=282.6295989680599, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:37.116 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8231
dss8440-001: [2023-07-27 22:50:43,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=5780, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:43,293] [INFO] [timer.py:215:stop] epoch=0/micro_step=5780/global_step=5780, RunningAvgSamplesPerSec=276.37739943098194, CurrSamplesPerSec=295.0641836337743, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:43.293 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8224
dss8440-001: [2023-07-27 22:50:49,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=5790, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:49,396] [INFO] [timer.py:215:stop] epoch=0/micro_step=5790/global_step=5790, RunningAvgSamplesPerSec=276.3832696425531, CurrSamplesPerSec=271.4250608704562, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:49.397 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8214
dss8440-001: [2023-07-27 22:50:55,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=5800, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:50:55,577] [INFO] [timer.py:215:stop] epoch=0/micro_step=5800/global_step=5800, RunningAvgSamplesPerSec=276.3838910456689, CurrSamplesPerSec=262.41481052246627, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:50:55.578 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8210
dss8440-001: [2023-07-27 22:51:01,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=5810, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:01,823] [INFO] [timer.py:215:stop] epoch=0/micro_step=5810/global_step=5810, RunningAvgSamplesPerSec=276.379211991705, CurrSamplesPerSec=253.19367880935098, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:01.823 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8204
dss8440-001: [2023-07-27 22:51:08,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=5820, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:08,085] [INFO] [timer.py:215:stop] epoch=0/micro_step=5820/global_step=5820, RunningAvgSamplesPerSec=276.3729556732845, CurrSamplesPerSec=274.43122950216326, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:08.086 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8197
dss8440-001: [2023-07-27 22:51:14,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=5830, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:14,277] [INFO] [timer.py:215:stop] epoch=0/micro_step=5830/global_step=5830, RunningAvgSamplesPerSec=276.37264531238554, CurrSamplesPerSec=281.9931494980581, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:14.278 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8189
dss8440-001: [2023-07-27 22:51:20,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=5840, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:20,429] [INFO] [timer.py:215:stop] epoch=0/micro_step=5840/global_step=5840, RunningAvgSamplesPerSec=276.37544855647957, CurrSamplesPerSec=265.65122872205666, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:20.430 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8183
dss8440-001: [2023-07-27 22:51:26,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=5850, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:26,578] [INFO] [timer.py:215:stop] epoch=0/micro_step=5850/global_step=5850, RunningAvgSamplesPerSec=276.37855676322107, CurrSamplesPerSec=266.93977970290655, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:26.578 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8177
dss8440-001: [2023-07-27 22:51:32,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=5860, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:32,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=5860/global_step=5860, RunningAvgSamplesPerSec=276.3829983191825, CurrSamplesPerSec=279.8788213873769, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:32.702 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8170
dss8440-001: [2023-07-27 22:51:38,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=5870, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:39,008] [INFO] [timer.py:215:stop] epoch=0/micro_step=5870/global_step=5870, RunningAvgSamplesPerSec=276.3732211921883, CurrSamplesPerSec=270.32795766783306, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:39.009 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8165
dss8440-001: [2023-07-27 22:51:45,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=5880, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:45,144] [INFO] [timer.py:215:stop] epoch=0/micro_step=5880/global_step=5880, RunningAvgSamplesPerSec=276.37692723797744, CurrSamplesPerSec=273.7898466932746, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:45.144 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8161
dss8440-001: [2023-07-27 22:51:51,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=5890, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:51,392] [INFO] [timer.py:215:stop] epoch=0/micro_step=5890/global_step=5890, RunningAvgSamplesPerSec=276.371943614731, CurrSamplesPerSec=275.91036711142715, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:51.392 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8153
dss8440-001: [2023-07-27 22:51:57,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=5900, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:51:57,548] [INFO] [timer.py:215:stop] epoch=0/micro_step=5900/global_step=5900, RunningAvgSamplesPerSec=276.373823725756, CurrSamplesPerSec=254.41390605096922, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:51:57.548 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8149
dss8440-001: [2023-07-27 22:52:03,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=5910, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:03,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=5910/global_step=5910, RunningAvgSamplesPerSec=276.3577192614098, CurrSamplesPerSec=255.01173539607822, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:03.942 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8141
dss8440-001: [2023-07-27 22:52:09,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=5920, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:10,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=5920/global_step=5920, RunningAvgSamplesPerSec=276.360511309104, CurrSamplesPerSec=281.1939485142446, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:10.092 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8134
dss8440-001: [2023-07-27 22:52:16,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=5930, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:16,189] [INFO] [timer.py:215:stop] epoch=0/micro_step=5930/global_step=5930, RunningAvgSamplesPerSec=276.3675682646435, CurrSamplesPerSec=279.5039176152467, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:16.190 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8130
dss8440-001: [2023-07-27 22:52:22,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=5940, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:22,277] [INFO] [timer.py:215:stop] epoch=0/micro_step=5940/global_step=5940, RunningAvgSamplesPerSec=276.37499822247725, CurrSamplesPerSec=287.4088175192009, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:22.278 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8128
dss8440-001: [2023-07-27 22:52:28,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=5950, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:28,563] [INFO] [timer.py:215:stop] epoch=0/micro_step=5950/global_step=5950, RunningAvgSamplesPerSec=276.3683051017355, CurrSamplesPerSec=280.8307362927029, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:28.564 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8123
dss8440-001: [2023-07-27 22:52:34,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=5960, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:34,794] [INFO] [timer.py:215:stop] epoch=0/micro_step=5960/global_step=5960, RunningAvgSamplesPerSec=276.36526906161924, CurrSamplesPerSec=269.3480585266805, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:34.795 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8117
dss8440-001: [2023-07-27 22:52:40,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=5970, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:40,998] [INFO] [timer.py:215:stop] epoch=0/micro_step=5970/global_step=5970, RunningAvgSamplesPerSec=276.36436652271993, CurrSamplesPerSec=275.81025445717904, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:40.999 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8109
dss8440-001: [2023-07-27 22:52:47,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=5980, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:47,160] [INFO] [timer.py:215:stop] epoch=0/micro_step=5980/global_step=5980, RunningAvgSamplesPerSec=276.3667945426769, CurrSamplesPerSec=284.68437236508316, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:47.161 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8102
dss8440-001: [2023-07-27 22:52:53,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=5990, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:53,364] [INFO] [timer.py:215:stop] epoch=0/micro_step=5990/global_step=5990, RunningAvgSamplesPerSec=276.36628850710474, CurrSamplesPerSec=267.13327035146006, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:53.365 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8100
dss8440-001: [2023-07-27 22:52:59,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:52:59,587] [INFO] [timer.py:215:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=276.3641524995164, CurrSamplesPerSec=291.9328835703952, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:52:59.587 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8090
dss8440-001: [2023-07-27 22:52:59,594] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
dss8440-001: [2023-07-27 22:52:59,597] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 22:52:59,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 22:52:59,652] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 22:52:59,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 22:52:59,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,673] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step6000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 22:52:59,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 22:52:59,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-002: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 22:52:59,675] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step6000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 22:52:59,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
dss8440-001: 2023-07-27 22:52:59.677 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 22:53:05,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=6010, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:05,816] [INFO] [timer.py:215:stop] epoch=0/micro_step=6010/global_step=6010, RunningAvgSamplesPerSec=276.3678087436734, CurrSamplesPerSec=279.9686721253115, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:05.816 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8085
dss8440-001: [2023-07-27 22:53:11,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=6020, skipped=22, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:12,008] [INFO] [timer.py:215:stop] epoch=0/micro_step=6020/global_step=6020, RunningAvgSamplesPerSec=276.3672782354561, CurrSamplesPerSec=266.15966412899985, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:12.008 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8077
dss8440-001: [2023-07-27 22:53:13,355] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 22:53:17,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=6030, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:17,662] [INFO] [timer.py:215:stop] epoch=0/micro_step=6030/global_step=6030, RunningAvgSamplesPerSec=276.407381403727, CurrSamplesPerSec=289.79708945694125, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:17.663 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8071
dss8440-001: [2023-07-27 22:53:23,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=6040, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:23,841] [INFO] [timer.py:215:stop] epoch=0/micro_step=6040/global_step=6040, RunningAvgSamplesPerSec=276.4073832899587, CurrSamplesPerSec=271.6541174710802, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:23.841 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8065
dss8440-001: [2023-07-27 22:53:29,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=6050, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:30,016] [INFO] [timer.py:215:stop] epoch=0/micro_step=6050/global_step=6050, RunningAvgSamplesPerSec=276.4078065192549, CurrSamplesPerSec=282.5426773352152, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:30.016 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8059
dss8440-001: [2023-07-27 22:53:36,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=6060, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:36,223] [INFO] [timer.py:215:stop] epoch=0/micro_step=6060/global_step=6060, RunningAvgSamplesPerSec=276.4060037438251, CurrSamplesPerSec=289.623065838158, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:36.224 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8052
dss8440-001: [2023-07-27 22:53:42,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=6070, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:42,436] [INFO] [timer.py:215:stop] epoch=0/micro_step=6070/global_step=6070, RunningAvgSamplesPerSec=276.4038092296193, CurrSamplesPerSec=268.29042586122017, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:42.436 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8049
dss8440-001: [2023-07-27 22:53:48,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=6080, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:48,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=6080/global_step=6080, RunningAvgSamplesPerSec=276.39812346076843, CurrSamplesPerSec=279.15876848036146, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:48.703 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8041
dss8440-001: [2023-07-27 22:53:54,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=6090, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:53:54,878] [INFO] [timer.py:215:stop] epoch=0/micro_step=6090/global_step=6090, RunningAvgSamplesPerSec=276.39920356452353, CurrSamplesPerSec=290.8946267046081, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:53:54.879 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8034
dss8440-001: [2023-07-27 22:54:00,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=6100, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:01,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=6100/global_step=6100, RunningAvgSamplesPerSec=276.4013769223821, CurrSamplesPerSec=285.02649540773774, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:01.034 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8025
dss8440-001: [2023-07-27 22:54:07,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=6110, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:07,244] [INFO] [timer.py:215:stop] epoch=0/micro_step=6110/global_step=6110, RunningAvgSamplesPerSec=276.3997449955083, CurrSamplesPerSec=280.67301322261113, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:07.245 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8020
dss8440-001: [2023-07-27 22:54:13,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=6120, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:13,429] [INFO] [timer.py:215:stop] epoch=0/micro_step=6120/global_step=6120, RunningAvgSamplesPerSec=276.39959764904825, CurrSamplesPerSec=286.86929436339955, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:13.430 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8014
dss8440-001: [2023-07-27 22:54:19,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=6130, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:19,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=6130/global_step=6130, RunningAvgSamplesPerSec=276.3955111169496, CurrSamplesPerSec=277.3821668261602, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:19.669 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8006
dss8440-001: [2023-07-27 22:54:25,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=6140, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:25,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=6140/global_step=6140, RunningAvgSamplesPerSec=276.3933255759147, CurrSamplesPerSec=297.79661387995634, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:25.886 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.8002
dss8440-001: [2023-07-27 22:54:32,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=6150, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:32,084] [INFO] [timer.py:215:stop] epoch=0/micro_step=6150/global_step=6150, RunningAvgSamplesPerSec=276.39191098796545, CurrSamplesPerSec=277.10161095282757, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:32.085 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7995
dss8440-001: [2023-07-27 22:54:38,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=6160, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:38,217] [INFO] [timer.py:215:stop] epoch=0/micro_step=6160/global_step=6160, RunningAvgSamplesPerSec=276.3956436814614, CurrSamplesPerSec=294.70421050782556, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:38.218 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7991
dss8440-001: [2023-07-27 22:54:44,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=6170, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:44,488] [INFO] [timer.py:215:stop] epoch=0/micro_step=6170/global_step=6170, RunningAvgSamplesPerSec=276.38944828529804, CurrSamplesPerSec=285.19157461375414, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:44.488 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7984
dss8440-001: [2023-07-27 22:54:50,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=6180, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:50,754] [INFO] [timer.py:215:stop] epoch=0/micro_step=6180/global_step=6180, RunningAvgSamplesPerSec=276.3833681519144, CurrSamplesPerSec=271.83971772987974, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:50.755 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7974
dss8440-001: [2023-07-27 22:54:56,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=6190, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:54:56,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=6190/global_step=6190, RunningAvgSamplesPerSec=276.38322104600604, CurrSamplesPerSec=276.5220130796414, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:54:56.946 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7969
dss8440-001: [2023-07-27 22:55:03,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=6200, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:03,197] [INFO] [timer.py:215:stop] epoch=0/micro_step=6200/global_step=6200, RunningAvgSamplesPerSec=276.37856085624026, CurrSamplesPerSec=277.10575189343365, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:03.198 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7964
dss8440-001: [2023-07-27 22:55:09,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=6210, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:09,466] [INFO] [timer.py:215:stop] epoch=0/micro_step=6210/global_step=6210, RunningAvgSamplesPerSec=276.3723761368992, CurrSamplesPerSec=268.81712511440054, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:09.466 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7959
dss8440-001: [2023-07-27 22:55:15,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=6220, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:15,561] [INFO] [timer.py:215:stop] epoch=0/micro_step=6220/global_step=6220, RunningAvgSamplesPerSec=276.3784671301837, CurrSamplesPerSec=288.037593843583, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:15.561 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7955
dss8440-001: [2023-07-27 22:55:21,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=6230, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:21,743] [INFO] [timer.py:215:stop] epoch=0/micro_step=6230/global_step=6230, RunningAvgSamplesPerSec=276.3788065596749, CurrSamplesPerSec=280.7501747110793, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:21.743 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7948
dss8440-001: [2023-07-27 22:55:27,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=6240, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:27,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=6240/global_step=6240, RunningAvgSamplesPerSec=276.3774100821089, CurrSamplesPerSec=281.2768983034204, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:27.941 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7944
dss8440-001: [2023-07-27 22:55:34,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=6250, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:34,107] [INFO] [timer.py:215:stop] epoch=0/micro_step=6250/global_step=6250, RunningAvgSamplesPerSec=276.3788156252965, CurrSamplesPerSec=278.8764853737314, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:34.108 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7937
dss8440-001: [2023-07-27 22:55:40,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=6260, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:40,435] [INFO] [timer.py:215:stop] epoch=0/micro_step=6260/global_step=6260, RunningAvgSamplesPerSec=276.36903716843676, CurrSamplesPerSec=282.4466384451072, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:40.435 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7934
dss8440-001: [2023-07-27 22:55:46,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=6270, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:46,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=6270/global_step=6270, RunningAvgSamplesPerSec=276.3664794278656, CurrSamplesPerSec=257.0846543415135, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:46.659 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7927
dss8440-001: [2023-07-27 22:55:52,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=6280, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:52,688] [INFO] [timer.py:215:stop] epoch=0/micro_step=6280/global_step=6280, RunningAvgSamplesPerSec=276.3778232336378, CurrSamplesPerSec=297.29253848091366, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:52.688 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7920
dss8440-001: [2023-07-27 22:55:58,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=6290, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:55:58,864] [INFO] [timer.py:215:stop] epoch=0/micro_step=6290/global_step=6290, RunningAvgSamplesPerSec=276.3784943253498, CurrSamplesPerSec=290.8013476906518, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:55:58.865 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7916
dss8440-001: [2023-07-27 22:56:04,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=6300, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:05,133] [INFO] [timer.py:215:stop] epoch=0/micro_step=6300/global_step=6300, RunningAvgSamplesPerSec=276.37226269139796, CurrSamplesPerSec=268.292264585644, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:05.133 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7911
dss8440-001: [2023-07-27 22:56:11,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=6310, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:11,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=6310/global_step=6310, RunningAvgSamplesPerSec=276.3617617439356, CurrSamplesPerSec=282.5359932702324, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:11.458 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7907
dss8440-001: [2023-07-27 22:56:17,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=6320, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:17,686] [INFO] [timer.py:215:stop] epoch=0/micro_step=6320/global_step=6320, RunningAvgSamplesPerSec=276.35910148137776, CurrSamplesPerSec=284.66447599648694, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:17.687 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7899
dss8440-001: [2023-07-27 22:56:23,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=6330, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:23,892] [INFO] [timer.py:215:stop] epoch=0/micro_step=6330/global_step=6330, RunningAvgSamplesPerSec=276.3571031934199, CurrSamplesPerSec=268.18290432781015, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:23.893 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7893
dss8440-001: [2023-07-27 22:56:29,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=6340, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:30,052] [INFO] [timer.py:215:stop] epoch=0/micro_step=6340/global_step=6340, RunningAvgSamplesPerSec=276.3583294900994, CurrSamplesPerSec=277.08559321634004, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:30.052 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7884
dss8440-001: [2023-07-27 22:56:36,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=6350, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:36,224] [INFO] [timer.py:215:stop] epoch=0/micro_step=6350/global_step=6350, RunningAvgSamplesPerSec=276.35944759304675, CurrSamplesPerSec=261.13433884537943, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:36.224 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7877
dss8440-001: [2023-07-27 22:56:42,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=6360, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:42,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=6360/global_step=6360, RunningAvgSamplesPerSec=276.35650564226415, CurrSamplesPerSec=290.8859805738351, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:42.450 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7872
dss8440-001: [2023-07-27 22:56:48,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=6370, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:48,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=6370/global_step=6370, RunningAvgSamplesPerSec=276.35178138463243, CurrSamplesPerSec=273.2197683318638, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:48.697 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7868
dss8440-001: [2023-07-27 22:56:54,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=6380, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:56:54,813] [INFO] [timer.py:215:stop] epoch=0/micro_step=6380/global_step=6380, RunningAvgSamplesPerSec=276.35633938201227, CurrSamplesPerSec=285.55412674651103, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:56:54.814 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7864
dss8440-001: [2023-07-27 22:57:00,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=6390, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:01,021] [INFO] [timer.py:215:stop] epoch=0/micro_step=6390/global_step=6390, RunningAvgSamplesPerSec=276.35473674879245, CurrSamplesPerSec=271.82629486682447, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:01.022 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7856
dss8440-001: [2023-07-27 22:57:07,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=6400, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:07,224] [INFO] [timer.py:215:stop] epoch=0/micro_step=6400/global_step=6400, RunningAvgSamplesPerSec=276.3539317215275, CurrSamplesPerSec=271.12981499195433, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:07.225 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7851
dss8440-001: [2023-07-27 22:57:13,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=6410, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:13,431] [INFO] [timer.py:215:stop] epoch=0/micro_step=6410/global_step=6410, RunningAvgSamplesPerSec=276.3522385251724, CurrSamplesPerSec=268.46368303867376, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:13.432 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7847
dss8440-001: [2023-07-27 22:57:19,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=6420, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:19,698] [INFO] [timer.py:215:stop] epoch=0/micro_step=6420/global_step=6420, RunningAvgSamplesPerSec=276.34646111784633, CurrSamplesPerSec=263.0338921933058, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:19.699 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7840
dss8440-001: [2023-07-27 22:57:25,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=6430, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:25,955] [INFO] [timer.py:215:stop] epoch=0/micro_step=6430/global_step=6430, RunningAvgSamplesPerSec=276.3421325805992, CurrSamplesPerSec=266.6218182148403, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:25.955 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7840
dss8440-001: [2023-07-27 22:57:32,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=6440, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:32,151] [INFO] [timer.py:215:stop] epoch=0/micro_step=6440/global_step=6440, RunningAvgSamplesPerSec=276.34130252945533, CurrSamplesPerSec=257.2251432976978, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:32.152 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7833
dss8440-001: [2023-07-27 22:57:38,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=6450, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:38,285] [INFO] [timer.py:215:stop] epoch=0/micro_step=6450/global_step=6450, RunningAvgSamplesPerSec=276.3448163863896, CurrSamplesPerSec=289.8680214405305, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:38.286 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7826
dss8440-001: [2023-07-27 22:57:44,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=6460, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:44,543] [INFO] [timer.py:215:stop] epoch=0/micro_step=6460/global_step=6460, RunningAvgSamplesPerSec=276.33983654112103, CurrSamplesPerSec=279.2082129587327, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:44.544 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7821
dss8440-001: [2023-07-27 22:57:50,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=6470, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:50,823] [INFO] [timer.py:215:stop] epoch=0/micro_step=6470/global_step=6470, RunningAvgSamplesPerSec=276.3327274251715, CurrSamplesPerSec=255.07903201867907, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:50.824 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7817
dss8440-001: [2023-07-27 22:57:56,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=6480, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:57:57,010] [INFO] [timer.py:215:stop] epoch=0/micro_step=6480/global_step=6480, RunningAvgSamplesPerSec=276.3323969091162, CurrSamplesPerSec=269.3317922044564, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:57:57.011 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7813
dss8440-001: [2023-07-27 22:58:03,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=6490, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:03,258] [INFO] [timer.py:215:stop] epoch=0/micro_step=6490/global_step=6490, RunningAvgSamplesPerSec=276.3284633186141, CurrSamplesPerSec=288.06291046762186, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:03.259 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7808
dss8440-001: [2023-07-27 22:58:09,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=6500, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:09,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=6500/global_step=6500, RunningAvgSamplesPerSec=276.32258899245295, CurrSamplesPerSec=264.62654748809797, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:09.525 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7799
dss8440-001: [2023-07-27 22:58:15,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=6510, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:15,683] [INFO] [timer.py:215:stop] epoch=0/micro_step=6510/global_step=6510, RunningAvgSamplesPerSec=276.32452255861784, CurrSamplesPerSec=282.17823475986285, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:15.683 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7793
dss8440-001: [2023-07-27 22:58:21,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=6520, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:21,993] [INFO] [timer.py:215:stop] epoch=0/micro_step=6520/global_step=6520, RunningAvgSamplesPerSec=276.31587129519284, CurrSamplesPerSec=255.4277144966848, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:21.994 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7789
dss8440-001: [2023-07-27 22:58:28,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=6530, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:28,124] [INFO] [timer.py:215:stop] epoch=0/micro_step=6530/global_step=6530, RunningAvgSamplesPerSec=276.31952544465344, CurrSamplesPerSec=283.5877949599637, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:28.125 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7783
dss8440-001: [2023-07-27 22:58:34,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=6540, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:34,294] [INFO] [timer.py:215:stop] epoch=0/micro_step=6540/global_step=6540, RunningAvgSamplesPerSec=276.320674352651, CurrSamplesPerSec=280.5827098985568, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:34.295 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7777
dss8440-001: [2023-07-27 22:58:40,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=6550, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:40,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=6550/global_step=6550, RunningAvgSamplesPerSec=276.31966175150086, CurrSamplesPerSec=259.8620860568136, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:40.495 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7773
dss8440-001: [2023-07-27 22:58:46,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=6560, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:46,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=6560/global_step=6560, RunningAvgSamplesPerSec=276.31079487342475, CurrSamplesPerSec=259.4591575641198, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:46.807 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7766
dss8440-001: [2023-07-27 22:58:52,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=6570, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:53,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=6570/global_step=6570, RunningAvgSamplesPerSec=276.3063532014032, CurrSamplesPerSec=270.2946714620644, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:53.054 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7757
dss8440-001: [2023-07-27 22:58:59,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=6580, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:58:59,212] [INFO] [timer.py:215:stop] epoch=0/micro_step=6580/global_step=6580, RunningAvgSamplesPerSec=276.3079702338332, CurrSamplesPerSec=273.8358111511127, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:58:59.213 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7750
dss8440-001: [2023-07-27 22:59:05,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=6590, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:05,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=6590/global_step=6590, RunningAvgSamplesPerSec=276.30380917548484, CurrSamplesPerSec=280.0070541887425, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:05.465 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7742
dss8440-001: [2023-07-27 22:59:11,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=6600, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:11,623] [INFO] [timer.py:215:stop] epoch=0/micro_step=6600/global_step=6600, RunningAvgSamplesPerSec=276.3064242665044, CurrSamplesPerSec=279.01385441245355, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:11.624 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7739
dss8440-001: [2023-07-27 22:59:17,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=6610, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:17,891] [INFO] [timer.py:215:stop] epoch=0/micro_step=6610/global_step=6610, RunningAvgSamplesPerSec=276.3008295870006, CurrSamplesPerSec=259.9587219932443, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:17.892 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7733
dss8440-001: [2023-07-27 22:59:24,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=6620, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:24,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=6620/global_step=6620, RunningAvgSamplesPerSec=276.29051642020517, CurrSamplesPerSec=269.52990141683887, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:24.228 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7726
dss8440-001: [2023-07-27 22:59:30,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=6630, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:30,364] [INFO] [timer.py:215:stop] epoch=0/micro_step=6630/global_step=6630, RunningAvgSamplesPerSec=276.29373746700895, CurrSamplesPerSec=276.99867209151483, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:30.365 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7722
dss8440-001: [2023-07-27 22:59:36,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=6640, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:36,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=6640/global_step=6640, RunningAvgSamplesPerSec=276.29432975325403, CurrSamplesPerSec=279.31523669389617, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:36.538 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7715
dss8440-001: [2023-07-27 22:59:42,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=6650, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:42,782] [INFO] [timer.py:215:stop] epoch=0/micro_step=6650/global_step=6650, RunningAvgSamplesPerSec=276.29036563662817, CurrSamplesPerSec=266.0907153007159, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:42.782 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7709
dss8440-001: [2023-07-27 22:59:48,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=6660, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:48,982] [INFO] [timer.py:215:stop] epoch=0/micro_step=6660/global_step=6660, RunningAvgSamplesPerSec=276.28951358239027, CurrSamplesPerSec=276.88155647747715, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:48.982 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7705
dss8440-001: [2023-07-27 22:59:54,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=6670, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 22:59:55,071] [INFO] [timer.py:215:stop] epoch=0/micro_step=6670/global_step=6670, RunningAvgSamplesPerSec=276.2962334051382, CurrSamplesPerSec=289.2471462911637, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 22:59:55.072 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7703
dss8440-001: [2023-07-27 23:00:01,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=6680, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:01,299] [INFO] [timer.py:215:stop] epoch=0/micro_step=6680/global_step=6680, RunningAvgSamplesPerSec=276.29397132545984, CurrSamplesPerSec=266.8971118846421, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:01.300 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7697
dss8440-001: [2023-07-27 23:00:07,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=6690, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:07,495] [INFO] [timer.py:215:stop] epoch=0/micro_step=6690/global_step=6690, RunningAvgSamplesPerSec=276.2932987952644, CurrSamplesPerSec=287.5877013368378, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:07.495 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7691
dss8440-001: [2023-07-27 23:00:13,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=6700, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:13,657] [INFO] [timer.py:215:stop] epoch=0/micro_step=6700/global_step=6700, RunningAvgSamplesPerSec=276.2951068252631, CurrSamplesPerSec=278.3038453031691, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:13.658 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7687
dss8440-001: [2023-07-27 23:00:19,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=6710, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:19,855] [INFO] [timer.py:215:stop] epoch=0/micro_step=6710/global_step=6710, RunningAvgSamplesPerSec=276.2940758061683, CurrSamplesPerSec=268.9542046240043, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:19.855 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7681
dss8440-001: [2023-07-27 23:00:26,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=6720, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:26,138] [INFO] [timer.py:215:stop] epoch=0/micro_step=6720/global_step=6720, RunningAvgSamplesPerSec=276.28796493029137, CurrSamplesPerSec=274.47334682374753, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:26.139 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7678
dss8440-001: [2023-07-27 23:00:32,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=6730, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:32,297] [INFO] [timer.py:215:stop] epoch=0/micro_step=6730/global_step=6730, RunningAvgSamplesPerSec=276.29029039824917, CurrSamplesPerSec=262.12176645730943, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:32.298 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7673
dss8440-001: [2023-07-27 23:00:38,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=6740, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:38,457] [INFO] [timer.py:215:stop] epoch=0/micro_step=6740/global_step=6740, RunningAvgSamplesPerSec=276.29231584850544, CurrSamplesPerSec=271.95355073364306, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:38.457 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7669
dss8440-001: [2023-07-27 23:00:44,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=6750, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:44,728] [INFO] [timer.py:215:stop] epoch=0/micro_step=6750/global_step=6750, RunningAvgSamplesPerSec=276.28661353152904, CurrSamplesPerSec=264.83610631283176, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:44.729 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7662
dss8440-001: [2023-07-27 23:00:50,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=6760, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:50,892] [INFO] [timer.py:215:stop] epoch=0/micro_step=6760/global_step=6760, RunningAvgSamplesPerSec=276.2887040662664, CurrSamplesPerSec=281.5646268397881, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:50.893 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7654
dss8440-001: [2023-07-27 23:00:57,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=6770, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:00:57,113] [INFO] [timer.py:215:stop] epoch=0/micro_step=6770/global_step=6770, RunningAvgSamplesPerSec=276.2864668588357, CurrSamplesPerSec=263.0528436603697, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:00:57.113 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7649
dss8440-001: [2023-07-27 23:01:03,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=6780, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:03,265] [INFO] [timer.py:215:stop] epoch=0/micro_step=6780/global_step=6780, RunningAvgSamplesPerSec=276.288131937119, CurrSamplesPerSec=275.68033520930044, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:03.266 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7641
dss8440-001: [2023-07-27 23:01:09,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=6790, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:09,476] [INFO] [timer.py:215:stop] epoch=0/micro_step=6790/global_step=6790, RunningAvgSamplesPerSec=276.2862043898051, CurrSamplesPerSec=271.497429870983, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:09.477 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7635
dss8440-001: [2023-07-27 23:01:15,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=6800, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:15,625] [INFO] [timer.py:215:stop] epoch=0/micro_step=6800/global_step=6800, RunningAvgSamplesPerSec=276.2893320556075, CurrSamplesPerSec=285.65819676617065, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:15.625 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7631
dss8440-001: [2023-07-27 23:01:21,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=6810, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:21,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=6810/global_step=6810, RunningAvgSamplesPerSec=276.28957122648234, CurrSamplesPerSec=279.29431247104077, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:21.807 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7628
dss8440-001: [2023-07-27 23:01:27,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=6820, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:27,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=6820/global_step=6820, RunningAvgSamplesPerSec=276.29250185488064, CurrSamplesPerSec=269.69547325102883, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:27.951 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7623
dss8440-001: [2023-07-27 23:01:34,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=6830, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:34,210] [INFO] [timer.py:215:stop] epoch=0/micro_step=6830/global_step=6830, RunningAvgSamplesPerSec=276.28793517827177, CurrSamplesPerSec=274.2147539222844, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:34.211 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7615
dss8440-001: [2023-07-27 23:01:40,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=6840, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:40,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=6840/global_step=6840, RunningAvgSamplesPerSec=276.28762340118834, CurrSamplesPerSec=284.65849612870335, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:40.401 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7612
dss8440-001: [2023-07-27 23:01:46,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=6850, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:46,472] [INFO] [timer.py:215:stop] epoch=0/micro_step=6850/global_step=6850, RunningAvgSamplesPerSec=276.2949121282942, CurrSamplesPerSec=296.9988227831295, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:46.473 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7605
dss8440-001: [2023-07-27 23:01:52,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=6860, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:52,586] [INFO] [timer.py:215:stop] epoch=0/micro_step=6860/global_step=6860, RunningAvgSamplesPerSec=276.30027685828213, CurrSamplesPerSec=279.12570465425347, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:52.587 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7599
dss8440-001: [2023-07-27 23:01:58,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=6870, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:01:58,961] [INFO] [timer.py:215:stop] epoch=0/micro_step=6870/global_step=6870, RunningAvgSamplesPerSec=276.2878806826074, CurrSamplesPerSec=272.6384503506049, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:01:58.962 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7595
dss8440-001: [2023-07-27 23:02:05,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=6880, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:05,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=6880/global_step=6880, RunningAvgSamplesPerSec=276.28214914737606, CurrSamplesPerSec=258.3157755818165, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:05.232 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7586
dss8440-001: [2023-07-27 23:02:11,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=6890, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:11,598] [INFO] [timer.py:215:stop] epoch=0/micro_step=6890/global_step=6890, RunningAvgSamplesPerSec=276.27032079237387, CurrSamplesPerSec=261.2803556695966, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:11.599 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7581
dss8440-001: [2023-07-27 23:02:17,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=6900, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:17,722] [INFO] [timer.py:215:stop] epoch=0/micro_step=6900/global_step=6900, RunningAvgSamplesPerSec=276.27412921010495, CurrSamplesPerSec=275.86640363216253, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:17.723 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7577
dss8440-001: [2023-07-27 23:02:23,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=6910, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:23,895] [INFO] [timer.py:215:stop] epoch=0/micro_step=6910/global_step=6910, RunningAvgSamplesPerSec=276.27456484158625, CurrSamplesPerSec=288.79110318753266, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:23.896 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7571
dss8440-001: [2023-07-27 23:02:29,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=6920, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:30,084] [INFO] [timer.py:215:stop] epoch=0/micro_step=6920/global_step=6920, RunningAvgSamplesPerSec=276.27421449043163, CurrSamplesPerSec=277.41864150340217, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:30.085 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7564
dss8440-001: [2023-07-27 23:02:36,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=6930, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:36,209] [INFO] [timer.py:215:stop] epoch=0/micro_step=6930/global_step=6930, RunningAvgSamplesPerSec=276.27834801448927, CurrSamplesPerSec=273.08814983817626, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:36.209 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7559
dss8440-001: [2023-07-27 23:02:42,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=6940, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:42,476] [INFO] [timer.py:215:stop] epoch=0/micro_step=6940/global_step=6940, RunningAvgSamplesPerSec=276.2731509243766, CurrSamplesPerSec=259.6242538930204, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:42.477 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7555
dss8440-001: [2023-07-27 23:02:48,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=6950, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:48,687] [INFO] [timer.py:215:stop] epoch=0/micro_step=6950/global_step=6950, RunningAvgSamplesPerSec=276.2715958703929, CurrSamplesPerSec=271.37822983322, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:48.687 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7549
dss8440-001: [2023-07-27 23:02:54,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=6960, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:02:54,935] [INFO] [timer.py:215:stop] epoch=0/micro_step=6960/global_step=6960, RunningAvgSamplesPerSec=276.2673470611807, CurrSamplesPerSec=280.31961962244696, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:02:54.936 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7545
dss8440-001: [2023-07-27 23:03:01,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=6970, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:01,278] [INFO] [timer.py:215:stop] epoch=0/micro_step=6970/global_step=6970, RunningAvgSamplesPerSec=276.25713438011667, CurrSamplesPerSec=274.9350247215702, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:01.278 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7536
dss8440-001: [2023-07-27 23:03:07,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=6980, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:07,598] [INFO] [timer.py:215:stop] epoch=0/micro_step=6980/global_step=6980, RunningAvgSamplesPerSec=276.24873808278966, CurrSamplesPerSec=269.51010830614877, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:07.599 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7532
dss8440-001: [2023-07-27 23:03:13,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=6990, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:13,893] [INFO] [timer.py:215:stop] epoch=0/micro_step=6990/global_step=6990, RunningAvgSamplesPerSec=276.2418938725556, CurrSamplesPerSec=269.47846598644657, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:13.894 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7524
dss8440-001: [2023-07-27 23:03:20,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:20,078] [INFO] [timer.py:215:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=276.241867920954, CurrSamplesPerSec=272.48536708449586, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:20.079 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7519
dss8440-001: [2023-07-27 23:03:20,086] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7000 is about to be saved!
dss8440-001: [2023-07-27 23:03:20,089] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 23:03:20,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 23:03:20,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:03:20,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:03:20,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:03:20,165] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,165] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:03:20,165] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-003: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step7000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:03:20,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: [2023-07-27 23:03:20,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:03:20,166] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,168] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:03:20,167] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step7000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:03:20,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-002: [2023-07-27 23:03:20,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
dss8440-001: 2023-07-27 23:03:20.168 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 23:03:26,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=7010, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:26,484] [INFO] [timer.py:215:stop] epoch=0/micro_step=7010/global_step=7010, RunningAvgSamplesPerSec=276.2334479267849, CurrSamplesPerSec=262.0150394727595, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:26.484 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7514
dss8440-001: [2023-07-27 23:03:32,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=7020, skipped=23, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:32,635] [INFO] [timer.py:215:stop] epoch=0/micro_step=7020/global_step=7020, RunningAvgSamplesPerSec=276.2359891859623, CurrSamplesPerSec=290.6492524261913, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:32.635 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7510
dss8440-001: [2023-07-27 23:03:34,592] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 23:03:38,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=7030, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:38,309] [INFO] [timer.py:215:stop] epoch=0/micro_step=7030/global_step=7030, RunningAvgSamplesPerSec=276.26908146135764, CurrSamplesPerSec=273.0116512979465, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:38.310 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7503
dss8440-001: [2023-07-27 23:03:44,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=7040, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:44,495] [INFO] [timer.py:215:stop] epoch=0/micro_step=7040/global_step=7040, RunningAvgSamplesPerSec=276.26922070889515, CurrSamplesPerSec=289.27825750677584, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:44.496 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7498
dss8440-001: [2023-07-27 23:03:50,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=7050, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:50,761] [INFO] [timer.py:215:stop] epoch=0/micro_step=7050/global_step=7050, RunningAvgSamplesPerSec=276.26432356247085, CurrSamplesPerSec=263.5537379629581, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:50.762 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7492
dss8440-001: [2023-07-27 23:03:56,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=7060, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:03:56,951] [INFO] [timer.py:215:stop] epoch=0/micro_step=7060/global_step=7060, RunningAvgSamplesPerSec=276.2641382738973, CurrSamplesPerSec=272.4869476444042, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:03:56.952 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7485
dss8440-001: [2023-07-27 23:04:03,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=7070, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:03,079] [INFO] [timer.py:215:stop] epoch=0/micro_step=7070/global_step=7070, RunningAvgSamplesPerSec=276.2678558399901, CurrSamplesPerSec=285.59718100469263, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:03.080 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7480
dss8440-001: [2023-07-27 23:04:09,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=7080, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:09,326] [INFO] [timer.py:215:stop] epoch=0/micro_step=7080/global_step=7080, RunningAvgSamplesPerSec=276.2642117924218, CurrSamplesPerSec=265.4395785170572, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:09.327 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7476
dss8440-001: [2023-07-27 23:04:15,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=7090, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:15,540] [INFO] [timer.py:215:stop] epoch=0/micro_step=7090/global_step=7090, RunningAvgSamplesPerSec=276.2625897119732, CurrSamplesPerSec=261.2110063719579, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:15.541 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7471
dss8440-001: [2023-07-27 23:04:21,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=7100, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:21,700] [INFO] [timer.py:215:stop] epoch=0/micro_step=7100/global_step=7100, RunningAvgSamplesPerSec=276.264204722551, CurrSamplesPerSec=287.88708129514305, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:21.702 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7466
dss8440-001: [2023-07-27 23:04:27,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=7110, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:27,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=7110/global_step=7110, RunningAvgSamplesPerSec=276.260208450073, CurrSamplesPerSec=273.28271047610184, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:27.951 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7463
dss8440-001: [2023-07-27 23:04:33,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=7120, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:34,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=7120/global_step=7120, RunningAvgSamplesPerSec=276.2620215310132, CurrSamplesPerSec=289.34739110510316, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:34.108 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7458
dss8440-001: [2023-07-27 23:04:40,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=7130, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:40,280] [INFO] [timer.py:215:stop] epoch=0/micro_step=7130/global_step=7130, RunningAvgSamplesPerSec=276.2629928181244, CurrSamplesPerSec=276.58094934552526, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:40.281 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7454
dss8440-001: [2023-07-27 23:04:46,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=7140, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:46,419] [INFO] [timer.py:215:stop] epoch=0/micro_step=7140/global_step=7140, RunningAvgSamplesPerSec=276.26574246122414, CurrSamplesPerSec=284.02067105556443, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:46.420 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7449
dss8440-001: [2023-07-27 23:04:52,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=7150, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:52,691] [INFO] [timer.py:215:stop] epoch=0/micro_step=7150/global_step=7150, RunningAvgSamplesPerSec=276.26054075659596, CurrSamplesPerSec=270.91257740298147, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:52.692 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7444
dss8440-001: [2023-07-27 23:04:58,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=7160, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:04:58,754] [INFO] [timer.py:215:stop] epoch=0/micro_step=7160/global_step=7160, RunningAvgSamplesPerSec=276.2688568975095, CurrSamplesPerSec=276.89526563973595, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:04:58.755 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7438
dss8440-001: [2023-07-27 23:05:04,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=7170, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:05,120] [INFO] [timer.py:215:stop] epoch=0/micro_step=7170/global_step=7170, RunningAvgSamplesPerSec=276.25802969746746, CurrSamplesPerSec=255.3817052627306, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:05.121 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7435
dss8440-001: [2023-07-27 23:05:11,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=7180, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:11,308] [INFO] [timer.py:215:stop] epoch=0/micro_step=7180/global_step=7180, RunningAvgSamplesPerSec=276.25776250558357, CurrSamplesPerSec=278.182658428289, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:11.309 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7427
dss8440-001: [2023-07-27 23:05:17,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=7190, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:17,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=7190/global_step=7190, RunningAvgSamplesPerSec=276.26074542952335, CurrSamplesPerSec=265.0195807635979, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:17.451 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7424
dss8440-001: [2023-07-27 23:05:23,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=7200, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:23,656] [INFO] [timer.py:215:stop] epoch=0/micro_step=7200/global_step=7200, RunningAvgSamplesPerSec=276.2596797324992, CurrSamplesPerSec=268.37993969247503, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:23.657 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7416
dss8440-001: [2023-07-27 23:05:29,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=7210, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:29,777] [INFO] [timer.py:215:stop] epoch=0/micro_step=7210/global_step=7210, RunningAvgSamplesPerSec=276.2637050086426, CurrSamplesPerSec=278.92980471357015, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:29.777 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7411
dss8440-001: [2023-07-27 23:05:35,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=7220, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:35,855] [INFO] [timer.py:215:stop] epoch=0/micro_step=7220/global_step=7220, RunningAvgSamplesPerSec=276.27038996386466, CurrSamplesPerSec=277.6132987789071, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:35.855 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7409
dss8440-001: [2023-07-27 23:05:41,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=7230, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:42,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=7230/global_step=7230, RunningAvgSamplesPerSec=276.2719716837147, CurrSamplesPerSec=282.913741877668, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:42.016 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7405
dss8440-001: [2023-07-27 23:05:48,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=7240, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:48,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=7240/global_step=7240, RunningAvgSamplesPerSec=276.27503198904947, CurrSamplesPerSec=277.6416293565026, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:48.148 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7398
dss8440-001: [2023-07-27 23:05:54,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=7250, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:05:54,335] [INFO] [timer.py:215:stop] epoch=0/micro_step=7250/global_step=7250, RunningAvgSamplesPerSec=276.2749366162281, CurrSamplesPerSec=275.9852560971741, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:05:54.336 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7393
dss8440-001: [2023-07-27 23:06:00,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=7260, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:00,591] [INFO] [timer.py:215:stop] epoch=0/micro_step=7260/global_step=7260, RunningAvgSamplesPerSec=276.270964771844, CurrSamplesPerSec=278.4611170168624, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:00.592 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7388
dss8440-001: [2023-07-27 23:06:06,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=7270, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:06,866] [INFO] [timer.py:215:stop] epoch=0/micro_step=7270/global_step=7270, RunningAvgSamplesPerSec=276.26507274828435, CurrSamplesPerSec=276.8972242023089, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:06.867 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7383
dss8440-001: [2023-07-27 23:06:13,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=7280, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:13,098] [INFO] [timer.py:215:stop] epoch=0/micro_step=7280/global_step=7280, RunningAvgSamplesPerSec=276.2623555137389, CurrSamplesPerSec=270.7802583053481, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:13.099 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7380
dss8440-001: [2023-07-27 23:06:19,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=7290, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:19,259] [INFO] [timer.py:215:stop] epoch=0/micro_step=7290/global_step=7290, RunningAvgSamplesPerSec=276.2638369824309, CurrSamplesPerSec=281.3706826156211, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:19.260 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7373
dss8440-001: [2023-07-27 23:06:25,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=7300, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:25,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=7300/global_step=7300, RunningAvgSamplesPerSec=276.25786397940334, CurrSamplesPerSec=267.2433977355863, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:25.538 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7368
dss8440-001: [2023-07-27 23:06:31,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=7310, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:31,745] [INFO] [timer.py:215:stop] epoch=0/micro_step=7310/global_step=7310, RunningAvgSamplesPerSec=276.2569902929038, CurrSamplesPerSec=264.1319664483694, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:31.746 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7364
dss8440-001: [2023-07-27 23:06:37,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=7320, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:37,877] [INFO] [timer.py:215:stop] epoch=0/micro_step=7320/global_step=7320, RunningAvgSamplesPerSec=276.26050187231994, CurrSamplesPerSec=285.3473695463959, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:37.878 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7357
dss8440-001: [2023-07-27 23:06:43,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=7330, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:44,044] [INFO] [timer.py:215:stop] epoch=0/micro_step=7330/global_step=7330, RunningAvgSamplesPerSec=276.26192632817185, CurrSamplesPerSec=280.05501874338256, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:44.044 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7353
dss8440-001: [2023-07-27 23:06:50,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=7340, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:50,325] [INFO] [timer.py:215:stop] epoch=0/micro_step=7340/global_step=7340, RunningAvgSamplesPerSec=276.25660613009507, CurrSamplesPerSec=258.8179771632624, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:50.326 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7350
dss8440-001: [2023-07-27 23:06:56,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=7350, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:06:56,564] [INFO] [timer.py:215:stop] epoch=0/micro_step=7350/global_step=7350, RunningAvgSamplesPerSec=276.2537865026124, CurrSamplesPerSec=276.79476329719955, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:06:56.565 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7343
dss8440-001: [2023-07-27 23:07:02,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=7360, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:02,699] [INFO] [timer.py:215:stop] epoch=0/micro_step=7360/global_step=7360, RunningAvgSamplesPerSec=276.25656038332454, CurrSamplesPerSec=270.0447167635552, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:02.700 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7336
dss8440-001: [2023-07-27 23:07:08,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=7370, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:08,767] [INFO] [timer.py:215:stop] epoch=0/micro_step=7370/global_step=7370, RunningAvgSamplesPerSec=276.26403465322716, CurrSamplesPerSec=269.105194757085, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:08.767 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7334
dss8440-001: [2023-07-27 23:07:14,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=7380, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:14,864] [INFO] [timer.py:215:stop] epoch=0/micro_step=7380/global_step=7380, RunningAvgSamplesPerSec=276.2694991328389, CurrSamplesPerSec=265.6609437060188, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:14.865 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7328
dss8440-001: [2023-07-27 23:07:21,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=7390, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:21,202] [INFO] [timer.py:215:stop] epoch=0/micro_step=7390/global_step=7390, RunningAvgSamplesPerSec=276.2605465728385, CurrSamplesPerSec=290.26923111779365, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:21.203 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7327
dss8440-001: [2023-07-27 23:07:27,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=7400, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:27,434] [INFO] [timer.py:215:stop] epoch=0/micro_step=7400/global_step=7400, RunningAvgSamplesPerSec=276.2577843111087, CurrSamplesPerSec=277.9147304340624, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:27.435 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7321
dss8440-001: [2023-07-27 23:07:33,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=7410, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:33,701] [INFO] [timer.py:215:stop] epoch=0/micro_step=7410/global_step=7410, RunningAvgSamplesPerSec=276.2531736993723, CurrSamplesPerSec=259.3275751851267, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:33.702 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7314
dss8440-001: [2023-07-27 23:07:39,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=7420, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:39,898] [INFO] [timer.py:215:stop] epoch=0/micro_step=7420/global_step=7420, RunningAvgSamplesPerSec=276.2527709899196, CurrSamplesPerSec=281.6626562370714, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:39.899 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7308
dss8440-001: [2023-07-27 23:07:45,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=7430, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:45,979] [INFO] [timer.py:215:stop] epoch=0/micro_step=7430/global_step=7430, RunningAvgSamplesPerSec=276.2591335045891, CurrSamplesPerSec=278.06838027768833, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:45.980 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7305
dss8440-001: [2023-07-27 23:07:52,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=7440, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:52,205] [INFO] [timer.py:215:stop] epoch=0/micro_step=7440/global_step=7440, RunningAvgSamplesPerSec=276.2566031975044, CurrSamplesPerSec=276.92301452715515, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:52.206 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7301
dss8440-001: [2023-07-27 23:07:58,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=7450, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:07:58,509] [INFO] [timer.py:215:stop] epoch=0/micro_step=7450/global_step=7450, RunningAvgSamplesPerSec=276.2492835406392, CurrSamplesPerSec=261.4988139382298, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:07:58.510 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7297
dss8440-001: [2023-07-27 23:08:04,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=7460, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:04,758] [INFO] [timer.py:215:stop] epoch=0/micro_step=7460/global_step=7460, RunningAvgSamplesPerSec=276.2465607424739, CurrSamplesPerSec=269.570630762839, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:04.759 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7293
dss8440-001: [2023-07-27 23:08:10,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=7470, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:11,087] [INFO] [timer.py:215:stop] epoch=0/micro_step=7470/global_step=7470, RunningAvgSamplesPerSec=276.2380606914457, CurrSamplesPerSec=275.49613328099514, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:11.088 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7287
dss8440-001: [2023-07-27 23:08:17,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=7480, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:17,298] [INFO] [timer.py:215:stop] epoch=0/micro_step=7480/global_step=7480, RunningAvgSamplesPerSec=276.23624309146805, CurrSamplesPerSec=266.8581969805133, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:17.298 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7279
dss8440-001: [2023-07-27 23:08:23,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=7490, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:23,492] [INFO] [timer.py:215:stop] epoch=0/micro_step=7490/global_step=7490, RunningAvgSamplesPerSec=276.23560189174185, CurrSamplesPerSec=277.26592686387573, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:23.493 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7271
dss8440-001: [2023-07-27 23:08:29,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=7500, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:29,584] [INFO] [timer.py:215:stop] epoch=0/micro_step=7500/global_step=7500, RunningAvgSamplesPerSec=276.2411102707926, CurrSamplesPerSec=282.6860643406468, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:29.585 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7268
dss8440-001: [2023-07-27 23:08:35,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=7510, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:35,677] [INFO] [timer.py:215:stop] epoch=0/micro_step=7510/global_step=7510, RunningAvgSamplesPerSec=276.24669468014633, CurrSamplesPerSec=270.14306893057716, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:35.677 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7263
dss8440-001: [2023-07-27 23:08:41,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=7520, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:41,923] [INFO] [timer.py:215:stop] epoch=0/micro_step=7520/global_step=7520, RunningAvgSamplesPerSec=276.24296994477, CurrSamplesPerSec=286.16411316883625, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:41.924 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7256
dss8440-001: [2023-07-27 23:08:48,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=7530, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:48,153] [INFO] [timer.py:215:stop] epoch=0/micro_step=7530/global_step=7530, RunningAvgSamplesPerSec=276.24019276246895, CurrSamplesPerSec=271.4228653045251, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:48.154 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7251
dss8440-001: [2023-07-27 23:08:54,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=7540, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:08:54,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=7540/global_step=7540, RunningAvgSamplesPerSec=276.2379458317292, CurrSamplesPerSec=281.61425202335425, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:08:54.373 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7245
dss8440-001: [2023-07-27 23:09:00,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=7550, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:00,527] [INFO] [timer.py:215:stop] epoch=0/micro_step=7550/global_step=7550, RunningAvgSamplesPerSec=276.23988917507535, CurrSamplesPerSec=270.59599752845673, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:00.528 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7236
dss8440-001: [2023-07-27 23:09:06,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=7560, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:06,776] [INFO] [timer.py:215:stop] epoch=0/micro_step=7560/global_step=7560, RunningAvgSamplesPerSec=276.23658939340265, CurrSamplesPerSec=264.98270610302933, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:06.777 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7232
dss8440-001: [2023-07-27 23:09:12,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=7570, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:13,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=7570/global_step=7570, RunningAvgSamplesPerSec=276.23182121159135, CurrSamplesPerSec=280.83297477440277, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:13.047 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7226
dss8440-001: [2023-07-27 23:09:19,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=7580, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:19,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=7580/global_step=7580, RunningAvgSamplesPerSec=276.22722240229393, CurrSamplesPerSec=276.5821435258182, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:19.315 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7221
dss8440-001: [2023-07-27 23:09:25,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=7590, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:25,637] [INFO] [timer.py:215:stop] epoch=0/micro_step=7590/global_step=7590, RunningAvgSamplesPerSec=276.2197606187904, CurrSamplesPerSec=277.7134114451447, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:25.638 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7216
dss8440-001: [2023-07-27 23:09:31,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=7600, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:31,836] [INFO] [timer.py:215:stop] epoch=0/micro_step=7600/global_step=7600, RunningAvgSamplesPerSec=276.2191343800505, CurrSamplesPerSec=275.1168655929722, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:31.836 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7214
dss8440-001: [2023-07-27 23:09:38,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=7610, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:38,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=7610/global_step=7610, RunningAvgSamplesPerSec=276.2128314681335, CurrSamplesPerSec=280.35519779230793, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:38.132 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7210
dss8440-001: [2023-07-27 23:09:44,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=7620, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:44,421] [INFO] [timer.py:215:stop] epoch=0/micro_step=7620/global_step=7620, RunningAvgSamplesPerSec=276.2068049598968, CurrSamplesPerSec=270.4823634859614, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:44.422 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7204
dss8440-001: [2023-07-27 23:09:50,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=7630, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:50,578] [INFO] [timer.py:215:stop] epoch=0/micro_step=7630/global_step=7630, RunningAvgSamplesPerSec=276.2084084284831, CurrSamplesPerSec=282.6720025802373, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:50.579 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7197
dss8440-001: [2023-07-27 23:09:56,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=7640, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:09:56,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=7640/global_step=7640, RunningAvgSamplesPerSec=276.2084781902294, CurrSamplesPerSec=261.6477647675651, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:09:56.760 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7191
dss8440-001: [2023-07-27 23:10:02,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=7650, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:02,947] [INFO] [timer.py:215:stop] epoch=0/micro_step=7650/global_step=7650, RunningAvgSamplesPerSec=276.20828104644056, CurrSamplesPerSec=273.7097649681462, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:02.947 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7188
dss8440-001: [2023-07-27 23:10:09,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=7660, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:09,151] [INFO] [timer.py:215:stop] epoch=0/micro_step=7660/global_step=7660, RunningAvgSamplesPerSec=276.20720184133404, CurrSamplesPerSec=279.96088574432224, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:09.152 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7183
dss8440-001: [2023-07-27 23:10:15,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=7670, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:15,402] [INFO] [timer.py:215:stop] epoch=0/micro_step=7670/global_step=7670, RunningAvgSamplesPerSec=276.2033484473152, CurrSamplesPerSec=285.0279942188886, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:15.403 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7176
dss8440-001: [2023-07-27 23:10:21,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=7680, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:21,452] [INFO] [timer.py:215:stop] epoch=0/micro_step=7680/global_step=7680, RunningAvgSamplesPerSec=276.21143280821354, CurrSamplesPerSec=273.06201066140056, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:21.453 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7170
dss8440-001: [2023-07-27 23:10:27,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=7690, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:27,690] [INFO] [timer.py:215:stop] epoch=0/micro_step=7690/global_step=7690, RunningAvgSamplesPerSec=276.20862153976367, CurrSamplesPerSec=270.86102941656964, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:27.691 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7167
dss8440-001: [2023-07-27 23:10:33,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=7700, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:33,862] [INFO] [timer.py:215:stop] epoch=0/micro_step=7700/global_step=7700, RunningAvgSamplesPerSec=276.2101404370576, CurrSamplesPerSec=267.0518729629349, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:33.863 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7164
dss8440-001: [2023-07-27 23:10:39,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=7710, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:40,066] [INFO] [timer.py:215:stop] epoch=0/micro_step=7710/global_step=7710, RunningAvgSamplesPerSec=276.2090437669594, CurrSamplesPerSec=280.1838587221677, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:40.067 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7156
dss8440-001: [2023-07-27 23:10:46,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=7720, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:46,166] [INFO] [timer.py:215:stop] epoch=0/micro_step=7720/global_step=7720, RunningAvgSamplesPerSec=276.2145070123341, CurrSamplesPerSec=267.5114459882759, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:46.167 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7147
dss8440-001: [2023-07-27 23:10:52,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=7730, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:52,369] [INFO] [timer.py:215:stop] epoch=0/micro_step=7730/global_step=7730, RunningAvgSamplesPerSec=276.21366895938206, CurrSamplesPerSec=266.9665804489811, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:52.370 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7141
dss8440-001: [2023-07-27 23:10:58,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=7740, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:10:58,505] [INFO] [timer.py:215:stop] epoch=0/micro_step=7740/global_step=7740, RunningAvgSamplesPerSec=276.21669621582794, CurrSamplesPerSec=276.22954895326535, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:10:58.506 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7134
dss8440-001: [2023-07-27 23:11:04,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=7750, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:04,732] [INFO] [timer.py:215:stop] epoch=0/micro_step=7750/global_step=7750, RunningAvgSamplesPerSec=276.21397645335446, CurrSamplesPerSec=275.3471412695361, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:04.733 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7126
dss8440-001: [2023-07-27 23:11:10,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=7760, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:10,859] [INFO] [timer.py:215:stop] epoch=0/micro_step=7760/global_step=7760, RunningAvgSamplesPerSec=276.21751932465503, CurrSamplesPerSec=274.7085433189204, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:10.860 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7120
dss8440-001: [2023-07-27 23:11:17,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=7770, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:17,104] [INFO] [timer.py:215:stop] epoch=0/micro_step=7770/global_step=7770, RunningAvgSamplesPerSec=276.21409469772385, CurrSamplesPerSec=266.8326305074887, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:17.104 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7115
dss8440-001: [2023-07-27 23:11:23,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=7780, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:23,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=7780/global_step=7780, RunningAvgSamplesPerSec=276.2088482932232, CurrSamplesPerSec=263.6958619195332, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:23.376 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7112
dss8440-001: [2023-07-27 23:11:29,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=7790, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:29,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=7790/global_step=7790, RunningAvgSamplesPerSec=276.2151784597058, CurrSamplesPerSec=287.1314897081511, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:29.463 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7107
dss8440-001: [2023-07-27 23:11:35,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=7800, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:35,581] [INFO] [timer.py:215:stop] epoch=0/micro_step=7800/global_step=7800, RunningAvgSamplesPerSec=276.219834535155, CurrSamplesPerSec=292.81129951485053, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:35.582 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7100
dss8440-001: [2023-07-27 23:11:41,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=7810, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:41,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=7810/global_step=7810, RunningAvgSamplesPerSec=276.21674235065063, CurrSamplesPerSec=256.4106278365011, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:41.821 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7096
dss8440-001: [2023-07-27 23:11:47,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=7820, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:47,976] [INFO] [timer.py:215:stop] epoch=0/micro_step=7820/global_step=7820, RunningAvgSamplesPerSec=276.21883031826775, CurrSamplesPerSec=278.2265943253299, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:47.977 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7091
dss8440-001: [2023-07-27 23:11:53,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=7830, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:11:54,035] [INFO] [timer.py:215:stop] epoch=0/micro_step=7830/global_step=7830, RunningAvgSamplesPerSec=276.22675009801, CurrSamplesPerSec=288.41663054516255, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:11:54.036 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7088
dss8440-001: [2023-07-27 23:12:00,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=7840, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:00,313] [INFO] [timer.py:215:stop] epoch=0/micro_step=7840/global_step=7840, RunningAvgSamplesPerSec=276.2215033726944, CurrSamplesPerSec=281.139984471616, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:00.314 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7082
dss8440-001: [2023-07-27 23:12:06,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=7850, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:06,530] [INFO] [timer.py:215:stop] epoch=0/micro_step=7850/global_step=7850, RunningAvgSamplesPerSec=276.21962706728345, CurrSamplesPerSec=276.5438263382671, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:06.530 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7077
dss8440-001: [2023-07-27 23:12:12,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=7860, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:12,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=7860/global_step=7860, RunningAvgSamplesPerSec=276.2171321961082, CurrSamplesPerSec=283.8859916507328, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:12.763 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7074
dss8440-001: [2023-07-27 23:12:18,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=7870, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:19,027] [INFO] [timer.py:215:stop] epoch=0/micro_step=7870/global_step=7870, RunningAvgSamplesPerSec=276.21244297878343, CurrSamplesPerSec=263.41255488204916, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:19.028 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7069
dss8440-001: [2023-07-27 23:12:25,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=7880, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:25,256] [INFO] [timer.py:215:stop] epoch=0/micro_step=7880/global_step=7880, RunningAvgSamplesPerSec=276.21037475461395, CurrSamplesPerSec=275.9200906572731, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:25.257 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7064
dss8440-001: [2023-07-27 23:12:31,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=7890, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:31,344] [INFO] [timer.py:215:stop] epoch=0/micro_step=7890/global_step=7890, RunningAvgSamplesPerSec=276.21602911384514, CurrSamplesPerSec=284.3335465233814, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:31.345 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7060
dss8440-001: [2023-07-27 23:12:37,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=7900, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:37,492] [INFO] [timer.py:215:stop] epoch=0/micro_step=7900/global_step=7900, RunningAvgSamplesPerSec=276.21796396679485, CurrSamplesPerSec=286.3413340918788, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:37.492 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7058
dss8440-001: [2023-07-27 23:12:43,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=7910, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:43,698] [INFO] [timer.py:215:stop] epoch=0/micro_step=7910/global_step=7910, RunningAvgSamplesPerSec=276.21675522417837, CurrSamplesPerSec=264.41454183580885, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:43.699 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7051
dss8440-001: [2023-07-27 23:12:49,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=7920, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:49,967] [INFO] [timer.py:215:stop] epoch=0/micro_step=7920/global_step=7920, RunningAvgSamplesPerSec=276.21203738943615, CurrSamplesPerSec=275.74668231979336, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:49.967 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7046
dss8440-001: [2023-07-27 23:12:56,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=7930, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:12:56,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=7930/global_step=7930, RunningAvgSamplesPerSec=276.21041310464057, CurrSamplesPerSec=269.17489224685414, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:12:56.186 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7041
dss8440-001: [2023-07-27 23:13:02,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=7940, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:02,514] [INFO] [timer.py:215:stop] epoch=0/micro_step=7940/global_step=7940, RunningAvgSamplesPerSec=276.2021441650172, CurrSamplesPerSec=257.1431023736295, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:02.514 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7038
dss8440-001: [2023-07-27 23:13:08,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=7950, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:08,819] [INFO] [timer.py:215:stop] epoch=0/micro_step=7950/global_step=7950, RunningAvgSamplesPerSec=276.1954084789006, CurrSamplesPerSec=285.86204165882344, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:08.820 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7033
dss8440-001: [2023-07-27 23:13:15,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=7960, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:15,049] [INFO] [timer.py:215:stop] epoch=0/micro_step=7960/global_step=7960, RunningAvgSamplesPerSec=276.19301505233545, CurrSamplesPerSec=283.72961729042294, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:15.050 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7029
dss8440-001: [2023-07-27 23:13:21,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=7970, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:21,272] [INFO] [timer.py:215:stop] epoch=0/micro_step=7970/global_step=7970, RunningAvgSamplesPerSec=276.1910468109096, CurrSamplesPerSec=273.50206880062353, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:21.273 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7024
dss8440-001: [2023-07-27 23:13:27,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=7980, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:27,472] [INFO] [timer.py:215:stop] epoch=0/micro_step=7980/global_step=7980, RunningAvgSamplesPerSec=276.1904977606739, CurrSamplesPerSec=262.2410721230515, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:27.473 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7017
dss8440-001: [2023-07-27 23:13:33,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=7990, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:33,710] [INFO] [timer.py:215:stop] epoch=0/micro_step=7990/global_step=7990, RunningAvgSamplesPerSec=276.18763008552, CurrSamplesPerSec=288.3587969224511, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:33.710 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7012
dss8440-001: [2023-07-27 23:13:39,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:39,856] [INFO] [timer.py:215:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=276.19036335068324, CurrSamplesPerSec=275.496564127426, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:39.857 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7009
dss8440-001: [2023-07-27 23:13:39,863] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8000 is about to be saved!
dss8440-001: [2023-07-27 23:13:39,866] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 23:13:39,866] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 23:13:39,921] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,927] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,927] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:13:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:13:39,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-003: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,941] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:13:39,941] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:13:39,941] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:13:39,941] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step8000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:13:39,942] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step8000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:13:39,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
dss8440-001: 2023-07-27 23:13:39.944 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 23:13:46,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=8010, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:46,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=8010/global_step=8010, RunningAvgSamplesPerSec=276.18786157546555, CurrSamplesPerSec=270.36322302681634, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:46.176 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7004
dss8440-001: [2023-07-27 23:13:52,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=8020, skipped=24, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:52,433] [INFO] [timer.py:215:stop] epoch=0/micro_step=8020/global_step=8020, RunningAvgSamplesPerSec=276.18390928107107, CurrSamplesPerSec=261.8257764837157, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:52.433 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.7001
dss8440-001: [2023-07-27 23:13:55,007] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 23:13:57,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=8030, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:13:58,084] [INFO] [timer.py:215:stop] epoch=0/micro_step=8030/global_step=8030, RunningAvgSamplesPerSec=276.2140377773275, CurrSamplesPerSec=272.90993128481364, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:13:58.085 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6996
dss8440-001: [2023-07-27 23:14:04,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=8040, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:04,287] [INFO] [timer.py:215:stop] epoch=0/micro_step=8040/global_step=8040, RunningAvgSamplesPerSec=276.2133516539283, CurrSamplesPerSec=298.2611127854175, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:04.288 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6992
dss8440-001: [2023-07-27 23:14:10,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=8050, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:10,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=8050/global_step=8050, RunningAvgSamplesPerSec=276.21230656895585, CurrSamplesPerSec=282.81676953568063, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:10.491 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6983
dss8440-001: [2023-07-27 23:14:16,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=8060, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:16,580] [INFO] [timer.py:215:stop] epoch=0/micro_step=8060/global_step=8060, RunningAvgSamplesPerSec=276.2180984644615, CurrSamplesPerSec=289.9026059194773, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:16.581 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6976
dss8440-001: [2023-07-27 23:14:22,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=8070, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:22,739] [INFO] [timer.py:215:stop] epoch=0/micro_step=8070/global_step=8070, RunningAvgSamplesPerSec=276.21945790863583, CurrSamplesPerSec=270.9737314601423, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:22.740 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6971
dss8440-001: [2023-07-27 23:14:28,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=8080, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:28,911] [INFO] [timer.py:215:stop] epoch=0/micro_step=8080/global_step=8080, RunningAvgSamplesPerSec=276.22051622714275, CurrSamplesPerSec=271.54095351060647, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:28.912 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6966
dss8440-001: [2023-07-27 23:14:34,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=8090, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:35,032] [INFO] [timer.py:215:stop] epoch=0/micro_step=8090/global_step=8090, RunningAvgSamplesPerSec=276.22431768051496, CurrSamplesPerSec=286.1081086348532, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:35.033 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6964
dss8440-001: [2023-07-27 23:14:41,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=8100, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:41,205] [INFO] [timer.py:215:stop] epoch=0/micro_step=8100/global_step=8100, RunningAvgSamplesPerSec=276.2250506371643, CurrSamplesPerSec=279.00634199746986, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:41.206 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6960
dss8440-001: [2023-07-27 23:14:47,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=8110, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:47,399] [INFO] [timer.py:215:stop] epoch=0/micro_step=8110/global_step=8110, RunningAvgSamplesPerSec=276.22428346813837, CurrSamplesPerSec=265.27369670521296, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:47.399 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6953
dss8440-001: [2023-07-27 23:14:53,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=8120, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:53,617] [INFO] [timer.py:215:stop] epoch=0/micro_step=8120/global_step=8120, RunningAvgSamplesPerSec=276.22231138210765, CurrSamplesPerSec=264.7332248817489, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:53.618 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6949
dss8440-001: [2023-07-27 23:14:59,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=8130, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:14:59,789] [INFO] [timer.py:215:stop] epoch=0/micro_step=8130/global_step=8130, RunningAvgSamplesPerSec=276.2235206438801, CurrSamplesPerSec=283.0785428653382, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:14:59.790 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6948
dss8440-001: [2023-07-27 23:15:05,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=8140, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:05,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=8140/global_step=8140, RunningAvgSamplesPerSec=276.2247710926834, CurrSamplesPerSec=259.02186149095724, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:05.950 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6941
dss8440-001: [2023-07-27 23:15:12,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=8150, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:12,135] [INFO] [timer.py:215:stop] epoch=0/micro_step=8150/global_step=8150, RunningAvgSamplesPerSec=276.22490355121676, CurrSamplesPerSec=276.61710500954126, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:12.136 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6938
dss8440-001: [2023-07-27 23:15:18,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=8160, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:18,412] [INFO] [timer.py:215:stop] epoch=0/micro_step=8160/global_step=8160, RunningAvgSamplesPerSec=276.22003121034015, CurrSamplesPerSec=265.8904037015599, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:18.413 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6930
dss8440-001: [2023-07-27 23:15:24,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=8170, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:24,645] [INFO] [timer.py:215:stop] epoch=0/micro_step=8170/global_step=8170, RunningAvgSamplesPerSec=276.217549396651, CurrSamplesPerSec=274.51526318311494, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:24.647 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6926
dss8440-001: [2023-07-27 23:15:30,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=8180, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:30,896] [INFO] [timer.py:215:stop] epoch=0/micro_step=8180/global_step=8180, RunningAvgSamplesPerSec=276.2148922070645, CurrSamplesPerSec=270.21817537567847, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:30.897 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6919
dss8440-001: [2023-07-27 23:15:36,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=8190, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:37,118] [INFO] [timer.py:215:stop] epoch=0/micro_step=8190/global_step=8190, RunningAvgSamplesPerSec=276.2130657676626, CurrSamplesPerSec=260.5486243274533, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:37.119 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6914
dss8440-001: [2023-07-27 23:15:43,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=8200, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:43,367] [INFO] [timer.py:215:stop] epoch=0/micro_step=8200/global_step=8200, RunningAvgSamplesPerSec=276.2095814740026, CurrSamplesPerSec=280.51680728805474, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:43.368 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6909
dss8440-001: [2023-07-27 23:15:49,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=8210, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:49,547] [INFO] [timer.py:215:stop] epoch=0/micro_step=8210/global_step=8210, RunningAvgSamplesPerSec=276.2097498162891, CurrSamplesPerSec=276.84011083928186, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:49.548 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6904
dss8440-001: [2023-07-27 23:15:55,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=8220, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:15:55,738] [INFO] [timer.py:215:stop] epoch=0/micro_step=8220/global_step=8220, RunningAvgSamplesPerSec=276.20963595410893, CurrSamplesPerSec=284.17450135383973, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:15:55.739 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6899
dss8440-001: [2023-07-27 23:16:01,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=8230, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:01,936] [INFO] [timer.py:215:stop] epoch=0/micro_step=8230/global_step=8230, RunningAvgSamplesPerSec=276.2095916626553, CurrSamplesPerSec=288.2759818110038, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:01.937 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6896
dss8440-001: [2023-07-27 23:16:08,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=8240, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:08,102] [INFO] [timer.py:215:stop] epoch=0/micro_step=8240/global_step=8240, RunningAvgSamplesPerSec=276.2108484467223, CurrSamplesPerSec=270.12908820329517, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:08.102 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6890
dss8440-001: [2023-07-27 23:16:14,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=8250, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:14,291] [INFO] [timer.py:215:stop] epoch=0/micro_step=8250/global_step=8250, RunningAvgSamplesPerSec=276.2106305800185, CurrSamplesPerSec=273.7629348576508, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:14.291 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6888
dss8440-001: [2023-07-27 23:16:20,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=8260, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:20,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=8260/global_step=8260, RunningAvgSamplesPerSec=276.21174878732387, CurrSamplesPerSec=280.64316646188587, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:20.461 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6885
dss8440-001: [2023-07-27 23:16:26,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=8270, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:26,611] [INFO] [timer.py:215:stop] epoch=0/micro_step=8270/global_step=8270, RunningAvgSamplesPerSec=276.21380508723934, CurrSamplesPerSec=291.367944703845, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:26.611 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6882
dss8440-001: [2023-07-27 23:16:32,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=8280, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:32,812] [INFO] [timer.py:215:stop] epoch=0/micro_step=8280/global_step=8280, RunningAvgSamplesPerSec=276.21292618984194, CurrSamplesPerSec=275.1591936446251, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:32.812 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6873
dss8440-001: [2023-07-27 23:16:38,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=8290, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:39,024] [INFO] [timer.py:215:stop] epoch=0/micro_step=8290/global_step=8290, RunningAvgSamplesPerSec=276.21195934470325, CurrSamplesPerSec=275.2765770594477, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:39.024 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6870
dss8440-001: [2023-07-27 23:16:45,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=8300, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:45,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=8300/global_step=8300, RunningAvgSamplesPerSec=276.2128921521044, CurrSamplesPerSec=293.12214742319117, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:45.194 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6868
dss8440-001: [2023-07-27 23:16:51,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=8310, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:51,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=8310/global_step=8310, RunningAvgSamplesPerSec=276.2092754962435, CurrSamplesPerSec=272.1895986356587, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:51.451 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6863
dss8440-001: [2023-07-27 23:16:57,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=8320, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:16:57,660] [INFO] [timer.py:215:stop] epoch=0/micro_step=8320/global_step=8320, RunningAvgSamplesPerSec=276.20781628734454, CurrSamplesPerSec=275.7655674157853, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:16:57.661 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6857
dss8440-001: [2023-07-27 23:17:03,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=8330, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:03,856] [INFO] [timer.py:215:stop] epoch=0/micro_step=8330/global_step=8330, RunningAvgSamplesPerSec=276.2074793019919, CurrSamplesPerSec=271.7622399306405, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:03.856 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6853
dss8440-001: [2023-07-27 23:17:09,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=8340, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:10,029] [INFO] [timer.py:215:stop] epoch=0/micro_step=8340/global_step=8340, RunningAvgSamplesPerSec=276.2090237032295, CurrSamplesPerSec=273.42979442021567, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:10.030 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6848
dss8440-001: [2023-07-27 23:17:16,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=8350, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:16,179] [INFO] [timer.py:215:stop] epoch=0/micro_step=8350/global_step=8350, RunningAvgSamplesPerSec=276.2112991424457, CurrSamplesPerSec=273.404544539208, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:16.180 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6845
dss8440-001: [2023-07-27 23:17:22,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=8360, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:22,414] [INFO] [timer.py:215:stop] epoch=0/micro_step=8360/global_step=8360, RunningAvgSamplesPerSec=276.20856582157955, CurrSamplesPerSec=265.89712606398575, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:22.414 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6841
dss8440-001: [2023-07-27 23:17:28,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=8370, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:28,619] [INFO] [timer.py:215:stop] epoch=0/micro_step=8370/global_step=8370, RunningAvgSamplesPerSec=276.20742210180913, CurrSamplesPerSec=292.87531531216996, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:28.621 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6836
dss8440-001: [2023-07-27 23:17:34,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=8380, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:34,841] [INFO] [timer.py:215:stop] epoch=0/micro_step=8380/global_step=8380, RunningAvgSamplesPerSec=276.2055441348644, CurrSamplesPerSec=284.89903885807996, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:34.841 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6833
dss8440-001: [2023-07-27 23:17:40,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=8390, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:41,016] [INFO] [timer.py:215:stop] epoch=0/micro_step=8390/global_step=8390, RunningAvgSamplesPerSec=276.2062121693829, CurrSamplesPerSec=281.43339011448757, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:41.016 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6830
dss8440-001: [2023-07-27 23:17:47,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=8400, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:47,226] [INFO] [timer.py:215:stop] epoch=0/micro_step=8400/global_step=8400, RunningAvgSamplesPerSec=276.20487839438283, CurrSamplesPerSec=262.0381319154538, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:47.227 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6826
dss8440-001: [2023-07-27 23:17:53,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=8410, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:53,413] [INFO] [timer.py:215:stop] epoch=0/micro_step=8410/global_step=8410, RunningAvgSamplesPerSec=276.20483938460706, CurrSamplesPerSec=285.975736973599, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:53.414 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6819
dss8440-001: [2023-07-27 23:17:59,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=8420, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:17:59,553] [INFO] [timer.py:215:stop] epoch=0/micro_step=8420/global_step=8420, RunningAvgSamplesPerSec=276.2073761566348, CurrSamplesPerSec=273.76495572085605, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:17:59.554 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6818
dss8440-001: [2023-07-27 23:18:05,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=8430, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:05,646] [INFO] [timer.py:215:stop] epoch=0/micro_step=8430/global_step=8430, RunningAvgSamplesPerSec=276.2125435573254, CurrSamplesPerSec=271.18522250184054, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:05.647 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6813
dss8440-001: [2023-07-27 23:18:11,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=8440, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:11,798] [INFO] [timer.py:215:stop] epoch=0/micro_step=8440/global_step=8440, RunningAvgSamplesPerSec=276.2143989499678, CurrSamplesPerSec=267.32410060574836, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:11.799 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6811
dss8440-001: [2023-07-27 23:18:17,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=8450, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:17,999] [INFO] [timer.py:215:stop] epoch=0/micro_step=8450/global_step=8450, RunningAvgSamplesPerSec=276.21393655892473, CurrSamplesPerSec=261.0131372779987, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:18.000 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6804
dss8440-001: [2023-07-27 23:18:24,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=8460, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:24,213] [INFO] [timer.py:215:stop] epoch=0/micro_step=8460/global_step=8460, RunningAvgSamplesPerSec=276.21258052135073, CurrSamplesPerSec=276.32779901067676, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:24.213 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6800
dss8440-001: [2023-07-27 23:18:30,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=8470, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:30,371] [INFO] [timer.py:215:stop] epoch=0/micro_step=8470/global_step=8470, RunningAvgSamplesPerSec=276.21419148661766, CurrSamplesPerSec=275.0542568543695, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:30.372 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6797
dss8440-001: [2023-07-27 23:18:36,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=8480, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:36,605] [INFO] [timer.py:215:stop] epoch=0/micro_step=8480/global_step=8480, RunningAvgSamplesPerSec=276.2117971924238, CurrSamplesPerSec=272.5188788811552, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:36.606 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6792
dss8440-001: [2023-07-27 23:18:42,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=8490, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:42,773] [INFO] [timer.py:215:stop] epoch=0/micro_step=8490/global_step=8490, RunningAvgSamplesPerSec=276.21291522868694, CurrSamplesPerSec=281.93154595551385, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:42.774 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6789
dss8440-001: [2023-07-27 23:18:48,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=8500, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:48,990] [INFO] [timer.py:215:stop] epoch=0/micro_step=8500/global_step=8500, RunningAvgSamplesPerSec=276.211408837738, CurrSamplesPerSec=260.1731496749696, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:48.990 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6783
dss8440-001: [2023-07-27 23:18:55,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=8510, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:18:55,125] [INFO] [timer.py:215:stop] epoch=0/micro_step=8510/global_step=8510, RunningAvgSamplesPerSec=276.21399246659377, CurrSamplesPerSec=279.0540748077413, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:18:55.126 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6780
dss8440-001: [2023-07-27 23:19:01,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=8520, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:01,314] [INFO] [timer.py:215:stop] epoch=0/micro_step=8520/global_step=8520, RunningAvgSamplesPerSec=276.21371772440676, CurrSamplesPerSec=276.9721055904381, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:01.315 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6778
dss8440-001: [2023-07-27 23:19:07,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=8530, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:07,558] [INFO] [timer.py:215:stop] epoch=0/micro_step=8530/global_step=8530, RunningAvgSamplesPerSec=276.2106900678234, CurrSamplesPerSec=264.71353330017416, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:07.559 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6774
dss8440-001: [2023-07-27 23:19:13,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=8540, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:13,743] [INFO] [timer.py:215:stop] epoch=0/micro_step=8540/global_step=8540, RunningAvgSamplesPerSec=276.21070421614814, CurrSamplesPerSec=272.4138396018965, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:13.744 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6768
dss8440-001: [2023-07-27 23:19:19,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=8550, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:19,981] [INFO] [timer.py:215:stop] epoch=0/micro_step=8550/global_step=8550, RunningAvgSamplesPerSec=276.20785707241964, CurrSamplesPerSec=260.9541732339452, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:19.982 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6764
dss8440-001: [2023-07-27 23:19:26,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=8560, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:26,102] [INFO] [timer.py:215:stop] epoch=0/micro_step=8560/global_step=8560, RunningAvgSamplesPerSec=276.2113506777442, CurrSamplesPerSec=280.15010611338744, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:26.102 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6760
dss8440-001: [2023-07-27 23:19:32,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=8570, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:32,303] [INFO] [timer.py:215:stop] epoch=0/micro_step=8570/global_step=8570, RunningAvgSamplesPerSec=276.2106253405869, CurrSamplesPerSec=272.21378331237577, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:32.304 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6758
dss8440-001: [2023-07-27 23:19:38,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=8580, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:38,576] [INFO] [timer.py:215:stop] epoch=0/micro_step=8580/global_step=8580, RunningAvgSamplesPerSec=276.2066677387817, CurrSamplesPerSec=262.82903728990067, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:38.577 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6754
dss8440-001: [2023-07-27 23:19:44,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=8590, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:44,816] [INFO] [timer.py:215:stop] epoch=0/micro_step=8590/global_step=8590, RunningAvgSamplesPerSec=276.20396416278606, CurrSamplesPerSec=266.29998374936224, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:44.817 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6751
dss8440-001: [2023-07-27 23:19:51,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=8600, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:51,093] [INFO] [timer.py:215:stop] epoch=0/micro_step=8600/global_step=8600, RunningAvgSamplesPerSec=276.19877983625867, CurrSamplesPerSec=276.09101277988947, MemAllocated=0.07GB, MaxMemAllocated=1.7GB
dss8440-001: 2023-07-27 23:19:51.094 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6747
dss8440-001: [2023-07-27 23:19:57,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=8610, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:19:57,147] [INFO] [timer.py:215:stop] epoch=0/micro_step=8610/global_step=8610, RunningAvgSamplesPerSec=276.2058900903285, CurrSamplesPerSec=278.54488445205, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:19:57.148 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6743
dss8440-001: [2023-07-27 23:20:03,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=8620, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:03,308] [INFO] [timer.py:215:stop] epoch=0/micro_step=8620/global_step=8620, RunningAvgSamplesPerSec=276.2069126525882, CurrSamplesPerSec=285.26177098434397, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:03.309 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6735
dss8440-001: [2023-07-27 23:20:09,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=8630, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:09,476] [INFO] [timer.py:215:stop] epoch=0/micro_step=8630/global_step=8630, RunningAvgSamplesPerSec=276.2079203035258, CurrSamplesPerSec=282.90783533730865, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:09.477 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6731
dss8440-001: [2023-07-27 23:20:15,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=8640, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:15,672] [INFO] [timer.py:215:stop] epoch=0/micro_step=8640/global_step=8640, RunningAvgSamplesPerSec=276.207623943878, CurrSamplesPerSec=280.9169436063608, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:15.673 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6727
dss8440-001: [2023-07-27 23:20:21,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=8650, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:21,864] [INFO] [timer.py:215:stop] epoch=0/micro_step=8650/global_step=8650, RunningAvgSamplesPerSec=276.2073674686755, CurrSamplesPerSec=289.1024839713016, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:21.864 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6722
dss8440-001: [2023-07-27 23:20:28,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=8660, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:28,126] [INFO] [timer.py:215:stop] epoch=0/micro_step=8660/global_step=8660, RunningAvgSamplesPerSec=276.20351336327127, CurrSamplesPerSec=266.4868029778439, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:28.126 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6717
dss8440-001: [2023-07-27 23:20:34,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=8670, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:34,344] [INFO] [timer.py:215:stop] epoch=0/micro_step=8670/global_step=8670, RunningAvgSamplesPerSec=276.2016814285446, CurrSamplesPerSec=281.64317998321275, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:34.345 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6716
dss8440-001: [2023-07-27 23:20:40,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=8680, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:40,503] [INFO] [timer.py:215:stop] epoch=0/micro_step=8680/global_step=8680, RunningAvgSamplesPerSec=276.2032970217408, CurrSamplesPerSec=294.43219218235464, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:40.503 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6713
dss8440-001: [2023-07-27 23:20:46,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=8690, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:46,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=8690/global_step=8690, RunningAvgSamplesPerSec=276.20480883556615, CurrSamplesPerSec=267.6500433204961, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:46.660 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6706
dss8440-001: [2023-07-27 23:20:52,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=8700, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:52,825] [INFO] [timer.py:215:stop] epoch=0/micro_step=8700/global_step=8700, RunningAvgSamplesPerSec=276.20600590543813, CurrSamplesPerSec=285.8649409258034, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:52.825 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6700
dss8440-001: [2023-07-27 23:20:58,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=8710, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:20:58,994] [INFO] [timer.py:215:stop] epoch=0/micro_step=8710/global_step=8710, RunningAvgSamplesPerSec=276.2067364419846, CurrSamplesPerSec=264.8756284585188, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:20:58.994 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6697
dss8440-001: [2023-07-27 23:21:05,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=8720, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:05,145] [INFO] [timer.py:215:stop] epoch=0/micro_step=8720/global_step=8720, RunningAvgSamplesPerSec=276.2090233714978, CurrSamplesPerSec=277.39406918444985, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:05.145 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6693
dss8440-001: [2023-07-27 23:21:11,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=8730, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:11,359] [INFO] [timer.py:215:stop] epoch=0/micro_step=8730/global_step=8730, RunningAvgSamplesPerSec=276.2074982850271, CurrSamplesPerSec=285.9065808864104, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:11.359 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6690
dss8440-001: [2023-07-27 23:21:17,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=8740, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:17,604] [INFO] [timer.py:215:stop] epoch=0/micro_step=8740/global_step=8740, RunningAvgSamplesPerSec=276.2045133345912, CurrSamplesPerSec=271.23303097664484, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:17.605 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6687
dss8440-001: [2023-07-27 23:21:23,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=8750, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:23,785] [INFO] [timer.py:215:stop] epoch=0/micro_step=8750/global_step=8750, RunningAvgSamplesPerSec=276.204562829153, CurrSamplesPerSec=267.0609821452259, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:23.786 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6683
dss8440-001: [2023-07-27 23:21:29,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=8760, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:29,927] [INFO] [timer.py:215:stop] epoch=0/micro_step=8760/global_step=8760, RunningAvgSamplesPerSec=276.20705268316436, CurrSamplesPerSec=266.72486056093584, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:29.928 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6677
dss8440-001: [2023-07-27 23:21:36,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=8770, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:36,216] [INFO] [timer.py:215:stop] epoch=0/micro_step=8770/global_step=8770, RunningAvgSamplesPerSec=276.2017893440156, CurrSamplesPerSec=267.849554232228, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:36.216 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6672
dss8440-001: [2023-07-27 23:21:42,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=8780, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:42,445] [INFO] [timer.py:215:stop] epoch=0/micro_step=8780/global_step=8780, RunningAvgSamplesPerSec=276.1997958838559, CurrSamplesPerSec=263.27919563893425, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:42.446 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6670
dss8440-001: [2023-07-27 23:21:48,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=8790, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:48,575] [INFO] [timer.py:215:stop] epoch=0/micro_step=8790/global_step=8790, RunningAvgSamplesPerSec=276.2024058833541, CurrSamplesPerSec=275.617146359561, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:48.576 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6665
dss8440-001: [2023-07-27 23:21:54,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=8800, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:21:54,823] [INFO] [timer.py:215:stop] epoch=0/micro_step=8800/global_step=8800, RunningAvgSamplesPerSec=276.19912267129536, CurrSamplesPerSec=268.61535153828737, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:21:54.824 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6662
dss8440-001: [2023-07-27 23:22:00,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=8810, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:01,010] [INFO] [timer.py:215:stop] epoch=0/micro_step=8810/global_step=8810, RunningAvgSamplesPerSec=276.19945426813416, CurrSamplesPerSec=286.17108622214715, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:01.011 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6661
dss8440-001: [2023-07-27 23:22:07,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=8820, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:07,094] [INFO] [timer.py:215:stop] epoch=0/micro_step=8820/global_step=8820, RunningAvgSamplesPerSec=276.20448494393185, CurrSamplesPerSec=285.9290877528711, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:07.094 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6656
dss8440-001: [2023-07-27 23:22:13,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=8830, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:13,330] [INFO] [timer.py:215:stop] epoch=0/micro_step=8830/global_step=8830, RunningAvgSamplesPerSec=276.20214658433076, CurrSamplesPerSec=294.95450669718446, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:13.331 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6652
dss8440-001: [2023-07-27 23:22:19,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=8840, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:19,512] [INFO] [timer.py:215:stop] epoch=0/micro_step=8840/global_step=8840, RunningAvgSamplesPerSec=276.2024096111613, CurrSamplesPerSec=275.03525435107593, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:19.513 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6648
dss8440-001: [2023-07-27 23:22:25,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=8850, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:25,666] [INFO] [timer.py:215:stop] epoch=0/micro_step=8850/global_step=8850, RunningAvgSamplesPerSec=276.2043169712042, CurrSamplesPerSec=281.16129835431343, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:25.667 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6644
dss8440-001: [2023-07-27 23:22:31,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=8860, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:31,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=8860/global_step=8860, RunningAvgSamplesPerSec=276.2030483462581, CurrSamplesPerSec=260.1754552053695, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:31.887 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6639
dss8440-001: [2023-07-27 23:22:38,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=8870, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:38,070] [INFO] [timer.py:215:stop] epoch=0/micro_step=8870/global_step=8870, RunningAvgSamplesPerSec=276.2035408074837, CurrSamplesPerSec=288.8757541221518, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:38.071 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6636
dss8440-001: [2023-07-27 23:22:44,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=8880, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:44,327] [INFO] [timer.py:215:stop] epoch=0/micro_step=8880/global_step=8880, RunningAvgSamplesPerSec=276.2004471474855, CurrSamplesPerSec=279.97123060217154, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:44.328 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6632
dss8440-001: [2023-07-27 23:22:50,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=8890, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:50,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=8890/global_step=8890, RunningAvgSamplesPerSec=276.19986731603603, CurrSamplesPerSec=280.81506791993553, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:50.526 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6630
dss8440-001: [2023-07-27 23:22:56,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=8900, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:22:56,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=8900/global_step=8900, RunningAvgSamplesPerSec=276.19365643563856, CurrSamplesPerSec=265.43017966859793, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:22:56.833 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6624
dss8440-001: [2023-07-27 23:23:02,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=8910, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:02,979] [INFO] [timer.py:215:stop] epoch=0/micro_step=8910/global_step=8910, RunningAvgSamplesPerSec=276.19617576849, CurrSamplesPerSec=279.7138376051989, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:02.980 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6618
dss8440-001: [2023-07-27 23:23:09,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=8920, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:09,197] [INFO] [timer.py:215:stop] epoch=0/micro_step=8920/global_step=8920, RunningAvgSamplesPerSec=276.1949480435002, CurrSamplesPerSec=273.8567769190858, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:09.197 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6613
dss8440-001: [2023-07-27 23:23:15,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=8930, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:15,474] [INFO] [timer.py:215:stop] epoch=0/micro_step=8930/global_step=8930, RunningAvgSamplesPerSec=276.1900283896111, CurrSamplesPerSec=264.77659647221594, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:15.475 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6610
dss8440-001: [2023-07-27 23:23:21,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=8940, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:21,679] [INFO] [timer.py:215:stop] epoch=0/micro_step=8940/global_step=8940, RunningAvgSamplesPerSec=276.1894835705646, CurrSamplesPerSec=291.1505204937439, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:21.680 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6606
dss8440-001: [2023-07-27 23:23:27,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=8950, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:27,877] [INFO] [timer.py:215:stop] epoch=0/micro_step=8950/global_step=8950, RunningAvgSamplesPerSec=276.1892616241384, CurrSamplesPerSec=270.39330957527517, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:27.878 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6602
dss8440-001: [2023-07-27 23:23:34,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=8960, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:34,133] [INFO] [timer.py:215:stop] epoch=0/micro_step=8960/global_step=8960, RunningAvgSamplesPerSec=276.18573255878897, CurrSamplesPerSec=268.76022073148965, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:34.134 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6597
dss8440-001: [2023-07-27 23:23:40,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=8970, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:40,334] [INFO] [timer.py:215:stop] epoch=0/micro_step=8970/global_step=8970, RunningAvgSamplesPerSec=276.18481005970926, CurrSamplesPerSec=277.27149106359735, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:40.335 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6592
dss8440-001: [2023-07-27 23:23:46,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=8980, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:46,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=8980/global_step=8980, RunningAvgSamplesPerSec=276.1855409429908, CurrSamplesPerSec=280.66138676327904, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:46.516 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6588
dss8440-001: [2023-07-27 23:23:52,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=8990, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:52,691] [INFO] [timer.py:215:stop] epoch=0/micro_step=8990/global_step=8990, RunningAvgSamplesPerSec=276.1862328191023, CurrSamplesPerSec=285.0606260917689, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:52.692 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6584
dss8440-001: [2023-07-27 23:23:58,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:23:58,749] [INFO] [timer.py:215:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=276.19303585629547, CurrSamplesPerSec=286.3365634642087, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:23:58.749 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6580
dss8440-001: [2023-07-27 23:23:58,751] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9000 is about to be saved!
dss8440-001: [2023-07-27 23:23:58,753] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/mp_rank_00_model_states.pt
dss8440-001: [2023-07-27 23:23:58,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/mp_rank_00_model_states.pt...
dss8440-001: [2023-07-27 23:23:58,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/mp_rank_00_model_states.pt.
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_17_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_18_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_16_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_19_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_14_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_15_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_20_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_1_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_2_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_3_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_4_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_6_mp_rank_00_optim_states.pt...
dss8440-001: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_5_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_8_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_13_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_9_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_10_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_11_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_7_mp_rank_00_optim_states.pt...
dss8440-002: [2023-07-27 23:23:58,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_12_mp_rank_00_optim_states.pt...
dss8440-003: [2023-07-27 23:23:58,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_18_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,829] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_18_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-003: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_17_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_17_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-003: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_19_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_19_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_20_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_20_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_2_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_2_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_3_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_3_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_4_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_1_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_4_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_1_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_11_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,829] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_11_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:23:58,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_7_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_7_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_10_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_10_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_14_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_16_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_15_mp_rank_00_optim_states.pt.
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_16_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_15_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg/global_step9000/zero_pp_rank_14_mp_rank_00_optim_states.pt
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-003: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_0_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_8_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_13_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_9_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_8_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_6_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_13_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_9_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_12_mp_rank_00_optim_states.pt.
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_5_mp_rank_00_optim_states.pt.
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_12_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_6_mp_rank_00_optim_states.pt
dss8440-002: [2023-07-27 23:23:58,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [engine.py:3285:_save_zero_checkpoint] zero checkpoint saved ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg/global_step9000/zero_pp_rank_5_mp_rank_00_optim_states.pt
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: [2023-07-27 23:23:58,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
dss8440-001: 2023-07-27 23:23:58.831 | INFO     | __main__:log_dist:53 - [Rank 0] Saved model to ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: [2023-07-27 23:24:04,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=9010, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:04,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=9010/global_step=9010, RunningAvgSamplesPerSec=276.19424024989246, CurrSamplesPerSec=273.6952000229943, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:04.992 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6574
dss8440-001: [2023-07-27 23:24:11,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=9020, skipped=25, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:11,172] [INFO] [timer.py:215:stop] epoch=0/micro_step=9020/global_step=9020, RunningAvgSamplesPerSec=276.1948184208644, CurrSamplesPerSec=277.0652195880922, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:11.173 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6569
dss8440-001: [2023-07-27 23:24:14,404] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
dss8440-001: [2023-07-27 23:24:16,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=9030, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:16,848] [INFO] [timer.py:215:stop] epoch=0/micro_step=9030/global_step=9030, RunningAvgSamplesPerSec=276.2203547148323, CurrSamplesPerSec=287.93495993429303, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:16.849 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6564
dss8440-001: [2023-07-27 23:24:22,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=9040, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:22,993] [INFO] [timer.py:215:stop] epoch=0/micro_step=9040/global_step=9040, RunningAvgSamplesPerSec=276.2225861118769, CurrSamplesPerSec=271.650766211887, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:22.994 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6559
dss8440-001: [2023-07-27 23:24:29,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=9050, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:29,260] [INFO] [timer.py:215:stop] epoch=0/micro_step=9050/global_step=9050, RunningAvgSamplesPerSec=276.21907044799144, CurrSamplesPerSec=270.06655511036945, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:29.262 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6557
dss8440-001: [2023-07-27 23:24:35,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=9060, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:35,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=9060/global_step=9060, RunningAvgSamplesPerSec=276.2159420402107, CurrSamplesPerSec=272.3335081530803, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:35.520 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6554
dss8440-001: [2023-07-27 23:24:41,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=9070, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:41,770] [INFO] [timer.py:215:stop] epoch=0/micro_step=9070/global_step=9070, RunningAvgSamplesPerSec=276.2128523874202, CurrSamplesPerSec=265.71974954635067, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:41.771 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6551
dss8440-001: [2023-07-27 23:24:47,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=9080, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:47,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=9080/global_step=9080, RunningAvgSamplesPerSec=276.213237782521, CurrSamplesPerSec=287.5940396656342, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:47.946 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6547
dss8440-001: [2023-07-27 23:24:54,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=9090, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:24:54,197] [INFO] [timer.py:215:stop] epoch=0/micro_step=9090/global_step=9090, RunningAvgSamplesPerSec=276.2102286600915, CurrSamplesPerSec=268.5100250698194, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:24:54.198 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6544
dss8440-001: [2023-07-27 23:25:00,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=9100, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:00,470] [INFO] [timer.py:215:stop] epoch=0/micro_step=9100/global_step=9100, RunningAvgSamplesPerSec=276.2057127801581, CurrSamplesPerSec=266.1264918658119, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:00.471 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6540
dss8440-001: [2023-07-27 23:25:06,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=9110, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:06,639] [INFO] [timer.py:215:stop] epoch=0/micro_step=9110/global_step=9110, RunningAvgSamplesPerSec=276.2066705140341, CurrSamplesPerSec=277.27792835758873, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:06.640 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6535
dss8440-001: [2023-07-27 23:25:12,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=9120, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:12,767] [INFO] [timer.py:215:stop] epoch=0/micro_step=9120/global_step=9120, RunningAvgSamplesPerSec=276.209235454923, CurrSamplesPerSec=268.96939868058075, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:12.768 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6529
dss8440-001: [2023-07-27 23:25:18,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=9130, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:19,009] [INFO] [timer.py:215:stop] epoch=0/micro_step=9130/global_step=9130, RunningAvgSamplesPerSec=276.2064187504531, CurrSamplesPerSec=277.53828896248143, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:19.011 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6525
dss8440-001: [2023-07-27 23:25:25,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=9140, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:25,146] [INFO] [timer.py:215:stop] epoch=0/micro_step=9140/global_step=9140, RunningAvgSamplesPerSec=276.20937726860615, CurrSamplesPerSec=282.3256635409471, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:25.147 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6521
dss8440-001: [2023-07-27 23:25:31,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=9150, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:31,293] [INFO] [timer.py:215:stop] epoch=0/micro_step=9150/global_step=9150, RunningAvgSamplesPerSec=276.2116445393986, CurrSamplesPerSec=282.0739744419612, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:31.294 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6517
dss8440-001: [2023-07-27 23:25:37,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=9160, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:37,576] [INFO] [timer.py:215:stop] epoch=0/micro_step=9160/global_step=9160, RunningAvgSamplesPerSec=276.2070782483152, CurrSamplesPerSec=271.82912614168197, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:37.577 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6514
dss8440-001: [2023-07-27 23:25:43,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=9170, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:43,734] [INFO] [timer.py:215:stop] epoch=0/micro_step=9170/global_step=9170, RunningAvgSamplesPerSec=276.2083426251904, CurrSamplesPerSec=266.02280649364036, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:43.735 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6507
dss8440-001: [2023-07-27 23:25:49,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=9180, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:49,912] [INFO] [timer.py:215:stop] epoch=0/micro_step=9180/global_step=9180, RunningAvgSamplesPerSec=276.2087566567152, CurrSamplesPerSec=282.2008365385694, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:49.912 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6506
dss8440-001: [2023-07-27 23:25:56,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=9190, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:25:56,085] [INFO] [timer.py:215:stop] epoch=0/micro_step=9190/global_step=9190, RunningAvgSamplesPerSec=276.20950120850125, CurrSamplesPerSec=289.5375005393056, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:25:56.086 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6501
dss8440-001: [2023-07-27 23:26:02,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=9200, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:02,340] [INFO] [timer.py:215:stop] epoch=0/micro_step=9200/global_step=9200, RunningAvgSamplesPerSec=276.2059199001212, CurrSamplesPerSec=281.8015448147446, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:02.341 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6497
dss8440-001: [2023-07-27 23:26:08,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=9210, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:08,534] [INFO] [timer.py:215:stop] epoch=0/micro_step=9210/global_step=9210, RunningAvgSamplesPerSec=276.2056790774829, CurrSamplesPerSec=273.5137466608598, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:08.535 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6492
dss8440-001: [2023-07-27 23:26:14,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=9220, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:14,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=9220/global_step=9220, RunningAvgSamplesPerSec=276.2087776118754, CurrSamplesPerSec=294.9568525347744, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:14.662 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6488
dss8440-001: [2023-07-27 23:26:20,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=9230, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:20,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=9230/global_step=9230, RunningAvgSamplesPerSec=276.20924991677066, CurrSamplesPerSec=276.9856059623345, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:20.839 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6484
dss8440-001: [2023-07-27 23:26:26,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=9240, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:27,044] [INFO] [timer.py:215:stop] epoch=0/micro_step=9240/global_step=9240, RunningAvgSamplesPerSec=276.2084048187557, CurrSamplesPerSec=278.29626116069227, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:27.045 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6482
dss8440-001: [2023-07-27 23:26:33,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=9250, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:33,212] [INFO] [timer.py:215:stop] epoch=0/micro_step=9250/global_step=9250, RunningAvgSamplesPerSec=276.20930278070233, CurrSamplesPerSec=286.9456941841056, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:33.213 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6479
dss8440-001: [2023-07-27 23:26:39,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=9260, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:39,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=9260/global_step=9260, RunningAvgSamplesPerSec=276.20615136539135, CurrSamplesPerSec=283.37965767288176, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:39.465 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6475
dss8440-001: [2023-07-27 23:26:45,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=9270, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:45,643] [INFO] [timer.py:215:stop] epoch=0/micro_step=9270/global_step=9270, RunningAvgSamplesPerSec=276.20648942570546, CurrSamplesPerSec=292.2610196739452, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:45.644 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6467
dss8440-001: [2023-07-27 23:26:51,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=9280, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:51,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=9280/global_step=9280, RunningAvgSamplesPerSec=276.21302389710877, CurrSamplesPerSec=273.8734879772768, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:51.698 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6463
dss8440-001: [2023-07-27 23:26:57,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=9290, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:26:57,980] [INFO] [timer.py:215:stop] epoch=0/micro_step=9290/global_step=9290, RunningAvgSamplesPerSec=276.20813394657154, CurrSamplesPerSec=271.26811516371527, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:26:57.980 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6459
dss8440-001: [2023-07-27 23:27:03,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=9300, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:04,081] [INFO] [timer.py:215:stop] epoch=0/micro_step=9300/global_step=9300, RunningAvgSamplesPerSec=276.2123015214616, CurrSamplesPerSec=285.0350273125016, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:04.081 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6455
dss8440-001: [2023-07-27 23:27:10,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=9310, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:10,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=9310/global_step=9310, RunningAvgSamplesPerSec=276.2079172645323, CurrSamplesPerSec=279.7978050358144, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:10.355 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6452
dss8440-001: [2023-07-27 23:27:16,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=9320, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:16,569] [INFO] [timer.py:215:stop] epoch=0/micro_step=9320/global_step=9320, RunningAvgSamplesPerSec=276.2063391597966, CurrSamplesPerSec=270.8984127691999, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:16.570 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6447
dss8440-001: [2023-07-27 23:27:22,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=9330, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:22,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=9330/global_step=9330, RunningAvgSamplesPerSec=276.20626099510963, CurrSamplesPerSec=273.4488940665683, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:22.759 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6442
dss8440-001: [2023-07-27 23:27:28,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=9340, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:28,926] [INFO] [timer.py:215:stop] epoch=0/micro_step=9340/global_step=9340, RunningAvgSamplesPerSec=276.20681755421117, CurrSamplesPerSec=278.5600802659402, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:28.927 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6436
dss8440-001: [2023-07-27 23:27:35,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=9350, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:35,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=9350/global_step=9350, RunningAvgSamplesPerSec=276.20350619739924, CurrSamplesPerSec=284.86886241343484, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:35.179 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6431
dss8440-001: [2023-07-27 23:27:41,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=9360, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:41,391] [INFO] [timer.py:215:stop] epoch=0/micro_step=9360/global_step=9360, RunningAvgSamplesPerSec=276.2020723809775, CurrSamplesPerSec=271.57800144529557, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:41.392 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6426
dss8440-001: [2023-07-27 23:27:47,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=9370, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:47,565] [INFO] [timer.py:215:stop] epoch=0/micro_step=9370/global_step=9370, RunningAvgSamplesPerSec=276.2027344449817, CurrSamplesPerSec=275.1958383372095, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:47.565 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6426
dss8440-001: [2023-07-27 23:27:53,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=9380, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:27:53,840] [INFO] [timer.py:215:stop] epoch=0/micro_step=9380/global_step=9380, RunningAvgSamplesPerSec=276.19839878327133, CurrSamplesPerSec=264.64890980778637, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:27:53.841 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6420
dss8440-001: [2023-07-27 23:27:59,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=9390, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:00,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=9390/global_step=9390, RunningAvgSamplesPerSec=276.1984178107536, CurrSamplesPerSec=276.89047815897555, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:00.034 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6416
dss8440-001: [2023-07-27 23:28:06,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=9400, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:06,187] [INFO] [timer.py:215:stop] epoch=0/micro_step=9400/global_step=9400, RunningAvgSamplesPerSec=276.20006404451226, CurrSamplesPerSec=276.86088649790423, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:06.188 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6413
dss8440-001: [2023-07-27 23:28:12,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=9410, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:12,471] [INFO] [timer.py:215:stop] epoch=0/micro_step=9410/global_step=9410, RunningAvgSamplesPerSec=276.1952664336475, CurrSamplesPerSec=265.95824491875675, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:12.472 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6409
dss8440-001: [2023-07-27 23:28:18,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=9420, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:18,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=9420/global_step=9420, RunningAvgSamplesPerSec=276.19268241165963, CurrSamplesPerSec=281.54775151153854, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:18.712 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6406
dss8440-001: [2023-07-27 23:28:24,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=9430, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:24,858] [INFO] [timer.py:215:stop] epoch=0/micro_step=9430/global_step=9430, RunningAvgSamplesPerSec=276.19496163340887, CurrSamplesPerSec=282.2818936808535, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:24.859 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6402
dss8440-001: [2023-07-27 23:28:30,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=9440, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:31,043] [INFO] [timer.py:215:stop] epoch=0/micro_step=9440/global_step=9440, RunningAvgSamplesPerSec=276.19507335016124, CurrSamplesPerSec=287.1387440016691, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:31.044 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6399
dss8440-001: [2023-07-27 23:28:37,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=9450, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:37,193] [INFO] [timer.py:215:stop] epoch=0/micro_step=9450/global_step=9450, RunningAvgSamplesPerSec=276.19684299377036, CurrSamplesPerSec=282.2641407853562, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:37.194 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6396
dss8440-001: [2023-07-27 23:28:43,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=9460, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:43,379] [INFO] [timer.py:215:stop] epoch=0/micro_step=9460/global_step=9460, RunningAvgSamplesPerSec=276.19683745025026, CurrSamplesPerSec=287.3205720127072, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:43.379 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6391
dss8440-001: [2023-07-27 23:28:49,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=9470, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:49,619] [INFO] [timer.py:215:stop] epoch=0/micro_step=9470/global_step=9470, RunningAvgSamplesPerSec=276.19430202055884, CurrSamplesPerSec=267.22626985397125, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:49.620 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6387
dss8440-001: [2023-07-27 23:28:55,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=9480, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:28:55,776] [INFO] [timer.py:215:stop] epoch=0/micro_step=9480/global_step=9480, RunningAvgSamplesPerSec=276.1961570106706, CurrSamplesPerSec=281.0916475886276, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:28:55.777 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6383
dss8440-001: [2023-07-27 23:29:01,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=9490, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:01,976] [INFO] [timer.py:215:stop] epoch=0/micro_step=9490/global_step=9490, RunningAvgSamplesPerSec=276.19581352061294, CurrSamplesPerSec=277.71034681718703, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:01.977 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6377
dss8440-001: [2023-07-27 23:29:08,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=9500, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:08,275] [INFO] [timer.py:215:stop] epoch=0/micro_step=9500/global_step=9500, RunningAvgSamplesPerSec=276.190658124074, CurrSamplesPerSec=266.0354614240069, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:08.276 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6374
dss8440-001: [2023-07-27 23:29:14,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=9510, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:14,504] [INFO] [timer.py:215:stop] epoch=0/micro_step=9510/global_step=9510, RunningAvgSamplesPerSec=276.1888305611508, CurrSamplesPerSec=279.4323151689637, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:14.505 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6369
dss8440-001: [2023-07-27 23:29:20,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=9520, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:20,670] [INFO] [timer.py:215:stop] epoch=0/micro_step=9520/global_step=9520, RunningAvgSamplesPerSec=276.19012726759144, CurrSamplesPerSec=283.0960571460712, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:20.670 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6366
dss8440-001: [2023-07-27 23:29:26,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=9530, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:26,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=9530/global_step=9530, RunningAvgSamplesPerSec=276.19265245228763, CurrSamplesPerSec=277.5857394414746, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:26.807 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6361
dss8440-001: [2023-07-27 23:29:32,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=9540, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:33,043] [INFO] [timer.py:215:stop] epoch=0/micro_step=9540/global_step=9540, RunningAvgSamplesPerSec=276.19012672382934, CurrSamplesPerSec=283.47975351723125, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:33.044 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6358
dss8440-001: [2023-07-27 23:29:39,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=9550, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:39,285] [INFO] [timer.py:215:stop] epoch=0/micro_step=9550/global_step=9550, RunningAvgSamplesPerSec=276.18736559783184, CurrSamplesPerSec=284.746034266844, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:39.286 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6354
dss8440-001: [2023-07-27 23:29:45,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=9560, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:45,523] [INFO] [timer.py:215:stop] epoch=0/micro_step=9560/global_step=9560, RunningAvgSamplesPerSec=276.1847177622008, CurrSamplesPerSec=275.5561416207858, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:45.524 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6351
dss8440-001: [2023-07-27 23:29:51,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=9570, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:51,681] [INFO] [timer.py:215:stop] epoch=0/micro_step=9570/global_step=9570, RunningAvgSamplesPerSec=276.18610116266365, CurrSamplesPerSec=282.3820075227803, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:51.682 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6348
dss8440-001: [2023-07-27 23:29:57,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=9580, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:29:57,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=9580/global_step=9580, RunningAvgSamplesPerSec=276.18216795882046, CurrSamplesPerSec=266.9496902951572, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:29:57.950 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6343
dss8440-001: [2023-07-27 23:30:04,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=9590, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:04,079] [INFO] [timer.py:215:stop] epoch=0/micro_step=9590/global_step=9590, RunningAvgSamplesPerSec=276.18517409376216, CurrSamplesPerSec=283.9581785716157, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:04.080 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6339
dss8440-001: [2023-07-27 23:30:10,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=9600, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:10,261] [INFO] [timer.py:215:stop] epoch=0/micro_step=9600/global_step=9600, RunningAvgSamplesPerSec=276.1853077281073, CurrSamplesPerSec=280.34047469570635, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:10.262 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6334
dss8440-001: [2023-07-27 23:30:16,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=9610, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:16,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=9610/global_step=9610, RunningAvgSamplesPerSec=276.18532886743685, CurrSamplesPerSec=268.96447069035213, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:16.441 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6329
dss8440-001: [2023-07-27 23:30:22,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=9620, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:22,710] [INFO] [timer.py:215:stop] epoch=0/micro_step=9620/global_step=9620, RunningAvgSamplesPerSec=276.1811642854842, CurrSamplesPerSec=270.85530307340093, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:22.711 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6323
dss8440-001: [2023-07-27 23:30:28,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=9630, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:28,903] [INFO] [timer.py:215:stop] epoch=0/micro_step=9630/global_step=9630, RunningAvgSamplesPerSec=276.18107699501064, CurrSamplesPerSec=282.47200086267753, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:28.904 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6318
dss8440-001: [2023-07-27 23:30:35,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=9640, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:35,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=9640/global_step=9640, RunningAvgSamplesPerSec=276.18082952479483, CurrSamplesPerSec=273.34557559079343, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:35.093 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6314
dss8440-001: [2023-07-27 23:30:41,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=9650, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:41,282] [INFO] [timer.py:215:stop] epoch=0/micro_step=9650/global_step=9650, RunningAvgSamplesPerSec=276.18089500232037, CurrSamplesPerSec=265.46197845617263, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:41.283 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6313
dss8440-001: [2023-07-27 23:30:47,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=9660, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:47,496] [INFO] [timer.py:215:stop] epoch=0/micro_step=9660/global_step=9660, RunningAvgSamplesPerSec=276.1797305996153, CurrSamplesPerSec=271.84129080842183, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:47.497 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6310
dss8440-001: [2023-07-27 23:30:53,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=9670, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:53,728] [INFO] [timer.py:215:stop] epoch=0/micro_step=9670/global_step=9670, RunningAvgSamplesPerSec=276.17758260656115, CurrSamplesPerSec=279.5915308704383, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:53.729 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6307
dss8440-001: [2023-07-27 23:30:59,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=9680, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:30:59,962] [INFO] [timer.py:215:stop] epoch=0/micro_step=9680/global_step=9680, RunningAvgSamplesPerSec=276.1755099281683, CurrSamplesPerSec=284.5298748201607, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:30:59.963 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6302
dss8440-001: [2023-07-27 23:31:05,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=9690, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:06,201] [INFO] [timer.py:215:stop] epoch=0/micro_step=9690/global_step=9690, RunningAvgSamplesPerSec=276.1731546322702, CurrSamplesPerSec=262.3229807837639, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:06.202 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6296
dss8440-001: [2023-07-27 23:31:12,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=9700, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:12,266] [INFO] [timer.py:215:stop] epoch=0/micro_step=9700/global_step=9700, RunningAvgSamplesPerSec=276.1788890161691, CurrSamplesPerSec=270.40503476752576, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:12.267 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6293
dss8440-001: [2023-07-27 23:31:18,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=9710, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:18,551] [INFO] [timer.py:215:stop] epoch=0/micro_step=9710/global_step=9710, RunningAvgSamplesPerSec=276.1740920366521, CurrSamplesPerSec=289.61592354541216, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:18.551 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6287
dss8440-001: [2023-07-27 23:31:24,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=9720, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:24,713] [INFO] [timer.py:215:stop] epoch=0/micro_step=9720/global_step=9720, RunningAvgSamplesPerSec=276.1753925018586, CurrSamplesPerSec=272.10939936467923, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:24.714 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6281
dss8440-001: [2023-07-27 23:31:30,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=9730, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:30,889] [INFO] [timer.py:215:stop] epoch=0/micro_step=9730/global_step=9730, RunningAvgSamplesPerSec=276.1756684649425, CurrSamplesPerSec=282.0837985069608, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:30.889 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6277
dss8440-001: [2023-07-27 23:31:37,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=9740, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:37,150] [INFO] [timer.py:215:stop] epoch=0/micro_step=9740/global_step=9740, RunningAvgSamplesPerSec=276.17224128432184, CurrSamplesPerSec=267.92176067752763, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:37.151 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6274
dss8440-001: [2023-07-27 23:31:43,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=9750, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:43,281] [INFO] [timer.py:215:stop] epoch=0/micro_step=9750/global_step=9750, RunningAvgSamplesPerSec=276.1749046609672, CurrSamplesPerSec=283.6943197368882, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:43.281 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6270
dss8440-001: [2023-07-27 23:31:49,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=9760, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:49,392] [INFO] [timer.py:215:stop] epoch=0/micro_step=9760/global_step=9760, RunningAvgSamplesPerSec=276.1786501752983, CurrSamplesPerSec=278.7097043666466, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:49.392 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6266
dss8440-001: [2023-07-27 23:31:55,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=9770, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:31:55,649] [INFO] [timer.py:215:stop] epoch=0/micro_step=9770/global_step=9770, RunningAvgSamplesPerSec=276.1757041800738, CurrSamplesPerSec=269.41767586974225, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:31:55.650 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6259
dss8440-001: [2023-07-27 23:32:01,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=9780, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:01,928] [INFO] [timer.py:215:stop] epoch=0/micro_step=9780/global_step=9780, RunningAvgSamplesPerSec=276.1715086348263, CurrSamplesPerSec=273.0197964061694, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:01.929 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6253
dss8440-001: [2023-07-27 23:32:08,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=9790, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:08,173] [INFO] [timer.py:215:stop] epoch=0/micro_step=9790/global_step=9790, RunningAvgSamplesPerSec=276.1689391158532, CurrSamplesPerSec=257.35422428112497, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:08.173 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6250
dss8440-001: [2023-07-27 23:32:14,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=9800, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:14,440] [INFO] [timer.py:215:stop] epoch=0/micro_step=9800/global_step=9800, RunningAvgSamplesPerSec=276.1653574313164, CurrSamplesPerSec=269.5533063975022, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:14.441 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6248
dss8440-001: [2023-07-27 23:32:20,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=9810, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:20,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=9810/global_step=9810, RunningAvgSamplesPerSec=276.16319577947564, CurrSamplesPerSec=281.02360039179774, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:20.675 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6245
dss8440-001: [2023-07-27 23:32:26,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=9820, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:26,924] [INFO] [timer.py:215:stop] epoch=0/micro_step=9820/global_step=9820, RunningAvgSamplesPerSec=276.16023244942915, CurrSamplesPerSec=267.50525107360005, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:26.925 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6243
dss8440-001: [2023-07-27 23:32:32,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=9830, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:33,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=9830/global_step=9830, RunningAvgSamplesPerSec=276.1633006669624, CurrSamplesPerSec=277.1173035924336, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:33.047 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6237
dss8440-001: [2023-07-27 23:32:39,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=9840, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:39,305] [INFO] [timer.py:215:stop] epoch=0/micro_step=9840/global_step=9840, RunningAvgSamplesPerSec=276.16003223297065, CurrSamplesPerSec=286.64523616683636, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:39.306 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6233
dss8440-001: [2023-07-27 23:32:45,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=9850, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:45,453] [INFO] [timer.py:215:stop] epoch=0/micro_step=9850/global_step=9850, RunningAvgSamplesPerSec=276.16187047612067, CurrSamplesPerSec=272.8703000421711, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:45.454 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6230
dss8440-001: [2023-07-27 23:32:51,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=9860, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:51,567] [INFO] [timer.py:215:stop] epoch=0/micro_step=9860/global_step=9860, RunningAvgSamplesPerSec=276.1656103775008, CurrSamplesPerSec=286.19409970935567, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:51.568 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6225
dss8440-001: [2023-07-27 23:32:57,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=9870, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:32:57,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=9870/global_step=9870, RunningAvgSamplesPerSec=276.16644586019686, CurrSamplesPerSec=275.25732877646107, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:32:57.736 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6223
dss8440-001: [2023-07-27 23:33:03,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=9880, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:03,985] [INFO] [timer.py:215:stop] epoch=0/micro_step=9880/global_step=9880, RunningAvgSamplesPerSec=276.16344510204294, CurrSamplesPerSec=268.0209169817349, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:03.986 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6218
dss8440-001: [2023-07-27 23:33:10,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=9890, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:10,143] [INFO] [timer.py:215:stop] epoch=0/micro_step=9890/global_step=9890, RunningAvgSamplesPerSec=276.1647279666104, CurrSamplesPerSec=276.6875977241225, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:10.144 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6215
dss8440-001: [2023-07-27 23:33:16,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=9900, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:16,274] [INFO] [timer.py:215:stop] epoch=0/micro_step=9900/global_step=9900, RunningAvgSamplesPerSec=276.16736196057053, CurrSamplesPerSec=284.4398286683774, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:16.275 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6212
dss8440-001: [2023-07-27 23:33:22,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=9910, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:22,430] [INFO] [timer.py:215:stop] epoch=0/micro_step=9910/global_step=9910, RunningAvgSamplesPerSec=276.16898871586733, CurrSamplesPerSec=266.67115960803324, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:22.431 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6207
dss8440-001: [2023-07-27 23:33:28,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=9920, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:28,584] [INFO] [timer.py:215:stop] epoch=0/micro_step=9920/global_step=9920, RunningAvgSamplesPerSec=276.1704805829332, CurrSamplesPerSec=274.1635418239204, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:28.585 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6202
dss8440-001: [2023-07-27 23:33:34,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=9930, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:34,756] [INFO] [timer.py:215:stop] epoch=0/micro_step=9930/global_step=9930, RunningAvgSamplesPerSec=276.17146606889827, CurrSamplesPerSec=278.51813063543784, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:34.757 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6198
dss8440-001: [2023-07-27 23:33:40,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=9940, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:40,915] [INFO] [timer.py:215:stop] epoch=0/micro_step=9940/global_step=9940, RunningAvgSamplesPerSec=276.1728081421316, CurrSamplesPerSec=265.85810097010017, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:40.916 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6194
dss8440-001: [2023-07-27 23:33:46,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=9950, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:46,958] [INFO] [timer.py:215:stop] epoch=0/micro_step=9950/global_step=9950, RunningAvgSamplesPerSec=276.17935253817524, CurrSamplesPerSec=293.35425127423946, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:46.959 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6190
dss8440-001: [2023-07-27 23:33:53,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=9960, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:53,146] [INFO] [timer.py:215:stop] epoch=0/micro_step=9960/global_step=9960, RunningAvgSamplesPerSec=276.1798027036406, CurrSamplesPerSec=283.20289567910885, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:53.146 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6186
dss8440-001: [2023-07-27 23:33:59,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=9970, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:33:59,344] [INFO] [timer.py:215:stop] epoch=0/micro_step=9970/global_step=9970, RunningAvgSamplesPerSec=276.1790616133258, CurrSamplesPerSec=264.1436499752029, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:33:59.345 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6181
dss8440-001: [2023-07-27 23:34:05,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=9980, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:34:05,473] [INFO] [timer.py:215:stop] epoch=0/micro_step=9980/global_step=9980, RunningAvgSamplesPerSec=276.1816264915173, CurrSamplesPerSec=285.81461800083963, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:34:05.474 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6179
dss8440-001: [2023-07-27 23:34:11,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=9990, skipped=26, lr=[0.0001], mom=[(0.9, 0.999)]
dss8440-001: [2023-07-27 23:34:11,672] [INFO] [timer.py:215:stop] epoch=0/micro_step=9990/global_step=9990, RunningAvgSamplesPerSec=276.1809559934031, CurrSamplesPerSec=272.92007873375303, MemAllocated=0.07GB, MaxMemAllocated=1.71GB
dss8440-001: 2023-07-27 23:34:11.673 | INFO     | __main__:log_dist:53 - [Rank 0] Loss: 6.6176
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-003: ds_experiments/bert_pretrain.2023.7.27.6.50.51.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-002: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-001: ds_experiments/bert_pretrain.2023.7.27.6.50.52.addjtvxg
dss8440-003: worker2:61448:62892 [5] NCCL INFO [Service thread] Connection closed by localRank 5
dss8440-003: worker2:61448:61448 [5] NCCL INFO comm 0x76908a00 rank 19 nranks 21 cudaDev 5 busId 8a000 - Abort COMPLETE
dss8440-003: worker2:61447:62894 [4] NCCL INFO [Service thread] Connection closed by localRank 4
dss8440-003: worker2:61445:62888 [2] NCCL INFO [Service thread] Connection closed by localRank 2
dss8440-003: worker2:61447:61447 [4] NCCL INFO comm 0x757170c0 rank 18 nranks 21 cudaDev 4 busId 89000 - Abort COMPLETE
dss8440-003: worker2:61445:61445 [2] NCCL INFO comm 0x764e40e0 rank 16 nranks 21 cudaDev 2 busId 48000 - Abort COMPLETE
dss8440-002: worker1:4989:6425 [5] NCCL INFO [Service thread] Connection closed by localRank 5
dss8440-003: worker2:61444:62891 [1] NCCL INFO [Service thread] Connection closed by localRank 1
dss8440-003: worker2:61443:62890 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dss8440-002: worker1:4989:4989 [5] NCCL INFO comm 0x761bec40 rank 12 nranks 21 cudaDev 5 busId 8a000 - Abort COMPLETE
dss8440-003: worker2:61444:61444 [1] NCCL INFO comm 0x706cd1c0 rank 15 nranks 21 cudaDev 1 busId 13000 - Abort COMPLETE
dss8440-003: worker2:61443:61443 [0] NCCL INFO comm 0x73211200 rank 14 nranks 21 cudaDev 0 busId 12000 - Abort COMPLETE
dss8440-002: worker1:4987:6428 [3] NCCL INFO [Service thread] Connection closed by localRank 3
dss8440-002: worker1:4990:6431 [6] NCCL INFO [Service thread] Connection closed by localRank 6
dss8440-002: worker1:4987:4987 [3] NCCL INFO comm 0x75dca250 rank 10 nranks 21 cudaDev 3 busId 49000 - Abort COMPLETE
dss8440-002: worker1:4990:4990 [6] NCCL INFO comm 0x73855280 rank 13 nranks 21 cudaDev 6 busId c1000 - Abort COMPLETE
dss8440-003: worker2:61449:62893 [6] NCCL INFO [Service thread] Connection closed by localRank 6
dss8440-001: master:13936:15393 [2] NCCL INFO [Service thread] Connection closed by localRank 2
dss8440-003: worker2:61446:62889 [3] NCCL INFO [Service thread] Connection closed by localRank 3
dss8440-003: worker2:61449:61449 [6] NCCL INFO comm 0x740186b0 rank 20 nranks 21 cudaDev 6 busId c1000 - Abort COMPLETE
dss8440-001: master:13936:13936 [2] NCCL INFO comm 0x73f187d0 rank 2 nranks 21 cudaDev 2 busId 48000 - Abort COMPLETE
dss8440-003: worker2:61446:61446 [3] NCCL INFO comm 0x70f55260 rank 17 nranks 21 cudaDev 3 busId 49000 - Abort COMPLETE
dss8440-001: master:13937:15392 [3] NCCL INFO [Service thread] Connection closed by localRank 3
dss8440-001: master:13937:13937 [3] NCCL INFO comm 0x74794300 rank 3 nranks 21 cudaDev 3 busId 49000 - Abort COMPLETE
dss8440-002: worker1:4985:6430 [1] NCCL INFO [Service thread] Connection closed by localRank 1
dss8440-002: worker1:4986:6429 [2] NCCL INFO [Service thread] Connection closed by localRank 2
dss8440-002: worker1:4985:4985 [1] NCCL INFO comm 0x73007520 rank 8 nranks 21 cudaDev 1 busId 13000 - Abort COMPLETE
dss8440-002: worker1:4986:4986 [2] NCCL INFO comm 0x7539ca80 rank 9 nranks 21 cudaDev 2 busId 48000 - Abort COMPLETE
dss8440-001: master:13935:15394 [1] NCCL INFO [Service thread] Connection closed by localRank 1
dss8440-001: master:13939:15396 [5] NCCL INFO [Service thread] Connection closed by localRank 5
dss8440-001: master:13939:13939 [5] NCCL INFO comm 0x7516e3d0 rank 5 nranks 21 cudaDev 5 busId 8a000 - Abort COMPLETE
dss8440-001: master:13935:13935 [1] NCCL INFO comm 0x74680a80 rank 1 nranks 21 cudaDev 1 busId 13000 - Abort COMPLETE
dss8440-002: worker1:4988:6427 [4] NCCL INFO [Service thread] Connection closed by localRank 4
dss8440-001: master:13940:15397 [6] NCCL INFO [Service thread] Connection closed by localRank 6
dss8440-002: worker1:4988:4988 [4] NCCL INFO comm 0x746bd880 rank 11 nranks 21 cudaDev 4 busId 89000 - Abort COMPLETE
dss8440-001: master:13940:13940 [6] NCCL INFO comm 0x73b6a800 rank 6 nranks 21 cudaDev 6 busId c1000 - Abort COMPLETE
dss8440-001: master:13938:15398 [4] NCCL INFO [Service thread] Connection closed by localRank 4
dss8440-002: worker1:4984:6426 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dss8440-001: master:13938:13938 [4] NCCL INFO comm 0x75fb6340 rank 4 nranks 21 cudaDev 4 busId 89000 - Abort COMPLETE
dss8440-001: master:13934:15395 [0] NCCL INFO [Service thread] Connection closed by localRank 0
dss8440-002: worker1:4984:4984 [0] NCCL INFO comm 0x72cf2480 rank 7 nranks 21 cudaDev 0 busId 12000 - Abort COMPLETE
dss8440-001: master:13934:13934 [0] NCCL INFO comm 0x7556c3c0 rank 0 nranks 21 cudaDev 0 busId 12000 - Abort COMPLETE
dss8440-002: [2023-07-27 23:34:18,634] [INFO] [launch.py:347:main] Process 4989 exits successfully.
dss8440-003: [2023-07-27 23:34:19,504] [INFO] [launch.py:347:main] Process 61445 exits successfully.
dss8440-003: [2023-07-27 23:34:19,504] [INFO] [launch.py:347:main] Process 61448 exits successfully.
dss8440-003: [2023-07-27 23:34:19,504] [INFO] [launch.py:347:main] Process 61447 exits successfully.
dss8440-003: [2023-07-27 23:34:19,505] [INFO] [launch.py:347:main] Process 61449 exits successfully.
dss8440-003: [2023-07-27 23:34:19,505] [INFO] [launch.py:347:main] Process 61444 exits successfully.
dss8440-003: [2023-07-27 23:34:19,505] [INFO] [launch.py:347:main] Process 61443 exits successfully.
dss8440-003: [2023-07-27 23:34:19,505] [INFO] [launch.py:347:main] Process 61446 exits successfully.
dss8440-002: [2023-07-27 23:34:19,634] [INFO] [launch.py:347:main] Process 4987 exits successfully.
dss8440-002: [2023-07-27 23:34:19,634] [INFO] [launch.py:347:main] Process 4986 exits successfully.
dss8440-002: [2023-07-27 23:34:19,635] [INFO] [launch.py:347:main] Process 4985 exits successfully.
dss8440-002: [2023-07-27 23:34:19,635] [INFO] [launch.py:347:main] Process 4984 exits successfully.
dss8440-002: [2023-07-27 23:34:19,635] [INFO] [launch.py:347:main] Process 4988 exits successfully.
dss8440-002: [2023-07-27 23:34:19,635] [INFO] [launch.py:347:main] Process 4990 exits successfully.
dss8440-001: [2023-07-27 23:34:19,697] [INFO] [launch.py:347:main] Process 13936 exits successfully.
dss8440-001: [2023-07-27 23:34:19,698] [INFO] [launch.py:347:main] Process 13939 exits successfully.
dss8440-001: [2023-07-27 23:34:19,698] [INFO] [launch.py:347:main] Process 13938 exits successfully.
dss8440-001: [2023-07-27 23:34:19,698] [INFO] [launch.py:347:main] Process 13940 exits successfully.
dss8440-001: [2023-07-27 23:34:19,698] [INFO] [launch.py:347:main] Process 13935 exits successfully.
dss8440-001: [2023-07-27 23:34:19,698] [INFO] [launch.py:347:main] Process 13934 exits successfully.
dss8440-001: [2023-07-27 23:34:19,698] [INFO] [launch.py:347:main] Process 13937 exits successfully.
