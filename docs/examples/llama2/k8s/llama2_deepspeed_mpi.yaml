apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: llama2-finetuning-deepspeed
spec:
  slotsPerWorker: 2
  runPolicy:
    cleanPodPolicy: None  # No pods will be deleted after the job terminates.
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          hostIPC: True  # Setting True for now. Subject to change.
          containers:
          - name: llama2-training-deepspeed-mpi
            image: dockerhub.kubekey.local/kubesphereio/llama2_demo:2023-10-25
            imagePullPolicy: IfNotPresent
            command:
            - mpirun
            - -x
            - NCCL_DEBUG=WARN
            - -x
            - NCCL_P2P_LEVEL=SYS
            - -x
            - NCCL_IB_CUDA_SUPPORT=1
            - -x
            - NCCL_DEBUG_SUBSYS=ALL
            - -x
            - NCCL_IB_GDR_LEVEL=SYS
            - -x
            - NCCL_NET_GDR_LEVEL=SYS
            - -x
            - NCCL_NET_GDR_READ=1
            - -x
            - NCCL_NET=IB
            - -x
            - PATH
            #- -x
            #- NCCL_SOCKET_IFNAME=ibs1
            #- -x
            #- NCCL_IB_HCA=mlx5_
            - -np
            - "4"
            #- --allow-run-as-root
            - --nooversubscribe
            - --bind-to
            - none
            - --map-by
            - slot
            - -mca
            - pml ob1
            - -mca
            - btl ^openlib
            - mca
            - mpi_warn_on_fork "0"
            # All nodes where training processes run, including 1 master and 1+ workers. Please reach out to Glen to talk about this.
            #- -H
            # Node configurations, includong host names, slots, etc. Please reach out to Glen to talk about this.
            #- --hostfile
            - python3
            - llama2_demo.py
            env:
            - name: NCCL_TREE_THRESHOLD
              value: "0"  # Set NCCL tree threshold to 0 for DeepSpeed.
            - name: NCCL_ALGO
              value: "1"  # Set NCCL_ALGO to 1 for DeepSpeed.
    Worker:
      replicas: 2  # Number of nodes for training.
      template:
        spec:
          hostIPC: True
          nodeSelector:
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          containers:
          - name: llama2-training-deepspeed-mpi
            image: dockerhub.kubekey.local/kubesphereio/llama2_demo:2023-10-25
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                # TODO(yzhao): 添加 IB 设备
                nvidia.com/gpu: 2  # Number of GPUs required per replica.
